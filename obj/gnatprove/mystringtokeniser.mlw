(* Module for axiomatizing type "short_short_integer", created in Gnat2Why.Types.Translate_Type *)
module Standard__short_short_integer
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int

 type short_short_integer =
  < range -128 127 >
 
 val constant first 
   : int
  ensures { result =  (( -128) : int) }
 
 val constant last 
   : int
  ensures { result =  (127 : int) }
 
 predicate in_range 
   (x : int) =
  ( (first <= x) /\ (x <= last) )
 val in_range 
   (x : int) : bool
  ensures { result <-> in_range (x : int) }
 
 clone export "ada__model".Static_Discrete with axiom .,
 type t = short_short_integer, 
 function first = first, 
 function last = last, 
 predicate in_range = in_range
 
 type short_short_integer__ref =
  { mutable short_short_integer__content : short_short_integer }
 
 function short_short_integer__ref_short_short_integer__content__projection 
   (a : short_short_integer__ref) : short_short_integer =
  a.short_short_integer__content
 
 meta "model_projection" function short_short_integer__ref_short_short_integer__content__projection
 
 meta "inline:no" function short_short_integer__ref_short_short_integer__content__projection
 
 val short_short_integer__havoc 
   (x : short_short_integer__ref) : unit
  writes {x}

end

(* Module defining to_rep/of_rep for type "short_short_integer", created in Gnat2Why.Types.Translate_Type *)
module Standard__short_short_integer__rep
 use        Standard__short_short_integer as Standard__short_short_integer
 use        "_gnatprove_standard".Main
 use        "int".Int

 function to_rep 
   (x : Standard__short_short_integer.short_short_integer) : int =
  (Standard__short_short_integer.short_short_integer'int x)
 
 clone export "ada__model".Rep_Proj_Int with axiom .,
 type t = Standard__short_short_integer.short_short_integer, 
 predicate in_range = Standard__short_short_integer.in_range, 
 function to_rep = to_rep
 
 meta "model_projection" function to_rep
 
 meta "inline:no" function to_rep

end

(* Module for axiomatizing type "short_integer", created in Gnat2Why.Types.Translate_Type *)
module Standard__short_integer
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int

 type short_integer =
  < range -32768 32767 >
 
 val constant first 
   : int
  ensures { result =  (( -32768) : int) }
 
 val constant last 
   : int
  ensures { result =  (32767 : int) }
 
 predicate in_range 
   (x : int) =
  ( (first <= x) /\ (x <= last) )
 val in_range 
   (x : int) : bool
  ensures { result <-> in_range (x : int) }
 
 clone export "ada__model".Static_Discrete with axiom .,
 type t = short_integer, 
 function first = first, 
 function last = last, 
 predicate in_range = in_range
 
 type short_integer__ref =
  { mutable short_integer__content : short_integer }
 
 function short_integer__ref_short_integer__content__projection 
   (a : short_integer__ref) : short_integer =
  a.short_integer__content
 
 meta "model_projection" function short_integer__ref_short_integer__content__projection
 
 meta "inline:no" function short_integer__ref_short_integer__content__projection
 
 val short_integer__havoc 
   (x : short_integer__ref) : unit
  writes {x}

end

(* Module defining to_rep/of_rep for type "short_integer", created in Gnat2Why.Types.Translate_Type *)
module Standard__short_integer__rep
 use        Standard__short_integer as Standard__short_integer
 use        "_gnatprove_standard".Main
 use        "int".Int

 function to_rep 
   (x : Standard__short_integer.short_integer) : int =
  (Standard__short_integer.short_integer'int x)
 
 clone export "ada__model".Rep_Proj_Int with axiom .,
 type t = Standard__short_integer.short_integer, 
 predicate in_range = Standard__short_integer.in_range, 
 function to_rep = to_rep
 
 meta "model_projection" function to_rep
 
 meta "inline:no" function to_rep

end

(* Module for axiomatizing type "integer", created in Gnat2Why.Types.Translate_Type *)
module Standard__integer
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int

 type integer =
  < range -2147483648 2147483647 >
 
 val constant first 
   : int
  ensures { result =  (( -2147483648) : int) }
 
 val constant last 
   : int
  ensures { result =  (2147483647 : int) }
 
 predicate in_range 
   (x : int) =
  ( (first <= x) /\ (x <= last) )
 val in_range 
   (x : int) : bool
  ensures { result <-> in_range (x : int) }
 
 clone export "ada__model".Static_Discrete with axiom .,
 type t = integer, 
 function first = first, 
 function last = last, 
 predicate in_range = in_range
 
 type integer__ref =
  { mutable integer__content : integer }
 
 function integer__ref_integer__content__projection 
   (a : integer__ref) : integer =
  a.integer__content
 
 meta "model_projection" function integer__ref_integer__content__projection
 
 meta "inline:no" function integer__ref_integer__content__projection
 
 val integer__havoc 
   (x : integer__ref) : unit
  writes {x}

end

(* Module defining to_rep/of_rep for type "integer", created in Gnat2Why.Types.Translate_Type *)
module Standard__integer__rep
 use        Standard__integer as Standard__integer
 use        "_gnatprove_standard".Main
 use        "int".Int

 function to_rep 
   (x : Standard__integer.integer) : int =
  (Standard__integer.integer'int x)
 
 clone export "ada__model".Rep_Proj_Int with axiom .,
 type t = Standard__integer.integer, 
 predicate in_range = Standard__integer.in_range, 
 function to_rep = to_rep
 
 meta "model_projection" function to_rep
 
 meta "inline:no" function to_rep

end

(* Module for axiomatizing type "long_integer", created in Gnat2Why.Types.Translate_Type *)
module Standard__long_integer
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int

 type long_integer =
  < range -9223372036854775808 9223372036854775807 >
 
 val constant first 
   : int
  ensures { result =  (( -9223372036854775808) : int) }
 
 val constant last 
   : int
  ensures { result =  (9223372036854775807 : int) }
 
 predicate in_range 
   (x : int) =
  ( (first <= x) /\ (x <= last) )
 val in_range 
   (x : int) : bool
  ensures { result <-> in_range (x : int) }
 
 clone export "ada__model".Static_Discrete with axiom .,
 type t = long_integer, 
 function first = first, 
 function last = last, 
 predicate in_range = in_range
 
 type long_integer__ref =
  { mutable long_integer__content : long_integer }
 
 function long_integer__ref_long_integer__content__projection 
   (a : long_integer__ref) : long_integer =
  a.long_integer__content
 
 meta "model_projection" function long_integer__ref_long_integer__content__projection
 
 meta "inline:no" function long_integer__ref_long_integer__content__projection
 
 val long_integer__havoc 
   (x : long_integer__ref) : unit
  writes {x}

end

(* Module defining to_rep/of_rep for type "long_integer", created in Gnat2Why.Types.Translate_Type *)
module Standard__long_integer__rep
 use        Standard__long_integer as Standard__long_integer
 use        "_gnatprove_standard".Main
 use        "int".Int

 function to_rep 
   (x : Standard__long_integer.long_integer) : int =
  (Standard__long_integer.long_integer'int x)
 
 clone export "ada__model".Rep_Proj_Int with axiom .,
 type t = Standard__long_integer.long_integer, 
 predicate in_range = Standard__long_integer.in_range, 
 function to_rep = to_rep
 
 meta "model_projection" function to_rep
 
 meta "inline:no" function to_rep

end

(* Module for axiomatizing type "long_long_integer", created in Gnat2Why.Types.Translate_Type *)
module Standard__long_long_integer
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int

 type long_long_integer =
  < range -9223372036854775808 9223372036854775807 >
 
 val constant first 
   : int
  ensures { result =  (( -9223372036854775808) : int) }
 
 val constant last 
   : int
  ensures { result =  (9223372036854775807 : int) }
 
 predicate in_range 
   (x : int) =
  ( (first <= x) /\ (x <= last) )
 val in_range 
   (x : int) : bool
  ensures { result <-> in_range (x : int) }
 
 clone export "ada__model".Static_Discrete with axiom .,
 type t = long_long_integer, 
 function first = first, 
 function last = last, 
 predicate in_range = in_range
 
 type long_long_integer__ref =
  { mutable long_long_integer__content : long_long_integer }
 
 function long_long_integer__ref_long_long_integer__content__projection 
   (a : long_long_integer__ref) : long_long_integer =
  a.long_long_integer__content
 
 meta "model_projection" function long_long_integer__ref_long_long_integer__content__projection
 
 meta "inline:no" function long_long_integer__ref_long_long_integer__content__projection
 
 val long_long_integer__havoc 
   (x : long_long_integer__ref) : unit
  writes {x}

end

(* Module defining to_rep/of_rep for type "long_long_integer", created in Gnat2Why.Types.Translate_Type *)
module Standard__long_long_integer__rep
 use        Standard__long_long_integer as Standard__long_long_integer
 use        "_gnatprove_standard".Main
 use        "int".Int

 function to_rep 
   (x : Standard__long_long_integer.long_long_integer) : int =
  (Standard__long_long_integer.long_long_integer'int x)
 
 clone export "ada__model".Rep_Proj_Int with axiom .,
 type t = Standard__long_long_integer.long_long_integer, 
 predicate in_range = Standard__long_long_integer.in_range, 
 function to_rep = to_rep
 
 meta "model_projection" function to_rep
 
 meta "inline:no" function to_rep

end

(* Module for axiomatizing type "natural", created in Gnat2Why.Types.Translate_Type *)
module Standard__natural
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int

 type natural =
  < range 0 2147483647 >
 
 val constant first 
   : int
  ensures { result =  (0 : int) }
 
 val constant last 
   : int
  ensures { result =  (2147483647 : int) }
 
 predicate in_range 
   (x : int) =
  ( (first <= x) /\ (x <= last) )
 val in_range 
   (x : int) : bool
  ensures { result <-> in_range (x : int) }
 
 clone export "ada__model".Static_Discrete with axiom .,
 type t = natural, 
 function first = first, 
 function last = last, 
 predicate in_range = in_range
 
 type natural__ref =
  { mutable natural__content : natural }
 
 function natural__ref_natural__content__projection 
   (a : natural__ref) : natural =
  a.natural__content
 
 meta "model_projection" function natural__ref_natural__content__projection
 
 meta "inline:no" function natural__ref_natural__content__projection
 
 val natural__havoc 
   (x : natural__ref) : unit
  writes {x}

end

(* Module defining to_rep/of_rep for type "natural", created in Gnat2Why.Types.Translate_Type *)
module Standard__natural__rep
 use        Standard__natural as Standard__natural
 use        "_gnatprove_standard".Main
 use        "int".Int

 function to_rep 
   (x : Standard__natural.natural) : int =
  (Standard__natural.natural'int x)
 
 clone export "ada__model".Rep_Proj_Int with axiom .,
 type t = Standard__natural.natural, 
 predicate in_range = Standard__natural.in_range, 
 function to_rep = to_rep
 
 meta "model_projection" function to_rep
 
 meta "inline:no" function to_rep

end

(* Module for axiomatizing type "positive", created in Gnat2Why.Types.Translate_Type *)
module Standard__positive
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int

 type positive =
  < range 1 2147483647 >
 
 val constant first 
   : int
  ensures { result =  (1 : int) }
 
 val constant last 
   : int
  ensures { result =  (2147483647 : int) }
 
 predicate in_range 
   (x : int) =
  ( (first <= x) /\ (x <= last) )
 val in_range 
   (x : int) : bool
  ensures { result <-> in_range (x : int) }
 
 clone export "ada__model".Static_Discrete with axiom .,
 type t = positive, 
 function first = first, 
 function last = last, 
 predicate in_range = in_range
 
 type positive__ref =
  { mutable positive__content : positive }
 
 function positive__ref_positive__content__projection 
   (a : positive__ref) : positive =
  a.positive__content
 
 meta "model_projection" function positive__ref_positive__content__projection
 
 meta "inline:no" function positive__ref_positive__content__projection
 
 val positive__havoc 
   (x : positive__ref) : unit
  writes {x}

end

(* Module defining to_rep/of_rep for type "positive", created in Gnat2Why.Types.Translate_Type *)
module Standard__positive__rep
 use        Standard__positive as Standard__positive
 use        "_gnatprove_standard".Main
 use        "int".Int

 function to_rep 
   (x : Standard__positive.positive) : int =
  (Standard__positive.positive'int x)
 
 clone export "ada__model".Rep_Proj_Int with axiom .,
 type t = Standard__positive.positive, 
 predicate in_range = Standard__positive.in_range, 
 function to_rep = to_rep
 
 meta "model_projection" function to_rep
 
 meta "inline:no" function to_rep

end

(* Module for axiomatizing type "short_float", created in Gnat2Why.Types.Translate_Type *)
module Standard__short_float
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "_gnatprove_standard".Float32 as Float32

 type short_float 
 
 val constant first 
   : Float32.t
  ensures { result =  (Float32.neg (340282346638528859811704183484516925440.0:Float32.t)) }
 
 val constant last 
   : Float32.t
  ensures { result =  (340282346638528859811704183484516925440.0:Float32.t) }
 
 predicate in_range 
   (x : Float32.t) =
  (Float32.t'isFinite x)
 val in_range 
   (x : Float32.t) : bool
  ensures { result <-> in_range (x : Float32.t) }
 
 clone export "ada__model".Static_Float32 with axiom .,
 type t = short_float, 
 function first = first, 
 function last = last, 
 predicate in_range = in_range
 
 type short_float__ref =
  { mutable short_float__content : short_float }
 
 function short_float__ref_short_float__content__projection 
   (a : short_float__ref) : short_float =
  a.short_float__content
 
 meta "model_projection" function short_float__ref_short_float__content__projection
 
 meta "inline:no" function short_float__ref_short_float__content__projection
 
 val short_float__havoc 
   (x : short_float__ref) : unit
  writes {x}

end

(* Module defining to_rep/of_rep for type "short_float", created in Gnat2Why.Types.Translate_Type *)
module Standard__short_float__rep
 use        Standard__short_float as Standard__short_float
 use        "_gnatprove_standard".Float32 as Float32
 use        "_gnatprove_standard".Main
 use        "int".Int

 clone export "ada__model".Rep_Proj_Float32 with axiom .,
 type t = Standard__short_float.short_float, 
 predicate in_range = Standard__short_float.in_range
 
 meta "model_projection" function to_rep
 
 meta "inline:no" function to_rep

end

(* Module for axiomatizing type "float", created in Gnat2Why.Types.Translate_Type *)
module Standard__float
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "_gnatprove_standard".Float32 as Float32

 type float__ 
 
 val constant first 
   : Float32.t
  ensures { result =  (Float32.neg (340282346638528859811704183484516925440.0:Float32.t)) }
 
 val constant last 
   : Float32.t
  ensures { result =  (340282346638528859811704183484516925440.0:Float32.t) }
 
 predicate in_range 
   (x : Float32.t) =
  (Float32.t'isFinite x)
 val in_range 
   (x : Float32.t) : bool
  ensures { result <-> in_range (x : Float32.t) }
 
 clone export "ada__model".Static_Float32 with axiom .,
 type t = float__, 
 function first = first, 
 function last = last, 
 predicate in_range = in_range
 
 type float____ref =
  { mutable float____content : float__ }
 
 function float____ref_float____content__projection 
   (a : float____ref) : float__ =
  a.float____content
 
 meta "model_projection" function float____ref_float____content__projection
 
 meta "inline:no" function float____ref_float____content__projection
 
 val float____havoc 
   (x : float____ref) : unit
  writes {x}

end

(* Module defining to_rep/of_rep for type "float", created in Gnat2Why.Types.Translate_Type *)
module Standard__float__rep
 use        Standard__float as Standard__float
 use        "_gnatprove_standard".Float32 as Float32
 use        "_gnatprove_standard".Main
 use        "int".Int

 clone export "ada__model".Rep_Proj_Float32 with axiom .,
 type t = Standard__float.float__, 
 predicate in_range = Standard__float.in_range
 
 meta "model_projection" function to_rep
 
 meta "inline:no" function to_rep

end

(* Module for axiomatizing type "long_float", created in Gnat2Why.Types.Translate_Type *)
module Standard__long_float
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "_gnatprove_standard".Float64 as Float64

 type long_float 
 
 val constant first 
   : Float64.t
  ensures { result =  (Float64.neg (179769313486231570814527423731704356798070567525844996598917476803157260780028538760589558632766878171540458953514382464234321326889464182768467546703537516986049910576551282076245490090389328944075868508455133942304583236903222948165808559332123348274797826204144723168738177180919299881250404026184124858368.0:Float64.t)) }
 
 val constant last 
   : Float64.t
  ensures { result =  (179769313486231570814527423731704356798070567525844996598917476803157260780028538760589558632766878171540458953514382464234321326889464182768467546703537516986049910576551282076245490090389328944075868508455133942304583236903222948165808559332123348274797826204144723168738177180919299881250404026184124858368.0:Float64.t) }
 
 predicate in_range 
   (x : Float64.t) =
  (Float64.t'isFinite x)
 val in_range 
   (x : Float64.t) : bool
  ensures { result <-> in_range (x : Float64.t) }
 
 clone export "ada__model".Static_Float64 with axiom .,
 type t = long_float, 
 function first = first, 
 function last = last, 
 predicate in_range = in_range
 
 type long_float__ref =
  { mutable long_float__content : long_float }
 
 function long_float__ref_long_float__content__projection 
   (a : long_float__ref) : long_float =
  a.long_float__content
 
 meta "model_projection" function long_float__ref_long_float__content__projection
 
 meta "inline:no" function long_float__ref_long_float__content__projection
 
 val long_float__havoc 
   (x : long_float__ref) : unit
  writes {x}

end

(* Module defining to_rep/of_rep for type "long_float", created in Gnat2Why.Types.Translate_Type *)
module Standard__long_float__rep
 use        Standard__long_float as Standard__long_float
 use        "_gnatprove_standard".Float64 as Float64
 use        "_gnatprove_standard".Main
 use        "int".Int

 clone export "ada__model".Rep_Proj_Float64 with axiom .,
 type t = Standard__long_float.long_float, 
 predicate in_range = Standard__long_float.in_range
 
 meta "model_projection" function to_rep
 
 meta "inline:no" function to_rep

end

(* Module for axiomatizing type "character", created in Gnat2Why.Types.Translate_Type *)
module Standard__character
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int

 type character 
 
 val constant first 
   : int
  ensures { result =  (0 : int) }
 
 val constant last 
   : int
  ensures { result =  (255 : int) }
 
 predicate in_range 
   (x : int) =
  ( (first <= x) /\ (x <= last) )
 val in_range 
   (x : int) : bool
  ensures { result <-> in_range (x : int) }
 
 clone export "ada__model".Static_Discrete with axiom .,
 type t = character, 
 function first = first, 
 function last = last, 
 predicate in_range = in_range
 
 type character__ref =
  { mutable character__content : character }
 
 function character__ref_character__content__projection 
   (a : character__ref) : character =
  a.character__content
 
 meta "model_projection" function character__ref_character__content__projection
 
 meta "inline:no" function character__ref_character__content__projection
 
 val character__havoc 
   (x : character__ref) : unit
  writes {x}

end

(* Module defining to_rep/of_rep for type "character", created in Gnat2Why.Types.Translate_Type *)
module Standard__character__rep
 use        Standard__character as Standard__character
 use        "_gnatprove_standard".Main
 use        "int".Int

 clone export "ada__model".Rep_Proj_Int with axiom .,
 type t = Standard__character.character, 
 predicate in_range = Standard__character.in_range
 
 meta "model_projection" function to_rep
 
 meta "inline:no" function to_rep

end

(* Module for axiomatizing type "wide_character", created in Gnat2Why.Types.Translate_Type *)
module Standard__wide_character
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int

 type wide_character 
 
 val constant first 
   : int
  ensures { result =  (0 : int) }
 
 val constant last 
   : int
  ensures { result =  (65535 : int) }
 
 predicate in_range 
   (x : int) =
  ( (first <= x) /\ (x <= last) )
 val in_range 
   (x : int) : bool
  ensures { result <-> in_range (x : int) }
 
 clone export "ada__model".Static_Discrete with axiom .,
 type t = wide_character, 
 function first = first, 
 function last = last, 
 predicate in_range = in_range
 
 type wide_character__ref =
  { mutable wide_character__content : wide_character }
 
 function wide_character__ref_wide_character__content__projection 
   (a : wide_character__ref) : wide_character =
  a.wide_character__content
 
 meta "model_projection" function wide_character__ref_wide_character__content__projection
 
 meta "inline:no" function wide_character__ref_wide_character__content__projection
 
 val wide_character__havoc 
   (x : wide_character__ref) : unit
  writes {x}

end

(* Module defining to_rep/of_rep for type "wide_character", created in Gnat2Why.Types.Translate_Type *)
module Standard__wide_character__rep
 use        Standard__wide_character as Standard__wide_character
 use        "_gnatprove_standard".Main
 use        "int".Int

 clone export "ada__model".Rep_Proj_Int with axiom .,
 type t = Standard__wide_character.wide_character, 
 predicate in_range = Standard__wide_character.in_range
 
 meta "model_projection" function to_rep
 
 meta "inline:no" function to_rep

end

(* Module for axiomatizing type "wide_wide_character", created in Gnat2Why.Types.Translate_Type *)
module Standard__wide_wide_character
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int

 type wide_wide_character 
 
 val constant first 
   : int
  ensures { result =  (0 : int) }
 
 val constant last 
   : int
  ensures { result =  (2147483647 : int) }
 
 predicate in_range 
   (x : int) =
  ( (first <= x) /\ (x <= last) )
 val in_range 
   (x : int) : bool
  ensures { result <-> in_range (x : int) }
 
 clone export "ada__model".Static_Discrete with axiom .,
 type t = wide_wide_character, 
 function first = first, 
 function last = last, 
 predicate in_range = in_range
 
 type wide_wide_character__ref =
  { mutable wide_wide_character__content : wide_wide_character }
 
 function wide_wide_character__ref_wide_wide_character__content__projection 
   (a : wide_wide_character__ref) : wide_wide_character =
  a.wide_wide_character__content
 
 meta "model_projection" function wide_wide_character__ref_wide_wide_character__content__projection
 
 meta "inline:no" function wide_wide_character__ref_wide_wide_character__content__projection
 
 val wide_wide_character__havoc 
   (x : wide_wide_character__ref) : unit
  writes {x}

end

(* Module defining to_rep/of_rep for type "wide_wide_character", created in Gnat2Why.Types.Translate_Type *)
module Standard__wide_wide_character__rep
 use        Standard__wide_wide_character as Standard__wide_wide_character
 use        "_gnatprove_standard".Main
 use        "int".Int

 clone export "ada__model".Rep_Proj_Int with axiom .,
 type t = Standard__wide_wide_character.wide_wide_character, 
 predicate in_range = Standard__wide_wide_character.in_range
 
 meta "model_projection" function to_rep
 
 meta "inline:no" function to_rep

end

(* Module for axiomatizing the array theory associated to type "string", created in Why.Gen.Arrays.Create_Rep_Array_Theory *)
module Array__Int__Standard__character
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int
 use        Standard__character as Standard__character
 use        Standard__character__rep as Standard__character__rep

 function index_I1_one 
   : int =
  (1 : int)
 
 type component_type =
  Standard__character.character
 
 clone export "_gnatprove_standard".Array__1 with axiom .,
 type I1.t = int, 
 predicate I1.le = Int.(<=), 
 predicate I1.lt = Int.(<), 
 predicate I1.gt = Int.(>), 
 function I1.add = Int.(+), 
 function I1.sub = Int.(-), 
 function I1.one = index_I1_one, 
 type component_type = component_type
 
 function bool_eq 
   (a : map) (a__first : int) (a__last : int) (b : map) (b__first : int) (b__last : int) : bool =
  ( (if ((a__first <= a__last)) then (
   ( (b__first <= b__last) /\ ((a__last - a__first) = (b__last - b__first)) )) else (
   (b__first > b__last))) /\ (forall temp___idx_91   : int.
   (if (( (a__first <= temp___idx_91) /\ (temp___idx_91 <= a__last) )) then (
    ((Standard__character__rep.to_rep (get a temp___idx_91)) = (Standard__character__rep.to_rep (get b ((b__first - a__first) + temp___idx_91))))) else true)) )
 val bool_eq 
   (a : map) (a__first : int) (a__last : int) (b : map) (b__first : int) (b__last : int) : bool
  ensures { result = bool_eq (a : map) (a__first : int) (a__last : int) (b : map) (b__first : int) (b__last : int) }
 
 axiom bool_eq_rev :
  (forall a   b   : map.
  (forall a__first   a__last   b__first   b__last   : int.
   ( ((bool_eq b b__first b__last a a__first a__last) = True) -> ( (if ((a__first <= a__last)) then (
    ( (b__first <= b__last) /\ ((a__last - a__first) = (b__last - b__first)) )) else (
    (b__first > b__last))) /\ (forall temp___idx_91   : int.
    (if (( (a__first <= temp___idx_91) /\ (temp___idx_91 <= a__last) )) then (
     ((Standard__character__rep.to_rep (get a temp___idx_91)) = (Standard__character__rep.to_rep (get b ((b__first - a__first) + temp___idx_91))))) else true)) ) )))

end

(* Module for axiomatizing concatenation for the array theory associated to type "string", created in Why.Gen.Arrays.Declare_Concatenation_Symbols *)
module Array__Int__Standard__character__Concat
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int
 use        Array__Int__Standard__character as Array__Int__Standard__character

 function index_Index_one 
   : int =
  (1 : int)
 
 clone export "_gnatprove_standard".Array__1__Concat with axiom .,
 type component_type = Array__Int__Standard__character.component_type, 
 type map = Array__Int__Standard__character.map, 
 type Index.t = int, 
 predicate Index.le = Int.(<=), 
 predicate Index.lt = Int.(<), 
 predicate Index.gt = Int.(>), 
 function Index.add = Int.(+), 
 function Index.sub = Int.(-), 
 function Index.one = index_Index_one, 
 function get = Array__Int__Standard__character.get

end

(* Module for axiomatizing comparison for the array theory associated to type "string", created in Why.Gen.Arrays.Declare_Comparison_Symbols *)
module Array__Int__Standard__character_Comp
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int
 use        Standard__character__rep as Standard__character__rep
 use        Array__Int__Standard__character as Array__Int__Standard__character

 function index_Index_one 
   : int =
  (1 : int)
 
 clone export "ada__model".Array_Int_Rep_Comparison_Axiom with axiom .,
 type component_type = Array__Int__Standard__character.component_type, 
 function to_rep = Standard__character__rep.to_rep, 
 type map = Array__Int__Standard__character.map, 
 type Index.t = int, 
 predicate Index.le = Int.(<=), 
 predicate Index.lt = Int.(<), 
 predicate Index.gt = Int.(>), 
 function Index.add = Int.(+), 
 function Index.sub = Int.(-), 
 function Index.one = index_Index_one, 
 function get = Array__Int__Standard__character.get, 
 function bool_eq = Array__Int__Standard__character.bool_eq

end

(* Module for axiomatizing type "string", created in Gnat2Why.Types.Translate_Type *)
module Standard__string
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int
 use        Standard__integer as Standard__integer
 use        Standard__positive as Standard__positive
 use        Standard__character as Standard__character
 use        Array__Int__Standard__character as Array__Int__Standard__character
 use        Standard__integer__rep as Standard__integer__rep

 type component_type =
  Standard__character.character
 
 function index_1_id 
   (x : int) : int =
  x
 
 clone export "ada__model".Unconstr_Array with axiom .,
 type map = Array__Int__Standard__character.map, 
 function array_bool_eq = Array__Int__Standard__character.bool_eq, 
 type index_base_type = Standard__integer.integer, 
 type index_rep_type = int, 
 function to_rep = Standard__integer__rep.to_rep, 
 function rep_to_int = index_1_id, 
 predicate in_range_base = Standard__integer.in_range, 
 predicate index_dynamic_property = Standard__positive.dynamic_property, 
 predicate index_rep_le = Int.(<=)
 
 type string =
  __t
 
 meta "model_projection" function to_array
 
 meta "inline:no" function to_array
 
 meta "model_projection" function first
 
 meta "inline:no" function first
 
 meta "model_projection" function last
 
 meta "inline:no" function last
 
 type string__ref =
  { mutable string__content : string }
 
 function string__ref_string__content__projection 
   (a : string__ref) : string =
  a.string__content
 
 meta "model_projection" function string__ref_string__content__projection
 
 meta "inline:no" function string__ref_string__content__projection
 
 val string__havoc 
   (x : string__ref) : unit
  writes {x}

end

(* Module defining to_string/of_string functions, created in Gnat2Why.Types.Translate_Type *)
module Standard_String__Img
 use        "int".Int
 use        Standard__string as Standard__string
 use        "_gnatprove_standard".Main
 use        "int".Int

 val function to_string 
   (x : Main.__image) (s : int) : Standard__string.string
 
 val function from_string 
   (x : Standard__string.string) : Main.__image
 
 axiom to_string__first :
  (forall x   : Main.__image.
  (forall s   : int [(to_string x s)].
   ((Standard__string.first (to_string x s)) = (1 : int))))
 
 axiom to_string__length :
  (forall x   : Main.__image.
  (forall s   : int [(to_string x s)].
   ( (s >= (0 : int)) -> ((Standard__string.length (to_string x s)) <= s) )))

end

(* Module for axiomatizing the array theory associated to type "wide_string", created in Why.Gen.Arrays.Create_Rep_Array_Theory *)
module Array__Int__Standard__wide_character
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int
 use        Standard__wide_character as Standard__wide_character
 use        Standard__wide_character__rep as Standard__wide_character__rep

 function index_I1_one 
   : int =
  (1 : int)
 
 type component_type =
  Standard__wide_character.wide_character
 
 clone export "_gnatprove_standard".Array__1 with axiom .,
 type I1.t = int, 
 predicate I1.le = Int.(<=), 
 predicate I1.lt = Int.(<), 
 predicate I1.gt = Int.(>), 
 function I1.add = Int.(+), 
 function I1.sub = Int.(-), 
 function I1.one = index_I1_one, 
 type component_type = component_type
 
 function bool_eq 
   (a : map) (a__first : int) (a__last : int) (b : map) (b__first : int) (b__last : int) : bool =
  ( (if ((a__first <= a__last)) then (
   ( (b__first <= b__last) /\ ((a__last - a__first) = (b__last - b__first)) )) else (
   (b__first > b__last))) /\ (forall temp___idx_98   : int.
   (if (( (a__first <= temp___idx_98) /\ (temp___idx_98 <= a__last) )) then (
    ((Standard__wide_character__rep.to_rep (get a temp___idx_98)) = (Standard__wide_character__rep.to_rep (get b ((b__first - a__first) + temp___idx_98))))) else true)) )
 val bool_eq 
   (a : map) (a__first : int) (a__last : int) (b : map) (b__first : int) (b__last : int) : bool
  ensures { result = bool_eq (a : map) (a__first : int) (a__last : int) (b : map) (b__first : int) (b__last : int) }
 
 axiom bool_eq_rev :
  (forall a   b   : map.
  (forall a__first   a__last   b__first   b__last   : int.
   ( ((bool_eq b b__first b__last a a__first a__last) = True) -> ( (if ((a__first <= a__last)) then (
    ( (b__first <= b__last) /\ ((a__last - a__first) = (b__last - b__first)) )) else (
    (b__first > b__last))) /\ (forall temp___idx_98   : int.
    (if (( (a__first <= temp___idx_98) /\ (temp___idx_98 <= a__last) )) then (
     ((Standard__wide_character__rep.to_rep (get a temp___idx_98)) = (Standard__wide_character__rep.to_rep (get b ((b__first - a__first) + temp___idx_98))))) else true)) ) )))

end

(* Module for axiomatizing concatenation for the array theory associated to type "wide_string", created in Why.Gen.Arrays.Declare_Concatenation_Symbols *)
module Array__Int__Standard__wide_character__Concat
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int
 use        Array__Int__Standard__wide_character as Array__Int__Standard__wide_character

 function index_Index_one 
   : int =
  (1 : int)
 
 clone export "_gnatprove_standard".Array__1__Concat with axiom .,
 type component_type = Array__Int__Standard__wide_character.component_type, 
 type map = Array__Int__Standard__wide_character.map, 
 type Index.t = int, 
 predicate Index.le = Int.(<=), 
 predicate Index.lt = Int.(<), 
 predicate Index.gt = Int.(>), 
 function Index.add = Int.(+), 
 function Index.sub = Int.(-), 
 function Index.one = index_Index_one, 
 function get = Array__Int__Standard__wide_character.get

end

(* Module for axiomatizing comparison for the array theory associated to type "wide_string", created in Why.Gen.Arrays.Declare_Comparison_Symbols *)
module Array__Int__Standard__wide_character_Comp
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int
 use        Standard__wide_character__rep as Standard__wide_character__rep
 use        Array__Int__Standard__wide_character as Array__Int__Standard__wide_character

 function index_Index_one 
   : int =
  (1 : int)
 
 clone export "ada__model".Array_Int_Rep_Comparison_Axiom with axiom .,
 type component_type = Array__Int__Standard__wide_character.component_type, 
 function to_rep = Standard__wide_character__rep.to_rep, 
 type map = Array__Int__Standard__wide_character.map, 
 type Index.t = int, 
 predicate Index.le = Int.(<=), 
 predicate Index.lt = Int.(<), 
 predicate Index.gt = Int.(>), 
 function Index.add = Int.(+), 
 function Index.sub = Int.(-), 
 function Index.one = index_Index_one, 
 function get = Array__Int__Standard__wide_character.get, 
 function bool_eq = Array__Int__Standard__wide_character.bool_eq

end

(* Module for axiomatizing type "wide_string", created in Gnat2Why.Types.Translate_Type *)
module Standard__wide_string
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int
 use        Standard__integer as Standard__integer
 use        Standard__positive as Standard__positive
 use        Standard__wide_character as Standard__wide_character
 use        Standard__integer__rep as Standard__integer__rep
 use        Array__Int__Standard__wide_character as Array__Int__Standard__wide_character

 type component_type =
  Standard__wide_character.wide_character
 
 function index_1_id 
   (x : int) : int =
  x
 
 clone export "ada__model".Unconstr_Array with axiom .,
 type map = Array__Int__Standard__wide_character.map, 
 function array_bool_eq = Array__Int__Standard__wide_character.bool_eq, 
 type index_base_type = Standard__integer.integer, 
 type index_rep_type = int, 
 function to_rep = Standard__integer__rep.to_rep, 
 function rep_to_int = index_1_id, 
 predicate in_range_base = Standard__integer.in_range, 
 predicate index_dynamic_property = Standard__positive.dynamic_property, 
 predicate index_rep_le = Int.(<=)
 
 type wide_string =
  __t
 
 meta "model_projection" function to_array
 
 meta "inline:no" function to_array
 
 meta "model_projection" function first
 
 meta "inline:no" function first
 
 meta "model_projection" function last
 
 meta "inline:no" function last
 
 type wide_string__ref =
  { mutable wide_string__content : wide_string }
 
 function wide_string__ref_wide_string__content__projection 
   (a : wide_string__ref) : wide_string =
  a.wide_string__content
 
 meta "model_projection" function wide_string__ref_wide_string__content__projection
 
 meta "inline:no" function wide_string__ref_wide_string__content__projection
 
 val wide_string__havoc 
   (x : wide_string__ref) : unit
  writes {x}

end

(* Module for axiomatizing the array theory associated to type "wide_wide_string", created in Why.Gen.Arrays.Create_Rep_Array_Theory *)
module Array__Int__Standard__wide_wide_character
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int
 use        Standard__wide_wide_character as Standard__wide_wide_character
 use        Standard__wide_wide_character__rep as Standard__wide_wide_character__rep

 function index_I1_one 
   : int =
  (1 : int)
 
 type component_type =
  Standard__wide_wide_character.wide_wide_character
 
 clone export "_gnatprove_standard".Array__1 with axiom .,
 type I1.t = int, 
 predicate I1.le = Int.(<=), 
 predicate I1.lt = Int.(<), 
 predicate I1.gt = Int.(>), 
 function I1.add = Int.(+), 
 function I1.sub = Int.(-), 
 function I1.one = index_I1_one, 
 type component_type = component_type
 
 function bool_eq 
   (a : map) (a__first : int) (a__last : int) (b : map) (b__first : int) (b__last : int) : bool =
  ( (if ((a__first <= a__last)) then (
   ( (b__first <= b__last) /\ ((a__last - a__first) = (b__last - b__first)) )) else (
   (b__first > b__last))) /\ (forall temp___idx_105   : int.
   (if (( (a__first <= temp___idx_105) /\ (temp___idx_105 <= a__last) )) then (
    ((Standard__wide_wide_character__rep.to_rep (get a temp___idx_105)) = (Standard__wide_wide_character__rep.to_rep (get b ((b__first - a__first) + temp___idx_105))))) else true)) )
 val bool_eq 
   (a : map) (a__first : int) (a__last : int) (b : map) (b__first : int) (b__last : int) : bool
  ensures { result = bool_eq (a : map) (a__first : int) (a__last : int) (b : map) (b__first : int) (b__last : int) }
 
 axiom bool_eq_rev :
  (forall a   b   : map.
  (forall a__first   a__last   b__first   b__last   : int.
   ( ((bool_eq b b__first b__last a a__first a__last) = True) -> ( (if ((a__first <= a__last)) then (
    ( (b__first <= b__last) /\ ((a__last - a__first) = (b__last - b__first)) )) else (
    (b__first > b__last))) /\ (forall temp___idx_105   : int.
    (if (( (a__first <= temp___idx_105) /\ (temp___idx_105 <= a__last) )) then (
     ((Standard__wide_wide_character__rep.to_rep (get a temp___idx_105)) = (Standard__wide_wide_character__rep.to_rep (get b ((b__first - a__first) + temp___idx_105))))) else true)) ) )))

end

(* Module for axiomatizing concatenation for the array theory associated to type "wide_wide_string", created in Why.Gen.Arrays.Declare_Concatenation_Symbols *)
module Array__Int__Standard__wide_wide_character__Concat
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int
 use        Array__Int__Standard__wide_wide_character as Array__Int__Standard__wide_wide_character

 function index_Index_one 
   : int =
  (1 : int)
 
 clone export "_gnatprove_standard".Array__1__Concat with axiom .,
 type component_type = Array__Int__Standard__wide_wide_character.component_type, 
 type map = Array__Int__Standard__wide_wide_character.map, 
 type Index.t = int, 
 predicate Index.le = Int.(<=), 
 predicate Index.lt = Int.(<), 
 predicate Index.gt = Int.(>), 
 function Index.add = Int.(+), 
 function Index.sub = Int.(-), 
 function Index.one = index_Index_one, 
 function get = Array__Int__Standard__wide_wide_character.get

end

(* Module for axiomatizing comparison for the array theory associated to type "wide_wide_string", created in Why.Gen.Arrays.Declare_Comparison_Symbols *)
module Array__Int__Standard__wide_wide_character_Comp
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int
 use        Standard__wide_wide_character__rep as Standard__wide_wide_character__rep
 use        Array__Int__Standard__wide_wide_character as Array__Int__Standard__wide_wide_character

 function index_Index_one 
   : int =
  (1 : int)
 
 clone export "ada__model".Array_Int_Rep_Comparison_Axiom with axiom .,
 type component_type = Array__Int__Standard__wide_wide_character.component_type, 
 function to_rep = Standard__wide_wide_character__rep.to_rep, 
 type map = Array__Int__Standard__wide_wide_character.map, 
 type Index.t = int, 
 predicate Index.le = Int.(<=), 
 predicate Index.lt = Int.(<), 
 predicate Index.gt = Int.(>), 
 function Index.add = Int.(+), 
 function Index.sub = Int.(-), 
 function Index.one = index_Index_one, 
 function get = Array__Int__Standard__wide_wide_character.get, 
 function bool_eq = Array__Int__Standard__wide_wide_character.bool_eq

end

(* Module for axiomatizing type "wide_wide_string", created in Gnat2Why.Types.Translate_Type *)
module Standard__wide_wide_string
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int
 use        Standard__integer as Standard__integer
 use        Standard__positive as Standard__positive
 use        Standard__wide_wide_character as Standard__wide_wide_character
 use        Standard__integer__rep as Standard__integer__rep
 use        Array__Int__Standard__wide_wide_character as Array__Int__Standard__wide_wide_character

 type component_type =
  Standard__wide_wide_character.wide_wide_character
 
 function index_1_id 
   (x : int) : int =
  x
 
 clone export "ada__model".Unconstr_Array with axiom .,
 type map = Array__Int__Standard__wide_wide_character.map, 
 function array_bool_eq = Array__Int__Standard__wide_wide_character.bool_eq, 
 type index_base_type = Standard__integer.integer, 
 type index_rep_type = int, 
 function to_rep = Standard__integer__rep.to_rep, 
 function rep_to_int = index_1_id, 
 predicate in_range_base = Standard__integer.in_range, 
 predicate index_dynamic_property = Standard__positive.dynamic_property, 
 predicate index_rep_le = Int.(<=)
 
 type wide_wide_string =
  __t
 
 meta "model_projection" function to_array
 
 meta "inline:no" function to_array
 
 meta "model_projection" function first
 
 meta "inline:no" function first
 
 meta "model_projection" function last
 
 meta "inline:no" function last
 
 type wide_wide_string__ref =
  { mutable wide_wide_string__content : wide_wide_string }
 
 function wide_wide_string__ref_wide_wide_string__content__projection 
   (a : wide_wide_string__ref) : wide_wide_string =
  a.wide_wide_string__content
 
 meta "model_projection" function wide_wide_string__ref_wide_wide_string__content__projection
 
 meta "inline:no" function wide_wide_string__ref_wide_wide_string__content__projection
 
 val wide_wide_string__havoc 
   (x : wide_wide_string__ref) : unit
  writes {x}

end

(* Module for fixed-point operation for type at system.ads:1, created in Why.Gen.Scalars.Create_Fixed_Point_Theory_If_Needed *)
module Fixed_Point__1_1000000000
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int

 function num_small 
   : int =
  (1 : int)
 
 function den_small 
   : int =
  (1000000000 : int)
 
 clone export "ada__model".Fixed_Point_Rep with axiom .,
 function num_small = num_small, 
 function den_small = den_small

end

(* Module for axiomatizing type "duration", created in Gnat2Why.Types.Translate_Type *)
module Standard__duration
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int
 use        "_gnatprove_standard".Main as Main

 type duration 
 
 val constant num_small 
   : Main.__fixed
  ensures { result =  (1 : int) }
 
 val constant den_small 
   : Main.__fixed
  ensures { result =  (1000000000 : int) }
 
 val constant first 
   : Main.__fixed
  ensures { result =  ( ( -9223372036854775808 ) : int ) }
 
 val constant last 
   : Main.__fixed
  ensures { result =  ( 9223372036854775807 : int ) }
 
 predicate in_range 
   (x : Main.__fixed) =
  ( (first <= x) /\ (x <= last) )
 val in_range 
   (x : Main.__fixed) : bool
  ensures { result <-> in_range (x : Main.__fixed) }
 
 clone export "ada__model".Static_Fixed_Point with axiom .,
 type t = duration, 
 function first = first, 
 function last = last, 
 predicate in_range = in_range
 
 type duration__ref =
  { mutable duration__content : duration }
 
 function duration__ref_duration__content__projection 
   (a : duration__ref) : duration =
  a.duration__content
 
 meta "model_projection" function duration__ref_duration__content__projection
 
 meta "inline:no" function duration__ref_duration__content__projection
 
 val duration__havoc 
   (x : duration__ref) : unit
  writes {x}

end

(* Module defining to_rep/of_rep for type "duration", created in Gnat2Why.Types.Translate_Type *)
module Standard__duration__rep
 use        Standard__duration as Standard__duration
 use        "_gnatprove_standard".Main
 use        "int".Int

 clone export "ada__model".Rep_Proj_Fixed with axiom .,
 type t = Standard__duration.duration, 
 predicate in_range = Standard__duration.in_range
 
 meta "model_projection" function to_rep
 
 meta "inline:no" function to_rep

end

(* Module for axiomatizing type "integer_8", created in Gnat2Why.Types.Translate_Type *)
module Standard__integer_8
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int

 type integer_8 =
  < range -128 127 >
 
 val constant first 
   : int
  ensures { result =  (( -128) : int) }
 
 val constant last 
   : int
  ensures { result =  (127 : int) }
 
 predicate in_range 
   (x : int) =
  ( (first <= x) /\ (x <= last) )
 val in_range 
   (x : int) : bool
  ensures { result <-> in_range (x : int) }
 
 clone export "ada__model".Static_Discrete with axiom .,
 type t = integer_8, 
 function first = first, 
 function last = last, 
 predicate in_range = in_range
 
 type integer_8__ref =
  { mutable integer_8__content : integer_8 }
 
 function integer_8__ref_integer_8__content__projection 
   (a : integer_8__ref) : integer_8 =
  a.integer_8__content
 
 meta "model_projection" function integer_8__ref_integer_8__content__projection
 
 meta "inline:no" function integer_8__ref_integer_8__content__projection
 
 val integer_8__havoc 
   (x : integer_8__ref) : unit
  writes {x}

end

(* Module defining to_rep/of_rep for type "integer_8", created in Gnat2Why.Types.Translate_Type *)
module Standard__integer_8__rep
 use        Standard__integer_8 as Standard__integer_8
 use        "_gnatprove_standard".Main
 use        "int".Int

 function to_rep 
   (x : Standard__integer_8.integer_8) : int =
  (Standard__integer_8.integer_8'int x)
 
 clone export "ada__model".Rep_Proj_Int with axiom .,
 type t = Standard__integer_8.integer_8, 
 predicate in_range = Standard__integer_8.in_range, 
 function to_rep = to_rep
 
 meta "model_projection" function to_rep
 
 meta "inline:no" function to_rep

end

(* Module for axiomatizing type "integer_16", created in Gnat2Why.Types.Translate_Type *)
module Standard__integer_16
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int

 type integer_16 =
  < range -32768 32767 >
 
 val constant first 
   : int
  ensures { result =  (( -32768) : int) }
 
 val constant last 
   : int
  ensures { result =  (32767 : int) }
 
 predicate in_range 
   (x : int) =
  ( (first <= x) /\ (x <= last) )
 val in_range 
   (x : int) : bool
  ensures { result <-> in_range (x : int) }
 
 clone export "ada__model".Static_Discrete with axiom .,
 type t = integer_16, 
 function first = first, 
 function last = last, 
 predicate in_range = in_range
 
 type integer_16__ref =
  { mutable integer_16__content : integer_16 }
 
 function integer_16__ref_integer_16__content__projection 
   (a : integer_16__ref) : integer_16 =
  a.integer_16__content
 
 meta "model_projection" function integer_16__ref_integer_16__content__projection
 
 meta "inline:no" function integer_16__ref_integer_16__content__projection
 
 val integer_16__havoc 
   (x : integer_16__ref) : unit
  writes {x}

end

(* Module defining to_rep/of_rep for type "integer_16", created in Gnat2Why.Types.Translate_Type *)
module Standard__integer_16__rep
 use        Standard__integer_16 as Standard__integer_16
 use        "_gnatprove_standard".Main
 use        "int".Int

 function to_rep 
   (x : Standard__integer_16.integer_16) : int =
  (Standard__integer_16.integer_16'int x)
 
 clone export "ada__model".Rep_Proj_Int with axiom .,
 type t = Standard__integer_16.integer_16, 
 predicate in_range = Standard__integer_16.in_range, 
 function to_rep = to_rep
 
 meta "model_projection" function to_rep
 
 meta "inline:no" function to_rep

end

(* Module for axiomatizing type "integer_32", created in Gnat2Why.Types.Translate_Type *)
module Standard__integer_32
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int

 type integer_32 =
  < range -2147483648 2147483647 >
 
 val constant first 
   : int
  ensures { result =  (( -2147483648) : int) }
 
 val constant last 
   : int
  ensures { result =  (2147483647 : int) }
 
 predicate in_range 
   (x : int) =
  ( (first <= x) /\ (x <= last) )
 val in_range 
   (x : int) : bool
  ensures { result <-> in_range (x : int) }
 
 clone export "ada__model".Static_Discrete with axiom .,
 type t = integer_32, 
 function first = first, 
 function last = last, 
 predicate in_range = in_range
 
 type integer_32__ref =
  { mutable integer_32__content : integer_32 }
 
 function integer_32__ref_integer_32__content__projection 
   (a : integer_32__ref) : integer_32 =
  a.integer_32__content
 
 meta "model_projection" function integer_32__ref_integer_32__content__projection
 
 meta "inline:no" function integer_32__ref_integer_32__content__projection
 
 val integer_32__havoc 
   (x : integer_32__ref) : unit
  writes {x}

end

(* Module defining to_rep/of_rep for type "integer_32", created in Gnat2Why.Types.Translate_Type *)
module Standard__integer_32__rep
 use        Standard__integer_32 as Standard__integer_32
 use        "_gnatprove_standard".Main
 use        "int".Int

 function to_rep 
   (x : Standard__integer_32.integer_32) : int =
  (Standard__integer_32.integer_32'int x)
 
 clone export "ada__model".Rep_Proj_Int with axiom .,
 type t = Standard__integer_32.integer_32, 
 predicate in_range = Standard__integer_32.in_range, 
 function to_rep = to_rep
 
 meta "model_projection" function to_rep
 
 meta "inline:no" function to_rep

end

(* Module for axiomatizing type "integer_64", created in Gnat2Why.Types.Translate_Type *)
module Standard__integer_64
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int

 type integer_64 =
  < range -9223372036854775808 9223372036854775807 >
 
 val constant first 
   : int
  ensures { result =  (( -9223372036854775808) : int) }
 
 val constant last 
   : int
  ensures { result =  (9223372036854775807 : int) }
 
 predicate in_range 
   (x : int) =
  ( (first <= x) /\ (x <= last) )
 val in_range 
   (x : int) : bool
  ensures { result <-> in_range (x : int) }
 
 clone export "ada__model".Static_Discrete with axiom .,
 type t = integer_64, 
 function first = first, 
 function last = last, 
 predicate in_range = in_range
 
 type integer_64__ref =
  { mutable integer_64__content : integer_64 }
 
 function integer_64__ref_integer_64__content__projection 
   (a : integer_64__ref) : integer_64 =
  a.integer_64__content
 
 meta "model_projection" function integer_64__ref_integer_64__content__projection
 
 meta "inline:no" function integer_64__ref_integer_64__content__projection
 
 val integer_64__havoc 
   (x : integer_64__ref) : unit
  writes {x}

end

(* Module defining to_rep/of_rep for type "integer_64", created in Gnat2Why.Types.Translate_Type *)
module Standard__integer_64__rep
 use        Standard__integer_64 as Standard__integer_64
 use        "_gnatprove_standard".Main
 use        "int".Int

 function to_rep 
   (x : Standard__integer_64.integer_64) : int =
  (Standard__integer_64.integer_64'int x)
 
 clone export "ada__model".Rep_Proj_Int with axiom .,
 type t = Standard__integer_64.integer_64, 
 predicate in_range = Standard__integer_64.in_range, 
 function to_rep = to_rep
 
 meta "model_projection" function to_rep
 
 meta "inline:no" function to_rep

end

(* Module for axiomatizing type "universal_integer", created in Gnat2Why.Types.Translate_Type *)
module Standard__universal_integer
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int

 type universal_integer =
  < range -9223372036854775808 9223372036854775807 >
 
 val constant first 
   : int
  ensures { result =  (( -9223372036854775808) : int) }
 
 val constant last 
   : int
  ensures { result =  (9223372036854775807 : int) }
 
 predicate in_range 
   (x : int) =
  ( (first <= x) /\ (x <= last) )
 val in_range 
   (x : int) : bool
  ensures { result <-> in_range (x : int) }
 
 clone export "ada__model".Static_Discrete with axiom .,
 type t = universal_integer, 
 function first = first, 
 function last = last, 
 predicate in_range = in_range
 
 type universal_integer__ref =
  { mutable universal_integer__content : universal_integer }
 
 function universal_integer__ref_universal_integer__content__projection 
   (a : universal_integer__ref) : universal_integer =
  a.universal_integer__content
 
 meta "model_projection" function universal_integer__ref_universal_integer__content__projection
 
 meta "inline:no" function universal_integer__ref_universal_integer__content__projection
 
 val universal_integer__havoc 
   (x : universal_integer__ref) : unit
  writes {x}

end

(* Module defining to_rep/of_rep for type "universal_integer", created in Gnat2Why.Types.Translate_Type *)
module Standard__universal_integer__rep
 use        Standard__universal_integer as Standard__universal_integer
 use        "_gnatprove_standard".Main
 use        "int".Int

 function to_rep 
   (x : Standard__universal_integer.universal_integer) : int =
  (Standard__universal_integer.universal_integer'int x)
 
 clone export "ada__model".Rep_Proj_Int with axiom .,
 type t = Standard__universal_integer.universal_integer, 
 predicate in_range = Standard__universal_integer.in_range, 
 function to_rep = to_rep
 
 meta "model_projection" function to_rep
 
 meta "inline:no" function to_rep

end

(* Module for axiomatizing the record theory associated to type "tokenextent" defined at mystringtokeniser.ads:5, created in Why.Gen.Records.Create_Rep_Record_Theory_If_Needed *)
module Mystringtokeniser__tokenextent__rep
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        Standard__natural as Standard__natural
 use        Standard__natural__rep as Standard__natural__rep
 use        Standard__positive as Standard__positive
 use        Standard__positive__rep as Standard__positive__rep

 type __split_fields =
  { rec__mystringtokeniser__tokenextent__start [@name:Start] [@model_trace:.2686] : Standard__positive.positive; rec__mystringtokeniser__tokenextent__length [@name:Length] [@model_trace:.2696] : Standard__natural.natural }
 
 function __split_fields_rec__mystringtokeniser__tokenextent__start__projection [@name:Start] [@model_trace:.2686] 
   (a : __split_fields) : Standard__positive.positive =
  a.rec__mystringtokeniser__tokenextent__start
 
 meta "model_projection" function __split_fields_rec__mystringtokeniser__tokenextent__start__projection
 
 meta "inline:no" function __split_fields_rec__mystringtokeniser__tokenextent__start__projection
 
 function __split_fields_rec__mystringtokeniser__tokenextent__length__projection [@name:Length] [@model_trace:.2696] 
   (a : __split_fields) : Standard__natural.natural =
  a.rec__mystringtokeniser__tokenextent__length
 
 meta "model_projection" function __split_fields_rec__mystringtokeniser__tokenextent__length__projection
 
 meta "inline:no" function __split_fields_rec__mystringtokeniser__tokenextent__length__projection
 
 type __split_fields__ref =
  { mutable __split_fields__content : __split_fields }
 
 function __split_fields__ref___split_fields__content__projection 
   (a : __split_fields__ref) : __split_fields =
  a.__split_fields__content
 
 meta "model_projection" function __split_fields__ref___split_fields__content__projection
 
 meta "inline:no" function __split_fields__ref___split_fields__content__projection
 
 val __split_fields__havoc 
   (x : __split_fields__ref) : unit
  writes {x}
 
 type __rep =
  { __split_fields : __split_fields }
 
 function __rep___split_fields__projection 
   (a : __rep) : __split_fields =
  a.__split_fields
 
 meta "model_projection" function __rep___split_fields__projection
 
 meta "inline:no" function __rep___split_fields__projection
 
 function to_base 
   (a : __rep) : __rep =
  a
 val to_base 
   (a : __rep) : __rep
  ensures { result = to_base (a : __rep) }
 
 function of_base 
   (a : __rep) : __rep =
  a
 val of_base 
   (a : __rep) : __rep
  ensures { result = of_base (a : __rep) }
 
 predicate mystringtokeniser__tokenextent__start__pred 
   (a : __rep) =
  true
 val mystringtokeniser__tokenextent__start__pred 
   (a : __rep) : bool
  ensures { result <-> mystringtokeniser__tokenextent__start__pred (a : __rep) }
 
 val rec__mystringtokeniser__tokenextent__start_ 
   (a : __rep) : Standard__positive.positive
  requires {  (mystringtokeniser__tokenextent__start__pred a) }
  ensures {  (result = a.__split_fields.rec__mystringtokeniser__tokenextent__start) }
 
 predicate mystringtokeniser__tokenextent__length__pred 
   (a : __rep) =
  true
 val mystringtokeniser__tokenextent__length__pred 
   (a : __rep) : bool
  ensures { result <-> mystringtokeniser__tokenextent__length__pred (a : __rep) }
 
 val rec__mystringtokeniser__tokenextent__length_ 
   (a : __rep) : Standard__natural.natural
  requires {  (mystringtokeniser__tokenextent__length__pred a) }
  ensures {  (result = a.__split_fields.rec__mystringtokeniser__tokenextent__length) }
 
 function bool_eq 
   (a : __rep) (b : __rep) : bool =
  (if (( ((Standard__positive__rep.to_rep a.__split_fields.rec__mystringtokeniser__tokenextent__start) = (Standard__positive__rep.to_rep b.__split_fields.rec__mystringtokeniser__tokenextent__start)) /\ ((Standard__natural__rep.to_rep a.__split_fields.rec__mystringtokeniser__tokenextent__length) = (Standard__natural__rep.to_rep b.__split_fields.rec__mystringtokeniser__tokenextent__length)) )) then (
   True) else (
   False))
 val bool_eq 
   (a : __rep) (b : __rep) : bool
  ensures { result = bool_eq (a : __rep) (b : __rep) }

end

(* Module for axiomatizing type "tokenextent" defined at mystringtokeniser.ads:5, created in Gnat2Why.Types.Translate_Type *)
module Mystringtokeniser__tokenextent
 use export Mystringtokeniser__tokenextent__rep
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int

 type tokenextent =
  __rep
 
 val constant value__size 
   : int

 
 val constant object__size 
   : int

 
 val constant alignment 
   : int

 
 axiom value__size_axiom :
  ((value__size ) >= (0 : int))
 
 axiom object__size_axiom :
  ((object__size ) >= (0 : int))
 
 axiom alignment_axiom :
  ((alignment ) >= (0 : int))
 
 val constant mystringtokeniser__tokenextent__start__first__bit 
   : int

 
 val constant mystringtokeniser__tokenextent__start__last__bit 
   : int

 
 val constant mystringtokeniser__tokenextent__start__position 
   : int

 
 axiom mystringtokeniser__tokenextent__start__first__bit_axiom :
  ((mystringtokeniser__tokenextent__start__first__bit ) >= (0 : int))
 
 axiom mystringtokeniser__tokenextent__start__last__bit_axiom :
  ((mystringtokeniser__tokenextent__start__last__bit ) > (mystringtokeniser__tokenextent__start__first__bit ))
 
 axiom mystringtokeniser__tokenextent__start__position_axiom :
  ((mystringtokeniser__tokenextent__start__position ) >= (0 : int))
 
 val constant mystringtokeniser__tokenextent__length__first__bit 
   : int

 
 val constant mystringtokeniser__tokenextent__length__last__bit 
   : int

 
 val constant mystringtokeniser__tokenextent__length__position 
   : int

 
 axiom mystringtokeniser__tokenextent__length__first__bit_axiom :
  ((mystringtokeniser__tokenextent__length__first__bit ) >= (0 : int))
 
 axiom mystringtokeniser__tokenextent__length__last__bit_axiom :
  ((mystringtokeniser__tokenextent__length__last__bit ) > (mystringtokeniser__tokenextent__length__first__bit ))
 
 axiom mystringtokeniser__tokenextent__length__position_axiom :
  ((mystringtokeniser__tokenextent__length__position ) >= (0 : int))
 
 val function user_eq 
   (a : tokenextent) (b : tokenextent) : bool
 
 val constant dummy 
   : tokenextent

 
 type tokenextent__ref =
  { mutable tokenextent__content : tokenextent }
 
 function tokenextent__ref_tokenextent__content__projection 
   (a : tokenextent__ref) : tokenextent =
  a.tokenextent__content
 
 meta "model_projection" function tokenextent__ref_tokenextent__content__projection
 
 meta "inline:no" function tokenextent__ref_tokenextent__content__projection
 
 val tokenextent__havoc 
   (x : tokenextent__ref) : unit
  writes {x}

end

(* Module for axiomatizing the array theory associated to type "tokenarray" defined at mystringtokeniser.ads:10, created in Why.Gen.Arrays.Create_Rep_Array_Theory *)
module Array__Int__Mystringtokeniser__tokenextent
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int
 use        Mystringtokeniser__tokenextent as Mystringtokeniser__tokenextent

 function index_I1_one 
   : int =
  (1 : int)
 
 type component_type =
  Mystringtokeniser__tokenextent.tokenextent
 
 clone export "_gnatprove_standard".Array__1 with axiom .,
 type I1.t = int, 
 predicate I1.le = Int.(<=), 
 predicate I1.lt = Int.(<), 
 predicate I1.gt = Int.(>), 
 function I1.add = Int.(+), 
 function I1.sub = Int.(-), 
 function I1.one = index_I1_one, 
 type component_type = component_type
 
 function bool_eq 
   (a : map) (a__first : int) (a__last : int) (b : map) (b__first : int) (b__last : int) : bool =
  ( (if ((a__first <= a__last)) then (
   ( (b__first <= b__last) /\ ((a__last - a__first) = (b__last - b__first)) )) else (
   (b__first > b__last))) /\ (forall temp___idx_154   : int.
   (if (( (a__first <= temp___idx_154) /\ (temp___idx_154 <= a__last) )) then (
    (Mystringtokeniser__tokenextent.bool_eq (get a temp___idx_154) (get b ((b__first - a__first) + temp___idx_154)))) else true)) )
 val bool_eq 
   (a : map) (a__first : int) (a__last : int) (b : map) (b__first : int) (b__last : int) : bool
  ensures { result = bool_eq (a : map) (a__first : int) (a__last : int) (b : map) (b__first : int) (b__last : int) }
 
 axiom bool_eq_rev :
  (forall a   b   : map.
  (forall a__first   a__last   b__first   b__last   : int.
   ( ((bool_eq b b__first b__last a a__first a__last) = True) -> ( (if ((a__first <= a__last)) then (
    ( (b__first <= b__last) /\ ((a__last - a__first) = (b__last - b__first)) )) else (
    (b__first > b__last))) /\ (forall temp___idx_154   : int.
    (if (( (a__first <= temp___idx_154) /\ (temp___idx_154 <= a__last) )) then (
     (Mystringtokeniser__tokenextent.bool_eq (get a temp___idx_154) (get b ((b__first - a__first) + temp___idx_154)))) else true)) ) )))

end

(* Module for axiomatizing concatenation for the array theory associated to type "tokenarray" defined at mystringtokeniser.ads:10, created in Why.Gen.Arrays.Declare_Concatenation_Symbols *)
module Array__Int__Mystringtokeniser__tokenextent__Concat
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int
 use        Array__Int__Mystringtokeniser__tokenextent as Array__Int__Mystringtokeniser__tokenextent

 function index_Index_one 
   : int =
  (1 : int)
 
 clone export "_gnatprove_standard".Array__1__Concat with axiom .,
 type component_type = Array__Int__Mystringtokeniser__tokenextent.component_type, 
 type map = Array__Int__Mystringtokeniser__tokenextent.map, 
 type Index.t = int, 
 predicate Index.le = Int.(<=), 
 predicate Index.lt = Int.(<), 
 predicate Index.gt = Int.(>), 
 function Index.add = Int.(+), 
 function Index.sub = Int.(-), 
 function Index.one = index_Index_one, 
 function get = Array__Int__Mystringtokeniser__tokenextent.get

end

(* Module for axiomatizing type "tokenarray" defined at mystringtokeniser.ads:10, created in Gnat2Why.Types.Translate_Type *)
module Mystringtokeniser__tokenarray
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int
 use        Standard__integer as Standard__integer
 use        Standard__positive as Standard__positive
 use        Standard__integer__rep as Standard__integer__rep
 use        Mystringtokeniser__tokenextent as Mystringtokeniser__tokenextent
 use        Array__Int__Mystringtokeniser__tokenextent as Array__Int__Mystringtokeniser__tokenextent

 type component_type =
  Mystringtokeniser__tokenextent.tokenextent
 
 function index_1_id 
   (x : int) : int =
  x
 
 clone export "ada__model".Unconstr_Array with axiom .,
 type map = Array__Int__Mystringtokeniser__tokenextent.map, 
 function array_bool_eq = Array__Int__Mystringtokeniser__tokenextent.bool_eq, 
 type index_base_type = Standard__integer.integer, 
 type index_rep_type = int, 
 function to_rep = Standard__integer__rep.to_rep, 
 function rep_to_int = index_1_id, 
 predicate in_range_base = Standard__integer.in_range, 
 predicate index_dynamic_property = Standard__positive.dynamic_property, 
 predicate index_rep_le = Int.(<=)
 
 type tokenarray =
  __t
 
 meta "model_projection" function to_array
 
 meta "inline:no" function to_array
 
 meta "model_projection" function first
 
 meta "inline:no" function first
 
 meta "model_projection" function last
 
 meta "inline:no" function last
 
 type tokenarray__ref =
  { mutable tokenarray__content : tokenarray }
 
 function tokenarray__ref_tokenarray__content__projection 
   (a : tokenarray__ref) : tokenarray =
  a.tokenarray__content
 
 meta "model_projection" function tokenarray__ref_tokenarray__content__projection
 
 meta "inline:no" function tokenarray__ref_tokenarray__content__projection
 
 val tokenarray__havoc 
   (x : tokenarray__ref) : unit
  writes {x}

end

(* Module for defining the constant "ch" defined at mystringtokeniser.ads:12, created in Gnat2Why.Decls.Translate_Constant *)
module Mystringtokeniser__is_whitespace__ch
 use        "_gnatprove_standard".Main
 use        "int".Int

 val constant ch [#"mystringtokeniser.ads" 12 0 0][@model_trace:2727] [@name:Ch] 
   : int

 
 val constant attr__ATTRIBUTE_ADDRESS 
   : int


end

(* Module giving an empty axiom for the entity "ch" defined at mystringtokeniser.ads:12, created in Gnat2Why.Driver.Translate_Entity.Generate_Empty_Axiom_Theory *)
module Mystringtokeniser__is_whitespace__ch___axiom
 use        "_gnatprove_standard".Main
 use        "int".Int


end

(* Module for defining the constant "lf" defined at a-chlat1.ads:35, created in Gnat2Why.Decls.Translate_Constant *)
module Ada__characters__latin_1__lf
 use        "_gnatprove_standard".Main
 use        "int".Int

 val constant lf [#"a-chlat1.ads" 35 0 0][@name:LF] [@model_trace:3082] 
   : int

 
 val constant attr__ATTRIBUTE_ADDRESS 
   : int


end

(* Module for defining the constant "ht" defined at a-chlat1.ads:34, created in Gnat2Why.Decls.Translate_Constant *)
module Ada__characters__latin_1__ht
 use        "_gnatprove_standard".Main
 use        "int".Int

 val constant ht [#"a-chlat1.ads" 34 0 0][@model_trace:3069] [@name:HT] 
   : int

 
 val constant attr__ATTRIBUTE_ADDRESS 
   : int


end

(* Module for possibly declaring a logic function for "is_whitespace" defined at mystringtokeniser.ads:12, created in Gnat2Why.Subprograms.Translate_Subprogram_Spec *)
module Mystringtokeniser__is_whitespace
 use        "_gnatprove_standard".Main
 use        "int".Int

 val function is_whitespace 
   (ch : int) : bool
 
 val predicate is_whitespace__function_guard 
   (temp___result_155 : bool) (ch : int)

end

(* Module for defining the constant "s" defined at mystringtokeniser.ads:70, created in Gnat2Why.Decls.Translate_Constant *)
module Mystringtokeniser__tokenise__s
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        Standard__string as Standard__string

 val constant s [#"mystringtokeniser.ads" 70 0 0][@model_projected] [@name:S] [@model_trace:2769] 
   : Standard__string.string

 
 val constant attr__ATTRIBUTE_ADDRESS 
   : int


end

(* Module giving an empty axiom for the entity "s" defined at mystringtokeniser.ads:70, created in Gnat2Why.Driver.Translate_Entity.Generate_Empty_Axiom_Theory *)
module Mystringtokeniser__tokenise__s___axiom
 use        "_gnatprove_standard".Main
 use        "int".Int


end

(* Module for axiomatizing type "T7s" defined at mystringtokeniser.ads:73, created in Gnat2Why.Types.Translate_Type *)
module Mystringtokeniser__tokenise__L6s__T7s
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int
 use        Standard__integer as Standard__integer
 use        Standard__integer__rep as Standard__integer__rep

 type t7s =
  Standard__integer.integer
 
 predicate dynamic_property 
   (first_int : int) (last_int : int) (x : int) =
  ( (first_int <= x) /\ (x <= last_int) )
 val dynamic_property 
   (first_int : int) (last_int : int) (x : int) : bool
  ensures { result <-> dynamic_property (first_int : int) (last_int : int) (x : int) }
 
 clone export "ada__model".Dynamic_Discrete with axiom .,
 type t = t7s, 
 type rep_type = int, 
 function base_to_rep = Standard__integer__rep.to_rep, 
 function base_of_rep = Standard__integer__rep.of_rep, 
 predicate dynamic_property = dynamic_property
 
 type t7s__ref =
  { mutable t7s__content : t7s }
 
 function t7s__ref_t7s__content__projection 
   (a : t7s__ref) : t7s =
  a.t7s__content
 
 meta "model_projection" function t7s__ref_t7s__content__projection
 
 meta "inline:no" function t7s__ref_t7s__content__projection
 
 val t7s__havoc 
   (x : t7s__ref) : unit
  writes {x}

end

(* Module for axiomatizing type "TS2bP1" defined at mystringtokeniser.adb:40, created in Gnat2Why.Types.Translate_Type *)
module Mystringtokeniser__tokenise__TS2bP1
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int
 use        Standard__integer as Standard__integer
 use        Standard__integer__rep as Standard__integer__rep

 type tS2bP1 =
  Standard__integer.integer
 
 predicate dynamic_property 
   (first_int : int) (last_int : int) (x : int) =
  ( (first_int <= x) /\ (x <= last_int) )
 val dynamic_property 
   (first_int : int) (last_int : int) (x : int) : bool
  ensures { result <-> dynamic_property (first_int : int) (last_int : int) (x : int) }
 
 clone export "ada__model".Dynamic_Discrete with axiom .,
 type t = tS2bP1, 
 type rep_type = int, 
 function base_to_rep = Standard__integer__rep.to_rep, 
 function base_of_rep = Standard__integer__rep.of_rep, 
 predicate dynamic_property = dynamic_property
 
 type tS2bP1__ref =
  { mutable tS2bP1__content : tS2bP1 }
 
 function tS2bP1__ref_tS2bP1__content__projection 
   (a : tS2bP1__ref) : tS2bP1 =
  a.tS2bP1__content
 
 meta "model_projection" function tS2bP1__ref_tS2bP1__content__projection
 
 meta "inline:no" function tS2bP1__ref_tS2bP1__content__projection
 
 val tS2bP1__havoc 
   (x : tS2bP1__ref) : unit
  writes {x}

end

(* Module for axiomatizing type "S2b" defined at mystringtokeniser.adb:40, created in Gnat2Why.Types.Translate_Type *)
module Mystringtokeniser__tokenise__S2b
 use export Mystringtokeniser__tokenarray
 use        "_gnatprove_standard".Main
 use        "int".Int

 type s2b =
  tokenarray
 
 type s2b__ref =
  { mutable s2b__content : s2b }
 
 function s2b__ref_s2b__content__projection 
   (a : s2b__ref) : s2b =
  a.s2b__content
 
 meta "model_projection" function s2b__ref_s2b__content__projection
 
 meta "inline:no" function s2b__ref_s2b__content__projection
 
 val s2b__havoc 
   (x : s2b__ref) : unit
  writes {x}

end

(* Module for axiomatizing type "TS1bP1" defined at mystringtokeniser.adb:40, created in Gnat2Why.Types.Translate_Type *)
module Mystringtokeniser__tokenise__TS1bP1
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int
 use        Standard__integer as Standard__integer
 use        Standard__integer__rep as Standard__integer__rep

 type tS1bP1 =
  Standard__integer.integer
 
 predicate dynamic_property 
   (first_int : int) (last_int : int) (x : int) =
  ( (first_int <= x) /\ (x <= last_int) )
 val dynamic_property 
   (first_int : int) (last_int : int) (x : int) : bool
  ensures { result <-> dynamic_property (first_int : int) (last_int : int) (x : int) }
 
 clone export "ada__model".Dynamic_Discrete with axiom .,
 type t = tS1bP1, 
 type rep_type = int, 
 function base_to_rep = Standard__integer__rep.to_rep, 
 function base_of_rep = Standard__integer__rep.of_rep, 
 predicate dynamic_property = dynamic_property
 
 type tS1bP1__ref =
  { mutable tS1bP1__content : tS1bP1 }
 
 function tS1bP1__ref_tS1bP1__content__projection 
   (a : tS1bP1__ref) : tS1bP1 =
  a.tS1bP1__content
 
 meta "model_projection" function tS1bP1__ref_tS1bP1__content__projection
 
 meta "inline:no" function tS1bP1__ref_tS1bP1__content__projection
 
 val tS1bP1__havoc 
   (x : tS1bP1__ref) : unit
  writes {x}

end

(* Module for axiomatizing type "S1b" defined at mystringtokeniser.adb:40, created in Gnat2Why.Types.Translate_Type *)
module Mystringtokeniser__tokenise__S1b
 use export Standard__string
 use        "_gnatprove_standard".Main
 use        "int".Int

 type s1b =
  string
 
 type s1b__ref =
  { mutable s1b__content : s1b }
 
 function s1b__ref_s1b__content__projection 
   (a : s1b__ref) : s1b =
  a.s1b__content
 
 meta "model_projection" function s1b__ref_s1b__content__projection
 
 meta "inline:no" function s1b__ref_s1b__content__projection
 
 val s1b__havoc 
   (x : s1b__ref) : unit
  writes {x}

end

(* Module for axiomatizing type "T7b" defined at mystringtokeniser.adb:52, created in Gnat2Why.Types.Translate_Type *)
module Mystringtokeniser__tokenise__L_1__L6b__T7b
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int
 use        Standard__integer as Standard__integer
 use        Standard__integer__rep as Standard__integer__rep

 type t7b =
  Standard__integer.integer
 
 predicate dynamic_property 
   (first_int : int) (last_int : int) (x : int) =
  ( (first_int <= x) /\ (x <= last_int) )
 val dynamic_property 
   (first_int : int) (last_int : int) (x : int) : bool
  ensures { result <-> dynamic_property (first_int : int) (last_int : int) (x : int) }
 
 clone export "ada__model".Dynamic_Discrete with axiom .,
 type t = t7b, 
 type rep_type = int, 
 function base_to_rep = Standard__integer__rep.to_rep, 
 function base_of_rep = Standard__integer__rep.of_rep, 
 predicate dynamic_property = dynamic_property
 
 type t7b__ref =
  { mutable t7b__content : t7b }
 
 function t7b__ref_t7b__content__projection 
   (a : t7b__ref) : t7b =
  a.t7b__content
 
 meta "model_projection" function t7b__ref_t7b__content__projection
 
 meta "inline:no" function t7b__ref_t7b__content__projection
 
 val t7b__havoc 
   (x : t7b__ref) : unit
  writes {x}

end

(* Module for axiomatizing type "T5b" defined at mystringtokeniser.adb:52, created in Gnat2Why.Types.Translate_Type *)
module Mystringtokeniser__tokenise__L_1__L4b__T5b
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int
 use        Standard__integer as Standard__integer
 use        Standard__integer__rep as Standard__integer__rep

 type t5b =
  Standard__integer.integer
 
 predicate dynamic_property 
   (first_int : int) (last_int : int) (x : int) =
  ( (first_int <= x) /\ (x <= last_int) )
 val dynamic_property 
   (first_int : int) (last_int : int) (x : int) : bool
  ensures { result <-> dynamic_property (first_int : int) (last_int : int) (x : int) }
 
 clone export "ada__model".Dynamic_Discrete with axiom .,
 type t = t5b, 
 type rep_type = int, 
 function base_to_rep = Standard__integer__rep.to_rep, 
 function base_of_rep = Standard__integer__rep.of_rep, 
 predicate dynamic_property = dynamic_property
 
 type t5b__ref =
  { mutable t5b__content : t5b }
 
 function t5b__ref_t5b__content__projection 
   (a : t5b__ref) : t5b =
  a.t5b__content
 
 meta "model_projection" function t5b__ref_t5b__content__projection
 
 meta "inline:no" function t5b__ref_t5b__content__projection
 
 val t5b__havoc 
   (x : t5b__ref) : unit
  writes {x}

end
(* Module for defining a ref holding the value of variable "tokens" defined at mystringtokeniser.ads:70, created in Gnat2Why.Decls.Translate_Variable *)
module Mystringtokeniser__tokenise__tokens
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        Standard__integer as Standard__integer
 use        Array__Int__Mystringtokeniser__tokenextent as Array__Int__Mystringtokeniser__tokenextent

 val tokens [#"mystringtokeniser.ads" 70 0 0][@name:Tokens] [@model_projected] [@model_trace:2778]  : Array__Int__Mystringtokeniser__tokenextent.map__ref 
 
 val constant tokens__first [#"mystringtokeniser.ads" 70 0 0][@name:Tokens] [@model_trace:2778'First] [@model_projected] 
   : Standard__integer.integer

 
 val constant tokens__last [#"mystringtokeniser.ads" 70 0 0][@name:Tokens] [@model_projected] [@model_trace:2778'Last] 
   : Standard__integer.integer

 
 val constant attr__ATTRIBUTE_ADDRESS 
   : int


end

(* Module giving an empty axiom for the entity "tokens" defined at mystringtokeniser.ads:70, created in Gnat2Why.Driver.Translate_Entity.Generate_Empty_Axiom_Theory *)
module Mystringtokeniser__tokenise__tokens___axiom
 use        "_gnatprove_standard".Main
 use        "int".Int


end

(* Module for defining a ref holding the value of variable "count" defined at mystringtokeniser.ads:70, created in Gnat2Why.Decls.Translate_Variable *)
module Mystringtokeniser__tokenise__count
 use        "_gnatprove_standard".Main
 use        "int".Int

 val count [#"mystringtokeniser.ads" 70 0 0][@model_trace:2787] [@model_projected] [@name:Count]  : int__ref 
 
 val constant attr__ATTRIBUTE_ADDRESS 
   : int


end

(* Module giving an empty axiom for the entity "count" defined at mystringtokeniser.ads:70, created in Gnat2Why.Driver.Translate_Entity.Generate_Empty_Axiom_Theory *)
module Mystringtokeniser__tokenise__count___axiom
 use        "_gnatprove_standard".Main
 use        "int".Int


end

(* Module for defining a ref holding the value of variable "index" defined at mystringtokeniser.adb:41, created in Gnat2Why.Decls.Translate_Variable *)
module Mystringtokeniser__tokenise__index
 use        "_gnatprove_standard".Main
 use        "int".Int

 val index [#"mystringtokeniser.adb" 41 0 0][@model_projected] [@model_trace:2343] [@name:Index]  : int__ref 
 
 val constant attr__ATTRIBUTE_ADDRESS 
   : int


end

(* Module giving an empty axiom for the entity "index" defined at mystringtokeniser.adb:41, created in Gnat2Why.Driver.Translate_Entity.Generate_Empty_Axiom_Theory *)
module Mystringtokeniser__tokenise__index___axiom
 use        "_gnatprove_standard".Main
 use        "int".Int


end

(* Module for defining a ref holding the value of variable "extent" defined at mystringtokeniser.adb:42, created in Gnat2Why.Decls.Translate_Variable *)
module Mystringtokeniser__tokenise__extent
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        Mystringtokeniser__tokenextent as Mystringtokeniser__tokenextent

 val extent__split_fields [#"mystringtokeniser.adb" 42 0 0][@model_trace:2352] [@model_projected] [@name:Extent]  : Mystringtokeniser__tokenextent.__split_fields__ref 
 
 val constant attr__ATTRIBUTE_ADDRESS 
   : int


end

(* Module giving an empty axiom for the entity "extent" defined at mystringtokeniser.adb:42, created in Gnat2Why.Driver.Translate_Entity.Generate_Empty_Axiom_Theory *)
module Mystringtokeniser__tokenise__extent___axiom
 use        "_gnatprove_standard".Main
 use        "int".Int


end

(* Module for defining a ref holding the value of variable "outindex" defined at mystringtokeniser.adb:43, created in Gnat2Why.Decls.Translate_Variable *)
module Mystringtokeniser__tokenise__outindex
 use        "_gnatprove_standard".Main
 use        "int".Int

 val outindex [#"mystringtokeniser.adb" 43 0 0][@model_trace:2361] [@name:OutIndex] [@model_projected]  : int__ref 
 
 val constant attr__ATTRIBUTE_ADDRESS 
   : int


end

(* Module giving an empty axiom for the entity "outindex" defined at mystringtokeniser.adb:43, created in Gnat2Why.Driver.Translate_Entity.Generate_Empty_Axiom_Theory *)
module Mystringtokeniser__tokenise__outindex___axiom
 use        "_gnatprove_standard".Main
 use        "int".Int


end
(* Module giving axioms for type "short_short_integer", created in Gnat2Why.Types.Generate_Type_Completion *)
module Standard__short_short_integer___axiom
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int
 use        Standard__short_short_integer as Standard__short_short_integer

 predicate dynamic_invariant [@inline] 
   (temp___expr_4 : int) (temp___is_init_0 : bool) (temp___skip_constant_1 : bool) (temp___do_toplevel_2 : bool) (temp___do_typ_inv_3 : bool) =
  (if (( temp___is_init_0 \/ (Standard__short_short_integer.first <= Standard__short_short_integer.last) )) then (
   (Standard__short_short_integer.dynamic_property Standard__short_short_integer.first Standard__short_short_integer.last temp___expr_4)) else true)
 val dynamic_invariant [@inline] 
   (temp___expr_4 : int) (temp___is_init_0 : bool) (temp___skip_constant_1 : bool) (temp___do_toplevel_2 : bool) (temp___do_typ_inv_3 : bool) : bool
  ensures { result <-> dynamic_invariant (temp___expr_4 : int) (temp___is_init_0 : bool) (temp___skip_constant_1 : bool) (temp___do_toplevel_2 : bool) (temp___do_typ_inv_3 : bool) }
 
 predicate default_initial_assumption [@inline] 
   (temp___expr_5 : int) (temp___skip_top_level_6 : bool) =
  true
 val default_initial_assumption [@inline] 
   (temp___expr_5 : int) (temp___skip_top_level_6 : bool) : bool
  ensures { result <-> default_initial_assumption (temp___expr_5 : int) (temp___skip_top_level_6 : bool) }

end

(* Module giving axioms for type "short_integer", created in Gnat2Why.Types.Generate_Type_Completion *)
module Standard__short_integer___axiom
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int
 use        Standard__short_integer as Standard__short_integer

 predicate dynamic_invariant [@inline] 
   (temp___expr_11 : int) (temp___is_init_7 : bool) (temp___skip_constant_8 : bool) (temp___do_toplevel_9 : bool) (temp___do_typ_inv_10 : bool) =
  (if (( temp___is_init_7 \/ (Standard__short_integer.first <= Standard__short_integer.last) )) then (
   (Standard__short_integer.dynamic_property Standard__short_integer.first Standard__short_integer.last temp___expr_11)) else true)
 val dynamic_invariant [@inline] 
   (temp___expr_11 : int) (temp___is_init_7 : bool) (temp___skip_constant_8 : bool) (temp___do_toplevel_9 : bool) (temp___do_typ_inv_10 : bool) : bool
  ensures { result <-> dynamic_invariant (temp___expr_11 : int) (temp___is_init_7 : bool) (temp___skip_constant_8 : bool) (temp___do_toplevel_9 : bool) (temp___do_typ_inv_10 : bool) }
 
 predicate default_initial_assumption [@inline] 
   (temp___expr_12 : int) (temp___skip_top_level_13 : bool) =
  true
 val default_initial_assumption [@inline] 
   (temp___expr_12 : int) (temp___skip_top_level_13 : bool) : bool
  ensures { result <-> default_initial_assumption (temp___expr_12 : int) (temp___skip_top_level_13 : bool) }

end

(* Module giving axioms for type "integer", created in Gnat2Why.Types.Generate_Type_Completion *)
module Standard__integer___axiom
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int
 use        Standard__integer as Standard__integer

 predicate dynamic_invariant [@inline] 
   (temp___expr_18 : int) (temp___is_init_14 : bool) (temp___skip_constant_15 : bool) (temp___do_toplevel_16 : bool) (temp___do_typ_inv_17 : bool) =
  (if (( temp___is_init_14 \/ (Standard__integer.first <= Standard__integer.last) )) then (
   (Standard__integer.dynamic_property Standard__integer.first Standard__integer.last temp___expr_18)) else true)
 val dynamic_invariant [@inline] 
   (temp___expr_18 : int) (temp___is_init_14 : bool) (temp___skip_constant_15 : bool) (temp___do_toplevel_16 : bool) (temp___do_typ_inv_17 : bool) : bool
  ensures { result <-> dynamic_invariant (temp___expr_18 : int) (temp___is_init_14 : bool) (temp___skip_constant_15 : bool) (temp___do_toplevel_16 : bool) (temp___do_typ_inv_17 : bool) }
 
 predicate default_initial_assumption [@inline] 
   (temp___expr_19 : int) (temp___skip_top_level_20 : bool) =
  true
 val default_initial_assumption [@inline] 
   (temp___expr_19 : int) (temp___skip_top_level_20 : bool) : bool
  ensures { result <-> default_initial_assumption (temp___expr_19 : int) (temp___skip_top_level_20 : bool) }

end

(* Module giving axioms for type "long_integer", created in Gnat2Why.Types.Generate_Type_Completion *)
module Standard__long_integer___axiom
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int
 use        Standard__long_integer as Standard__long_integer

 predicate dynamic_invariant [@inline] 
   (temp___expr_25 : int) (temp___is_init_21 : bool) (temp___skip_constant_22 : bool) (temp___do_toplevel_23 : bool) (temp___do_typ_inv_24 : bool) =
  (if (( temp___is_init_21 \/ (Standard__long_integer.first <= Standard__long_integer.last) )) then (
   (Standard__long_integer.dynamic_property Standard__long_integer.first Standard__long_integer.last temp___expr_25)) else true)
 val dynamic_invariant [@inline] 
   (temp___expr_25 : int) (temp___is_init_21 : bool) (temp___skip_constant_22 : bool) (temp___do_toplevel_23 : bool) (temp___do_typ_inv_24 : bool) : bool
  ensures { result <-> dynamic_invariant (temp___expr_25 : int) (temp___is_init_21 : bool) (temp___skip_constant_22 : bool) (temp___do_toplevel_23 : bool) (temp___do_typ_inv_24 : bool) }
 
 predicate default_initial_assumption [@inline] 
   (temp___expr_26 : int) (temp___skip_top_level_27 : bool) =
  true
 val default_initial_assumption [@inline] 
   (temp___expr_26 : int) (temp___skip_top_level_27 : bool) : bool
  ensures { result <-> default_initial_assumption (temp___expr_26 : int) (temp___skip_top_level_27 : bool) }

end

(* Module giving axioms for type "long_long_integer", created in Gnat2Why.Types.Generate_Type_Completion *)
module Standard__long_long_integer___axiom
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int
 use        Standard__long_long_integer as Standard__long_long_integer

 predicate dynamic_invariant [@inline] 
   (temp___expr_32 : int) (temp___is_init_28 : bool) (temp___skip_constant_29 : bool) (temp___do_toplevel_30 : bool) (temp___do_typ_inv_31 : bool) =
  (if (( temp___is_init_28 \/ (Standard__long_long_integer.first <= Standard__long_long_integer.last) )) then (
   (Standard__long_long_integer.dynamic_property Standard__long_long_integer.first Standard__long_long_integer.last temp___expr_32)) else true)
 val dynamic_invariant [@inline] 
   (temp___expr_32 : int) (temp___is_init_28 : bool) (temp___skip_constant_29 : bool) (temp___do_toplevel_30 : bool) (temp___do_typ_inv_31 : bool) : bool
  ensures { result <-> dynamic_invariant (temp___expr_32 : int) (temp___is_init_28 : bool) (temp___skip_constant_29 : bool) (temp___do_toplevel_30 : bool) (temp___do_typ_inv_31 : bool) }
 
 predicate default_initial_assumption [@inline] 
   (temp___expr_33 : int) (temp___skip_top_level_34 : bool) =
  true
 val default_initial_assumption [@inline] 
   (temp___expr_33 : int) (temp___skip_top_level_34 : bool) : bool
  ensures { result <-> default_initial_assumption (temp___expr_33 : int) (temp___skip_top_level_34 : bool) }

end

(* Module giving axioms for type "natural", created in Gnat2Why.Types.Generate_Type_Completion *)
module Standard__natural___axiom
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int
 use        Standard__natural as Standard__natural

 predicate dynamic_invariant [@inline] 
   (temp___expr_39 : int) (temp___is_init_35 : bool) (temp___skip_constant_36 : bool) (temp___do_toplevel_37 : bool) (temp___do_typ_inv_38 : bool) =
  (if (( temp___is_init_35 \/ (Standard__natural.first <= Standard__natural.last) )) then (
   (Standard__natural.dynamic_property Standard__natural.first Standard__natural.last temp___expr_39)) else true)
 val dynamic_invariant [@inline] 
   (temp___expr_39 : int) (temp___is_init_35 : bool) (temp___skip_constant_36 : bool) (temp___do_toplevel_37 : bool) (temp___do_typ_inv_38 : bool) : bool
  ensures { result <-> dynamic_invariant (temp___expr_39 : int) (temp___is_init_35 : bool) (temp___skip_constant_36 : bool) (temp___do_toplevel_37 : bool) (temp___do_typ_inv_38 : bool) }
 
 predicate default_initial_assumption [@inline] 
   (temp___expr_40 : int) (temp___skip_top_level_41 : bool) =
  true
 val default_initial_assumption [@inline] 
   (temp___expr_40 : int) (temp___skip_top_level_41 : bool) : bool
  ensures { result <-> default_initial_assumption (temp___expr_40 : int) (temp___skip_top_level_41 : bool) }

end

(* Module giving axioms for type "positive", created in Gnat2Why.Types.Generate_Type_Completion *)
module Standard__positive___axiom
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int
 use        Standard__positive as Standard__positive

 predicate dynamic_invariant [@inline] 
   (temp___expr_46 : int) (temp___is_init_42 : bool) (temp___skip_constant_43 : bool) (temp___do_toplevel_44 : bool) (temp___do_typ_inv_45 : bool) =
  (if (( temp___is_init_42 \/ (Standard__positive.first <= Standard__positive.last) )) then (
   (Standard__positive.dynamic_property Standard__positive.first Standard__positive.last temp___expr_46)) else true)
 val dynamic_invariant [@inline] 
   (temp___expr_46 : int) (temp___is_init_42 : bool) (temp___skip_constant_43 : bool) (temp___do_toplevel_44 : bool) (temp___do_typ_inv_45 : bool) : bool
  ensures { result <-> dynamic_invariant (temp___expr_46 : int) (temp___is_init_42 : bool) (temp___skip_constant_43 : bool) (temp___do_toplevel_44 : bool) (temp___do_typ_inv_45 : bool) }
 
 predicate default_initial_assumption [@inline] 
   (temp___expr_47 : int) (temp___skip_top_level_48 : bool) =
  true
 val default_initial_assumption [@inline] 
   (temp___expr_47 : int) (temp___skip_top_level_48 : bool) : bool
  ensures { result <-> default_initial_assumption (temp___expr_47 : int) (temp___skip_top_level_48 : bool) }

end

(* Module giving axioms for type "short_float", created in Gnat2Why.Types.Generate_Type_Completion *)
module Standard__short_float___axiom
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "_gnatprove_standard".Float32 as Float32
 use        Standard__short_float as Standard__short_float

 predicate dynamic_invariant [@inline] 
   (temp___expr_53 : Float32.t) (temp___is_init_49 : bool) (temp___skip_constant_50 : bool) (temp___do_toplevel_51 : bool) (temp___do_typ_inv_52 : bool) =
  (if (( temp___is_init_49 \/ (Float32.le Standard__short_float.first Standard__short_float.last) )) then (
   (Standard__short_float.dynamic_property Standard__short_float.first Standard__short_float.last temp___expr_53)) else true)
 val dynamic_invariant [@inline] 
   (temp___expr_53 : Float32.t) (temp___is_init_49 : bool) (temp___skip_constant_50 : bool) (temp___do_toplevel_51 : bool) (temp___do_typ_inv_52 : bool) : bool
  ensures { result <-> dynamic_invariant (temp___expr_53 : Float32.t) (temp___is_init_49 : bool) (temp___skip_constant_50 : bool) (temp___do_toplevel_51 : bool) (temp___do_typ_inv_52 : bool) }
 
 predicate default_initial_assumption [@inline] 
   (temp___expr_54 : Float32.t) (temp___skip_top_level_55 : bool) =
  true
 val default_initial_assumption [@inline] 
   (temp___expr_54 : Float32.t) (temp___skip_top_level_55 : bool) : bool
  ensures { result <-> default_initial_assumption (temp___expr_54 : Float32.t) (temp___skip_top_level_55 : bool) }

end

(* Module giving axioms for type "float", created in Gnat2Why.Types.Generate_Type_Completion *)
module Standard__float___axiom
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "_gnatprove_standard".Float32 as Float32
 use        Standard__float as Standard__float

 predicate dynamic_invariant [@inline] 
   (temp___expr_60 : Float32.t) (temp___is_init_56 : bool) (temp___skip_constant_57 : bool) (temp___do_toplevel_58 : bool) (temp___do_typ_inv_59 : bool) =
  (if (( temp___is_init_56 \/ (Float32.le Standard__float.first Standard__float.last) )) then (
   (Standard__float.dynamic_property Standard__float.first Standard__float.last temp___expr_60)) else true)
 val dynamic_invariant [@inline] 
   (temp___expr_60 : Float32.t) (temp___is_init_56 : bool) (temp___skip_constant_57 : bool) (temp___do_toplevel_58 : bool) (temp___do_typ_inv_59 : bool) : bool
  ensures { result <-> dynamic_invariant (temp___expr_60 : Float32.t) (temp___is_init_56 : bool) (temp___skip_constant_57 : bool) (temp___do_toplevel_58 : bool) (temp___do_typ_inv_59 : bool) }
 
 predicate default_initial_assumption [@inline] 
   (temp___expr_61 : Float32.t) (temp___skip_top_level_62 : bool) =
  true
 val default_initial_assumption [@inline] 
   (temp___expr_61 : Float32.t) (temp___skip_top_level_62 : bool) : bool
  ensures { result <-> default_initial_assumption (temp___expr_61 : Float32.t) (temp___skip_top_level_62 : bool) }

end

(* Module giving axioms for type "long_float", created in Gnat2Why.Types.Generate_Type_Completion *)
module Standard__long_float___axiom
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "_gnatprove_standard".Float64 as Float64
 use        Standard__long_float as Standard__long_float

 predicate dynamic_invariant [@inline] 
   (temp___expr_67 : Float64.t) (temp___is_init_63 : bool) (temp___skip_constant_64 : bool) (temp___do_toplevel_65 : bool) (temp___do_typ_inv_66 : bool) =
  (if (( temp___is_init_63 \/ (Float64.le Standard__long_float.first Standard__long_float.last) )) then (
   (Standard__long_float.dynamic_property Standard__long_float.first Standard__long_float.last temp___expr_67)) else true)
 val dynamic_invariant [@inline] 
   (temp___expr_67 : Float64.t) (temp___is_init_63 : bool) (temp___skip_constant_64 : bool) (temp___do_toplevel_65 : bool) (temp___do_typ_inv_66 : bool) : bool
  ensures { result <-> dynamic_invariant (temp___expr_67 : Float64.t) (temp___is_init_63 : bool) (temp___skip_constant_64 : bool) (temp___do_toplevel_65 : bool) (temp___do_typ_inv_66 : bool) }
 
 predicate default_initial_assumption [@inline] 
   (temp___expr_68 : Float64.t) (temp___skip_top_level_69 : bool) =
  true
 val default_initial_assumption [@inline] 
   (temp___expr_68 : Float64.t) (temp___skip_top_level_69 : bool) : bool
  ensures { result <-> default_initial_assumption (temp___expr_68 : Float64.t) (temp___skip_top_level_69 : bool) }

end

(* Module giving axioms for type "character", created in Gnat2Why.Types.Generate_Type_Completion *)
module Standard__character___axiom
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int
 use        Standard__character as Standard__character

 predicate dynamic_invariant [@inline] 
   (temp___expr_74 : int) (temp___is_init_70 : bool) (temp___skip_constant_71 : bool) (temp___do_toplevel_72 : bool) (temp___do_typ_inv_73 : bool) =
  (if (( temp___is_init_70 \/ (Standard__character.first <= Standard__character.last) )) then (
   (Standard__character.dynamic_property Standard__character.first Standard__character.last temp___expr_74)) else true)
 val dynamic_invariant [@inline] 
   (temp___expr_74 : int) (temp___is_init_70 : bool) (temp___skip_constant_71 : bool) (temp___do_toplevel_72 : bool) (temp___do_typ_inv_73 : bool) : bool
  ensures { result <-> dynamic_invariant (temp___expr_74 : int) (temp___is_init_70 : bool) (temp___skip_constant_71 : bool) (temp___do_toplevel_72 : bool) (temp___do_typ_inv_73 : bool) }
 
 predicate default_initial_assumption [@inline] 
   (temp___expr_75 : int) (temp___skip_top_level_76 : bool) =
  true
 val default_initial_assumption [@inline] 
   (temp___expr_75 : int) (temp___skip_top_level_76 : bool) : bool
  ensures { result <-> default_initial_assumption (temp___expr_75 : int) (temp___skip_top_level_76 : bool) }

end

(* Module giving axioms for type "wide_character", created in Gnat2Why.Types.Generate_Type_Completion *)
module Standard__wide_character___axiom
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int
 use        Standard__wide_character as Standard__wide_character

 predicate dynamic_invariant [@inline] 
   (temp___expr_81 : int) (temp___is_init_77 : bool) (temp___skip_constant_78 : bool) (temp___do_toplevel_79 : bool) (temp___do_typ_inv_80 : bool) =
  (if (( temp___is_init_77 \/ (Standard__wide_character.first <= Standard__wide_character.last) )) then (
   (Standard__wide_character.dynamic_property Standard__wide_character.first Standard__wide_character.last temp___expr_81)) else true)
 val dynamic_invariant [@inline] 
   (temp___expr_81 : int) (temp___is_init_77 : bool) (temp___skip_constant_78 : bool) (temp___do_toplevel_79 : bool) (temp___do_typ_inv_80 : bool) : bool
  ensures { result <-> dynamic_invariant (temp___expr_81 : int) (temp___is_init_77 : bool) (temp___skip_constant_78 : bool) (temp___do_toplevel_79 : bool) (temp___do_typ_inv_80 : bool) }
 
 predicate default_initial_assumption [@inline] 
   (temp___expr_82 : int) (temp___skip_top_level_83 : bool) =
  true
 val default_initial_assumption [@inline] 
   (temp___expr_82 : int) (temp___skip_top_level_83 : bool) : bool
  ensures { result <-> default_initial_assumption (temp___expr_82 : int) (temp___skip_top_level_83 : bool) }

end

(* Module giving axioms for type "wide_wide_character", created in Gnat2Why.Types.Generate_Type_Completion *)
module Standard__wide_wide_character___axiom
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int
 use        Standard__wide_wide_character as Standard__wide_wide_character

 predicate dynamic_invariant [@inline] 
   (temp___expr_88 : int) (temp___is_init_84 : bool) (temp___skip_constant_85 : bool) (temp___do_toplevel_86 : bool) (temp___do_typ_inv_87 : bool) =
  (if (( temp___is_init_84 \/ (Standard__wide_wide_character.first <= Standard__wide_wide_character.last) )) then (
   (Standard__wide_wide_character.dynamic_property Standard__wide_wide_character.first Standard__wide_wide_character.last temp___expr_88)) else true)
 val dynamic_invariant [@inline] 
   (temp___expr_88 : int) (temp___is_init_84 : bool) (temp___skip_constant_85 : bool) (temp___do_toplevel_86 : bool) (temp___do_typ_inv_87 : bool) : bool
  ensures { result <-> dynamic_invariant (temp___expr_88 : int) (temp___is_init_84 : bool) (temp___skip_constant_85 : bool) (temp___do_toplevel_86 : bool) (temp___do_typ_inv_87 : bool) }
 
 predicate default_initial_assumption [@inline] 
   (temp___expr_89 : int) (temp___skip_top_level_90 : bool) =
  true
 val default_initial_assumption [@inline] 
   (temp___expr_89 : int) (temp___skip_top_level_90 : bool) : bool
  ensures { result <-> default_initial_assumption (temp___expr_89 : int) (temp___skip_top_level_90 : bool) }

end

(* Module giving axioms for type "string", created in Gnat2Why.Types.Generate_Type_Completion *)
module Standard__string___axiom
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        Standard__positive as Standard__positive
 use        Standard__string as Standard__string

 predicate dynamic_invariant [@inline] 
   (temp___expr_96 : Standard__string.string) (temp___is_init_92 : bool) (temp___skip_constant_93 : bool) (temp___do_toplevel_94 : bool) (temp___do_typ_inv_95 : bool) =
  (if (temp___skip_constant_93) then (
   true) else (
   (Standard__string.dynamic_property Standard__positive.first Standard__positive.last (Standard__string.first temp___expr_96) (Standard__string.last temp___expr_96))))
 val dynamic_invariant [@inline] 
   (temp___expr_96 : Standard__string.string) (temp___is_init_92 : bool) (temp___skip_constant_93 : bool) (temp___do_toplevel_94 : bool) (temp___do_typ_inv_95 : bool) : bool
  ensures { result <-> dynamic_invariant (temp___expr_96 : Standard__string.string) (temp___is_init_92 : bool) (temp___skip_constant_93 : bool) (temp___do_toplevel_94 : bool) (temp___do_typ_inv_95 : bool) }

end

(* Module giving axioms for type "wide_string", created in Gnat2Why.Types.Generate_Type_Completion *)
module Standard__wide_string___axiom
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        Standard__positive as Standard__positive
 use        Standard__wide_string as Standard__wide_string

 predicate dynamic_invariant [@inline] 
   (temp___expr_103 : Standard__wide_string.wide_string) (temp___is_init_99 : bool) (temp___skip_constant_100 : bool) (temp___do_toplevel_101 : bool) (temp___do_typ_inv_102 : bool) =
  (if (temp___skip_constant_100) then (
   true) else (
   (Standard__wide_string.dynamic_property Standard__positive.first Standard__positive.last (Standard__wide_string.first temp___expr_103) (Standard__wide_string.last temp___expr_103))))
 val dynamic_invariant [@inline] 
   (temp___expr_103 : Standard__wide_string.wide_string) (temp___is_init_99 : bool) (temp___skip_constant_100 : bool) (temp___do_toplevel_101 : bool) (temp___do_typ_inv_102 : bool) : bool
  ensures { result <-> dynamic_invariant (temp___expr_103 : Standard__wide_string.wide_string) (temp___is_init_99 : bool) (temp___skip_constant_100 : bool) (temp___do_toplevel_101 : bool) (temp___do_typ_inv_102 : bool) }

end

(* Module giving axioms for type "wide_wide_string", created in Gnat2Why.Types.Generate_Type_Completion *)
module Standard__wide_wide_string___axiom
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        Standard__positive as Standard__positive
 use        Standard__wide_wide_string as Standard__wide_wide_string

 predicate dynamic_invariant [@inline] 
   (temp___expr_110 : Standard__wide_wide_string.wide_wide_string) (temp___is_init_106 : bool) (temp___skip_constant_107 : bool) (temp___do_toplevel_108 : bool) (temp___do_typ_inv_109 : bool) =
  (if (temp___skip_constant_107) then (
   true) else (
   (Standard__wide_wide_string.dynamic_property Standard__positive.first Standard__positive.last (Standard__wide_wide_string.first temp___expr_110) (Standard__wide_wide_string.last temp___expr_110))))
 val dynamic_invariant [@inline] 
   (temp___expr_110 : Standard__wide_wide_string.wide_wide_string) (temp___is_init_106 : bool) (temp___skip_constant_107 : bool) (temp___do_toplevel_108 : bool) (temp___do_typ_inv_109 : bool) : bool
  ensures { result <-> dynamic_invariant (temp___expr_110 : Standard__wide_wide_string.wide_wide_string) (temp___is_init_106 : bool) (temp___skip_constant_107 : bool) (temp___do_toplevel_108 : bool) (temp___do_typ_inv_109 : bool) }

end

(* Module giving axioms for type "duration", created in Gnat2Why.Types.Generate_Type_Completion *)
module Standard__duration___axiom
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int
 use        "_gnatprove_standard".Main as Main
 use        Standard__duration as Standard__duration

 predicate dynamic_invariant [@inline] 
   (temp___expr_116 : Main.__fixed) (temp___is_init_112 : bool) (temp___skip_constant_113 : bool) (temp___do_toplevel_114 : bool) (temp___do_typ_inv_115 : bool) =
  (if (( temp___is_init_112 \/ (Standard__duration.first <= Standard__duration.last) )) then (
   (Standard__duration.dynamic_property Standard__duration.first Standard__duration.last temp___expr_116)) else true)
 val dynamic_invariant [@inline] 
   (temp___expr_116 : Main.__fixed) (temp___is_init_112 : bool) (temp___skip_constant_113 : bool) (temp___do_toplevel_114 : bool) (temp___do_typ_inv_115 : bool) : bool
  ensures { result <-> dynamic_invariant (temp___expr_116 : Main.__fixed) (temp___is_init_112 : bool) (temp___skip_constant_113 : bool) (temp___do_toplevel_114 : bool) (temp___do_typ_inv_115 : bool) }
 
 predicate default_initial_assumption [@inline] 
   (temp___expr_117 : Main.__fixed) (temp___skip_top_level_118 : bool) =
  true
 val default_initial_assumption [@inline] 
   (temp___expr_117 : Main.__fixed) (temp___skip_top_level_118 : bool) : bool
  ensures { result <-> default_initial_assumption (temp___expr_117 : Main.__fixed) (temp___skip_top_level_118 : bool) }

end

(* Module giving axioms for type "integer_8", created in Gnat2Why.Types.Generate_Type_Completion *)
module Standard__integer_8___axiom
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int
 use        Standard__integer_8 as Standard__integer_8

 predicate dynamic_invariant [@inline] 
   (temp___expr_123 : int) (temp___is_init_119 : bool) (temp___skip_constant_120 : bool) (temp___do_toplevel_121 : bool) (temp___do_typ_inv_122 : bool) =
  (if (( temp___is_init_119 \/ (Standard__integer_8.first <= Standard__integer_8.last) )) then (
   (Standard__integer_8.dynamic_property Standard__integer_8.first Standard__integer_8.last temp___expr_123)) else true)
 val dynamic_invariant [@inline] 
   (temp___expr_123 : int) (temp___is_init_119 : bool) (temp___skip_constant_120 : bool) (temp___do_toplevel_121 : bool) (temp___do_typ_inv_122 : bool) : bool
  ensures { result <-> dynamic_invariant (temp___expr_123 : int) (temp___is_init_119 : bool) (temp___skip_constant_120 : bool) (temp___do_toplevel_121 : bool) (temp___do_typ_inv_122 : bool) }
 
 predicate default_initial_assumption [@inline] 
   (temp___expr_124 : int) (temp___skip_top_level_125 : bool) =
  true
 val default_initial_assumption [@inline] 
   (temp___expr_124 : int) (temp___skip_top_level_125 : bool) : bool
  ensures { result <-> default_initial_assumption (temp___expr_124 : int) (temp___skip_top_level_125 : bool) }

end

(* Module giving axioms for type "integer_16", created in Gnat2Why.Types.Generate_Type_Completion *)
module Standard__integer_16___axiom
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int
 use        Standard__integer_16 as Standard__integer_16

 predicate dynamic_invariant [@inline] 
   (temp___expr_130 : int) (temp___is_init_126 : bool) (temp___skip_constant_127 : bool) (temp___do_toplevel_128 : bool) (temp___do_typ_inv_129 : bool) =
  (if (( temp___is_init_126 \/ (Standard__integer_16.first <= Standard__integer_16.last) )) then (
   (Standard__integer_16.dynamic_property Standard__integer_16.first Standard__integer_16.last temp___expr_130)) else true)
 val dynamic_invariant [@inline] 
   (temp___expr_130 : int) (temp___is_init_126 : bool) (temp___skip_constant_127 : bool) (temp___do_toplevel_128 : bool) (temp___do_typ_inv_129 : bool) : bool
  ensures { result <-> dynamic_invariant (temp___expr_130 : int) (temp___is_init_126 : bool) (temp___skip_constant_127 : bool) (temp___do_toplevel_128 : bool) (temp___do_typ_inv_129 : bool) }
 
 predicate default_initial_assumption [@inline] 
   (temp___expr_131 : int) (temp___skip_top_level_132 : bool) =
  true
 val default_initial_assumption [@inline] 
   (temp___expr_131 : int) (temp___skip_top_level_132 : bool) : bool
  ensures { result <-> default_initial_assumption (temp___expr_131 : int) (temp___skip_top_level_132 : bool) }

end

(* Module giving axioms for type "integer_32", created in Gnat2Why.Types.Generate_Type_Completion *)
module Standard__integer_32___axiom
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int
 use        Standard__integer_32 as Standard__integer_32

 predicate dynamic_invariant [@inline] 
   (temp___expr_137 : int) (temp___is_init_133 : bool) (temp___skip_constant_134 : bool) (temp___do_toplevel_135 : bool) (temp___do_typ_inv_136 : bool) =
  (if (( temp___is_init_133 \/ (Standard__integer_32.first <= Standard__integer_32.last) )) then (
   (Standard__integer_32.dynamic_property Standard__integer_32.first Standard__integer_32.last temp___expr_137)) else true)
 val dynamic_invariant [@inline] 
   (temp___expr_137 : int) (temp___is_init_133 : bool) (temp___skip_constant_134 : bool) (temp___do_toplevel_135 : bool) (temp___do_typ_inv_136 : bool) : bool
  ensures { result <-> dynamic_invariant (temp___expr_137 : int) (temp___is_init_133 : bool) (temp___skip_constant_134 : bool) (temp___do_toplevel_135 : bool) (temp___do_typ_inv_136 : bool) }
 
 predicate default_initial_assumption [@inline] 
   (temp___expr_138 : int) (temp___skip_top_level_139 : bool) =
  true
 val default_initial_assumption [@inline] 
   (temp___expr_138 : int) (temp___skip_top_level_139 : bool) : bool
  ensures { result <-> default_initial_assumption (temp___expr_138 : int) (temp___skip_top_level_139 : bool) }

end

(* Module giving axioms for type "integer_64", created in Gnat2Why.Types.Generate_Type_Completion *)
module Standard__integer_64___axiom
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int
 use        Standard__integer_64 as Standard__integer_64

 predicate dynamic_invariant [@inline] 
   (temp___expr_144 : int) (temp___is_init_140 : bool) (temp___skip_constant_141 : bool) (temp___do_toplevel_142 : bool) (temp___do_typ_inv_143 : bool) =
  (if (( temp___is_init_140 \/ (Standard__integer_64.first <= Standard__integer_64.last) )) then (
   (Standard__integer_64.dynamic_property Standard__integer_64.first Standard__integer_64.last temp___expr_144)) else true)
 val dynamic_invariant [@inline] 
   (temp___expr_144 : int) (temp___is_init_140 : bool) (temp___skip_constant_141 : bool) (temp___do_toplevel_142 : bool) (temp___do_typ_inv_143 : bool) : bool
  ensures { result <-> dynamic_invariant (temp___expr_144 : int) (temp___is_init_140 : bool) (temp___skip_constant_141 : bool) (temp___do_toplevel_142 : bool) (temp___do_typ_inv_143 : bool) }
 
 predicate default_initial_assumption [@inline] 
   (temp___expr_145 : int) (temp___skip_top_level_146 : bool) =
  true
 val default_initial_assumption [@inline] 
   (temp___expr_145 : int) (temp___skip_top_level_146 : bool) : bool
  ensures { result <-> default_initial_assumption (temp___expr_145 : int) (temp___skip_top_level_146 : bool) }

end

(* Module giving axioms for type "universal_integer", created in Gnat2Why.Types.Generate_Type_Completion *)
module Standard__universal_integer___axiom
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int
 use        Standard__universal_integer as Standard__universal_integer

 predicate dynamic_invariant [@inline] 
   (temp___expr_151 : int) (temp___is_init_147 : bool) (temp___skip_constant_148 : bool) (temp___do_toplevel_149 : bool) (temp___do_typ_inv_150 : bool) =
  (if (( temp___is_init_147 \/ (Standard__universal_integer.first <= Standard__universal_integer.last) )) then (
   (Standard__universal_integer.dynamic_property Standard__universal_integer.first Standard__universal_integer.last temp___expr_151)) else true)
 val dynamic_invariant [@inline] 
   (temp___expr_151 : int) (temp___is_init_147 : bool) (temp___skip_constant_148 : bool) (temp___do_toplevel_149 : bool) (temp___do_typ_inv_150 : bool) : bool
  ensures { result <-> dynamic_invariant (temp___expr_151 : int) (temp___is_init_147 : bool) (temp___skip_constant_148 : bool) (temp___do_toplevel_149 : bool) (temp___do_typ_inv_150 : bool) }
 
 predicate default_initial_assumption [@inline] 
   (temp___expr_152 : int) (temp___skip_top_level_153 : bool) =
  true
 val default_initial_assumption [@inline] 
   (temp___expr_152 : int) (temp___skip_top_level_153 : bool) : bool
  ensures { result <-> default_initial_assumption (temp___expr_152 : int) (temp___skip_top_level_153 : bool) }

end

(* Module for possibly declaring a logic function for "tokenise" defined at mystringtokeniser.ads:70, created in Gnat2Why.Subprograms.Translate_Subprogram_Spec *)
module Mystringtokeniser__tokenise
 use        "_gnatprove_standard".Main
 use        "int".Int


end

(* Module for defining the loop exit exception for the loop "L_1" defined at mystringtokeniser.adb:50, created in Gnat2Why.Decls.Translate_Loop_Entity *)
module Mystringtokeniser__tokenise__L_1
 use        "_gnatprove_standard".Main
 use        "int".Int

 exception L_1

end

(* Module giving an empty axiom for the entity "L_1" defined at mystringtokeniser.adb:50, created in Gnat2Why.Driver.Translate_Entity.Generate_Empty_Axiom_Theory *)
module Mystringtokeniser__tokenise__L_1___axiom
 use        "_gnatprove_standard".Main
 use        "int".Int


end

(* Module for defining the loop exit exception for the loop "L_2" defined at mystringtokeniser.adb:60, created in Gnat2Why.Decls.Translate_Loop_Entity *)
module Mystringtokeniser__tokenise__L_2
 use        "_gnatprove_standard".Main
 use        "int".Int

 exception L_2

end

(* Module giving an empty axiom for the entity "L_2" defined at mystringtokeniser.adb:60, created in Gnat2Why.Driver.Translate_Entity.Generate_Empty_Axiom_Theory *)
module Mystringtokeniser__tokenise__L_2___axiom
 use        "_gnatprove_standard".Main
 use        "int".Int


end

(* Module for defining the loop exit exception for the loop "L_3" defined at mystringtokeniser.adb:69, created in Gnat2Why.Decls.Translate_Loop_Entity *)
module Mystringtokeniser__tokenise__L_3
 use        "_gnatprove_standard".Main
 use        "int".Int

 exception L_3

end

(* Module giving an empty axiom for the entity "L_3" defined at mystringtokeniser.adb:69, created in Gnat2Why.Driver.Translate_Entity.Generate_Empty_Axiom_Theory *)
module Mystringtokeniser__tokenise__L_3___axiom
 use        "_gnatprove_standard".Main
 use        "int".Int


end

(* Module giving axioms for type "tokenextent" defined at mystringtokeniser.ads:5, created in Gnat2Why.Types.Generate_Type_Completion *)
module Mystringtokeniser__tokenextent___axiom
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        Mystringtokeniser__tokenextent as Mystringtokeniser__tokenextent

 predicate dynamic_invariant [@inline] 
   (temp___expr_160 : Mystringtokeniser__tokenextent.tokenextent) (temp___is_init_156 : bool) (temp___skip_constant_157 : bool) (temp___do_toplevel_158 : bool) (temp___do_typ_inv_159 : bool) =
  true
 val dynamic_invariant [@inline] 
   (temp___expr_160 : Mystringtokeniser__tokenextent.tokenextent) (temp___is_init_156 : bool) (temp___skip_constant_157 : bool) (temp___do_toplevel_158 : bool) (temp___do_typ_inv_159 : bool) : bool
  ensures { result <-> dynamic_invariant (temp___expr_160 : Mystringtokeniser__tokenextent.tokenextent) (temp___is_init_156 : bool) (temp___skip_constant_157 : bool) (temp___do_toplevel_158 : bool) (temp___do_typ_inv_159 : bool) }
 
 predicate default_initial_assumption [@inline] 
   (temp___expr_161 : Mystringtokeniser__tokenextent.tokenextent) (temp___skip_top_level_162 : bool) =
  true
 val default_initial_assumption [@inline] 
   (temp___expr_161 : Mystringtokeniser__tokenextent.tokenextent) (temp___skip_top_level_162 : bool) : bool
  ensures { result <-> default_initial_assumption (temp___expr_161 : Mystringtokeniser__tokenextent.tokenextent) (temp___skip_top_level_162 : bool) }

end

(* Module giving axioms for type "tokenarray" defined at mystringtokeniser.ads:10, created in Gnat2Why.Types.Generate_Type_Completion *)
module Mystringtokeniser__tokenarray___axiom
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        Standard__positive as Standard__positive
 use        Mystringtokeniser__tokenarray as Mystringtokeniser__tokenarray

 predicate dynamic_invariant [@inline] 
   (temp___expr_169 : Mystringtokeniser__tokenarray.tokenarray) (temp___is_init_165 : bool) (temp___skip_constant_166 : bool) (temp___do_toplevel_167 : bool) (temp___do_typ_inv_168 : bool) =
  (if (temp___skip_constant_166) then (
   true) else (
   (Mystringtokeniser__tokenarray.dynamic_property Standard__positive.first Standard__positive.last (Mystringtokeniser__tokenarray.first temp___expr_169) (Mystringtokeniser__tokenarray.last temp___expr_169))))
 val dynamic_invariant [@inline] 
   (temp___expr_169 : Mystringtokeniser__tokenarray.tokenarray) (temp___is_init_165 : bool) (temp___skip_constant_166 : bool) (temp___do_toplevel_167 : bool) (temp___do_typ_inv_168 : bool) : bool
  ensures { result <-> dynamic_invariant (temp___expr_169 : Mystringtokeniser__tokenarray.tokenarray) (temp___is_init_165 : bool) (temp___skip_constant_166 : bool) (temp___do_toplevel_167 : bool) (temp___do_typ_inv_168 : bool) }

end

(* Module giving axioms for type "T7s" defined at mystringtokeniser.ads:73, created in Gnat2Why.Types.Generate_Type_Completion *)
module Mystringtokeniser__tokenise__L6s__T7s___axiom
 use        "_gnatprove_standard".Main
 use        "int".Int


end

(* Module giving axioms for type "TS2bP1" defined at mystringtokeniser.adb:40, created in Gnat2Why.Types.Generate_Type_Completion *)
module Mystringtokeniser__tokenise__TS2bP1___axiom
 use        "_gnatprove_standard".Main
 use        "int".Int


end

(* Module giving axioms for type "S2b" defined at mystringtokeniser.adb:40, created in Gnat2Why.Types.Generate_Type_Completion *)
module Mystringtokeniser__tokenise__S2b___axiom
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        Standard__integer__rep as Standard__integer__rep
 use        Mystringtokeniser__tokenise__tokens as Mystringtokeniser__tokenise__tokens
 use        Mystringtokeniser__tokenise__S2b as Mystringtokeniser__tokenise__S2b

 predicate dynamic_invariant [@inline] 
   (temp___expr_179 : Mystringtokeniser__tokenise__S2b.s2b) (temp___is_init_175 : bool) (temp___skip_constant_176 : bool) (temp___do_toplevel_177 : bool) (temp___do_typ_inv_178 : bool) =
  (if (temp___skip_constant_176) then (
   true) else (
   ( (Mystringtokeniser__tokenise__S2b.dynamic_property (Standard__integer__rep.to_rep Mystringtokeniser__tokenise__tokens.tokens__first) (Standard__integer__rep.to_rep Mystringtokeniser__tokenise__tokens.tokens__last) (Mystringtokeniser__tokenise__S2b.first temp___expr_179) (Mystringtokeniser__tokenise__S2b.last temp___expr_179)) /\ ( ((Mystringtokeniser__tokenise__S2b.first temp___expr_179) = (Standard__integer__rep.to_rep Mystringtokeniser__tokenise__tokens.tokens__first)) /\ ((Mystringtokeniser__tokenise__S2b.last temp___expr_179) = (Standard__integer__rep.to_rep Mystringtokeniser__tokenise__tokens.tokens__last)) ) )))
 val dynamic_invariant [@inline] 
   (temp___expr_179 : Mystringtokeniser__tokenise__S2b.s2b) (temp___is_init_175 : bool) (temp___skip_constant_176 : bool) (temp___do_toplevel_177 : bool) (temp___do_typ_inv_178 : bool) : bool
  ensures { result <-> dynamic_invariant (temp___expr_179 : Mystringtokeniser__tokenise__S2b.s2b) (temp___is_init_175 : bool) (temp___skip_constant_176 : bool) (temp___do_toplevel_177 : bool) (temp___do_typ_inv_178 : bool) }
 
 predicate default_initial_assumption [@inline] 
   (temp___expr_181 : Mystringtokeniser__tokenise__S2b.s2b) (temp___skip_top_level_182 : bool) =
  ( ( true /\ ((Mystringtokeniser__tokenise__S2b.first temp___expr_181) = (Standard__integer__rep.to_rep Mystringtokeniser__tokenise__tokens.tokens__first)) )/\((Mystringtokeniser__tokenise__S2b.last temp___expr_181) = (Standard__integer__rep.to_rep Mystringtokeniser__tokenise__tokens.tokens__last)) )
 val default_initial_assumption [@inline] 
   (temp___expr_181 : Mystringtokeniser__tokenise__S2b.s2b) (temp___skip_top_level_182 : bool) : bool
  ensures { result <-> default_initial_assumption (temp___expr_181 : Mystringtokeniser__tokenise__S2b.s2b) (temp___skip_top_level_182 : bool) }

end

(* Module giving axioms for type "TS1bP1" defined at mystringtokeniser.adb:40, created in Gnat2Why.Types.Generate_Type_Completion *)
module Mystringtokeniser__tokenise__TS1bP1___axiom
 use        "_gnatprove_standard".Main
 use        "int".Int


end

(* Module giving axioms for type "S1b" defined at mystringtokeniser.adb:40, created in Gnat2Why.Types.Generate_Type_Completion *)
module Mystringtokeniser__tokenise__S1b___axiom
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        Standard__string as Standard__string
 use        Mystringtokeniser__tokenise__s as Mystringtokeniser__tokenise__s
 use        Mystringtokeniser__tokenise__S1b as Mystringtokeniser__tokenise__S1b

 predicate dynamic_invariant [@inline] 
   (temp___expr_191 : Mystringtokeniser__tokenise__S1b.s1b) (temp___is_init_187 : bool) (temp___skip_constant_188 : bool) (temp___do_toplevel_189 : bool) (temp___do_typ_inv_190 : bool) =
  (if (temp___skip_constant_188) then (
   true) else (
   ( (Mystringtokeniser__tokenise__S1b.dynamic_property (Standard__string.first Mystringtokeniser__tokenise__s.s) (Standard__string.last Mystringtokeniser__tokenise__s.s) (Mystringtokeniser__tokenise__S1b.first temp___expr_191) (Mystringtokeniser__tokenise__S1b.last temp___expr_191)) /\ ( ((Mystringtokeniser__tokenise__S1b.first temp___expr_191) = (Standard__string.first Mystringtokeniser__tokenise__s.s)) /\ ((Mystringtokeniser__tokenise__S1b.last temp___expr_191) = (Standard__string.last Mystringtokeniser__tokenise__s.s)) ) )))
 val dynamic_invariant [@inline] 
   (temp___expr_191 : Mystringtokeniser__tokenise__S1b.s1b) (temp___is_init_187 : bool) (temp___skip_constant_188 : bool) (temp___do_toplevel_189 : bool) (temp___do_typ_inv_190 : bool) : bool
  ensures { result <-> dynamic_invariant (temp___expr_191 : Mystringtokeniser__tokenise__S1b.s1b) (temp___is_init_187 : bool) (temp___skip_constant_188 : bool) (temp___do_toplevel_189 : bool) (temp___do_typ_inv_190 : bool) }
 
 predicate default_initial_assumption [@inline] 
   (temp___expr_193 : Mystringtokeniser__tokenise__S1b.s1b) (temp___skip_top_level_194 : bool) =
  ( ( true /\ ((Mystringtokeniser__tokenise__S1b.first temp___expr_193) = (Standard__string.first Mystringtokeniser__tokenise__s.s)) )/\((Mystringtokeniser__tokenise__S1b.last temp___expr_193) = (Standard__string.last Mystringtokeniser__tokenise__s.s)) )
 val default_initial_assumption [@inline] 
   (temp___expr_193 : Mystringtokeniser__tokenise__S1b.s1b) (temp___skip_top_level_194 : bool) : bool
  ensures { result <-> default_initial_assumption (temp___expr_193 : Mystringtokeniser__tokenise__S1b.s1b) (temp___skip_top_level_194 : bool) }

end

(* Module giving axioms for type "T7b" defined at mystringtokeniser.adb:52, created in Gnat2Why.Types.Generate_Type_Completion *)
module Mystringtokeniser__tokenise__L_1__L6b__T7b___axiom
 use        "_gnatprove_standard".Main
 use        "int".Int


end

(* Module giving axioms for type "T5b" defined at mystringtokeniser.adb:52, created in Gnat2Why.Types.Generate_Type_Completion *)
module Mystringtokeniser__tokenise__L_1__L4b__T5b___axiom
 use        "_gnatprove_standard".Main
 use        "int".Int


end
(* Module for defining the value of constant "lf" defined at a-chlat1.ads:35, created in Gnat2Why.Decls.Translate_Constant_Value *)
module Ada__characters__latin_1__lf___axiom
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int
 use        Ada__characters__latin_1__lf as Ada__characters__latin_1__lf

 axiom lf__def_axiom :
  ((Ada__characters__latin_1__lf.lf ) = (10 : int))

end

(* Module for defining the value of constant "ht" defined at a-chlat1.ads:34, created in Gnat2Why.Decls.Translate_Constant_Value *)
module Ada__characters__latin_1__ht___axiom
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int
 use        Ada__characters__latin_1__ht as Ada__characters__latin_1__ht

 axiom ht__def_axiom :
  ((Ada__characters__latin_1__ht.ht ) = (9 : int))

end

(* Module giving a program function and a defining axiom for the expression function "is_whitespace" defined at mystringtokeniser.ads:12, created in Gnat2Why.Subprograms.Translate_Expression_Function_Body *)
module Mystringtokeniser__is_whitespace___axiom
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int
 use        Standard__character___axiom as Standard__character___axiom
 use        Mystringtokeniser__is_whitespace as Mystringtokeniser__is_whitespace

 val is_whitespace 
   (ch : int) : bool
  requires {  true }
  ensures {  ( ( (result = (Mystringtokeniser__is_whitespace.is_whitespace ch)) /\ (Mystringtokeniser__is_whitespace.is_whitespace__function_guard result ch) )/\( (result = True) <-> ( ( (ch = (32 : int)) \/ (ch = (10 : int)) ) \/ (ch = (9 : int)) ) ) ) }
 
 axiom is_whitespace__post_axiom :
  (forall ch   : int [(Mystringtokeniser__is_whitespace.is_whitespace ch)].
   ( (Standard__character___axiom.dynamic_invariant ch True True True True) -> (let result = (Mystringtokeniser__is_whitespace.is_whitespace ch) in (
    (if ((Mystringtokeniser__is_whitespace.is_whitespace__function_guard result ch)) then (
     ( ( true /\ true )/\true )) else true)))
    ))
 
 axiom is_whitespace__def_axiom :
  (forall ch   : int [(Mystringtokeniser__is_whitespace.is_whitespace ch)].
   ( (let result = (Mystringtokeniser__is_whitespace.is_whitespace ch) in (
    (Mystringtokeniser__is_whitespace.is_whitespace__function_guard result ch)))
    -> ( ((Mystringtokeniser__is_whitespace.is_whitespace ch) = True) <-> ( ( (ch = (32 : int)) \/ (ch = (10 : int)) ) \/ (ch = (9 : int)) ) ) ))

end

(* Module for declaring a program function (and possibly an axiom) for "tokenise" defined at mystringtokeniser.ads:70, created in Gnat2Why.Subprograms.Generate_Subprogram_Completion *)
module Mystringtokeniser__tokenise___axiom
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int
 use        "_gnatprove_standard".Integer as Integer
 use        Standard__integer as Standard__integer
 use        Standard__natural as Standard__natural
 use        Standard__natural___axiom as Standard__natural___axiom
 use        Standard__natural__rep as Standard__natural__rep
 use        Standard__positive as Standard__positive
 use        Standard__positive__rep as Standard__positive__rep
 use        Standard__string as Standard__string
 use        Standard__integer__rep as Standard__integer__rep
 use        Mystringtokeniser__tokenextent as Mystringtokeniser__tokenextent
 use        Mystringtokeniser__tokenextent___axiom as Mystringtokeniser__tokenextent___axiom
 use        Array__Int__Mystringtokeniser__tokenextent as Array__Int__Mystringtokeniser__tokenextent
 use        Mystringtokeniser__tokenarray as Mystringtokeniser__tokenarray
 use        Array__Int__Mystringtokeniser__tokenextent as Array__Int__Mystringtokeniser__tokenextent

 val tokenise 
   (s : Standard__string.string) (tokens : Array__Int__Mystringtokeniser__tokenextent.map__ref) (tokens__first : Standard__integer.integer) (tokens__last : Standard__integer.integer) (count : int__ref) : unit
  requires {  ( (if (((Standard__string.length s) > (0 : int))) then (
   ( [@GP_Pretty_Ada:2811] ((Standard__string.first s) <= (Standard__string.last s)) )) else (
   ( [@GP_Pretty_Ada:6213] true ))) /\ ( [@GP_Pretty_Ada:2820] ((Standard__integer__rep.to_rep tokens__first) <= (Standard__integer__rep.to_rep tokens__last)) ) ) }
  ensures {  ( ( ( [@GP_Pretty_Ada:2828] (count.int__content <= (Integer.length (Standard__integer__rep.to_rep tokens__first) (Standard__integer__rep.to_rep tokens__last))) ) /\ (forall index   [@model_trace:2833] [@name:Index]  : int.
   ( ( ((Standard__integer__rep.to_rep tokens__first) <= index) /\ (index <= ((Standard__integer__rep.to_rep tokens__first) + (count.int__content - (1 : int)))) ) -> ( ( ( [@GP_Pretty_Ada:2858] ((Standard__positive__rep.to_rep (Array__Int__Mystringtokeniser__tokenextent.get tokens.Array__Int__Mystringtokeniser__tokenextent.map__content index).Mystringtokeniser__tokenextent.__split_fields.Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__start) >= (Standard__string.first s)) ) /\ ( [@GP_Pretty_Ada:2868] ((Standard__natural__rep.to_rep (Array__Int__Mystringtokeniser__tokenextent.get tokens.Array__Int__Mystringtokeniser__tokenextent.map__content index).Mystringtokeniser__tokenextent.__split_fields.Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__length) > (0 : int)) ) ) /\ ( [@GP_Pretty_Ada:2878] (((Standard__natural__rep.to_rep (Array__Int__Mystringtokeniser__tokenextent.get tokens.Array__Int__Mystringtokeniser__tokenextent.map__content index).Mystringtokeniser__tokenextent.__split_fields.Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__length) - (1 : int)) <= ((Standard__string.last s) - (Standard__positive__rep.to_rep (Array__Int__Mystringtokeniser__tokenextent.get tokens.Array__Int__Mystringtokeniser__tokenextent.map__content index).Mystringtokeniser__tokenextent.__split_fields.Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__start))) ) ) )) ) /\ ( ( (if (True) then (
   true) else (
   (Mystringtokeniser__tokenarray.dynamic_property Standard__positive.first Standard__positive.last (Standard__integer__rep.to_rep tokens__first) (Standard__integer__rep.to_rep tokens__last)))) /\ (forall temp___172   : int.
   (if (( ((Standard__integer__rep.to_rep tokens__first) <= temp___172) /\ (temp___172 <= (Standard__integer__rep.to_rep tokens__last)) )) then (
    (Mystringtokeniser__tokenextent___axiom.dynamic_invariant (Array__Int__Mystringtokeniser__tokenextent.get tokens.Array__Int__Mystringtokeniser__tokenextent.map__content temp___172) True False True True)) else true)) ) /\ (Standard__natural___axiom.dynamic_invariant count.int__content True True True True) ) ) }
  writes {tokens, count}

end

(* Module for checking absence of run-time errors and package initial condition on package elaboration of "mystringtokeniser" defined at mystringtokeniser.ads:3, created in Gnat2Why.Subprograms.Generate_VCs_For_Package_Elaboration *)
module Mystringtokeniser__package_def
 use        "_gnatprove_standard".Main
 use        "int".Int

 let def [#"mystringtokeniser.ads" 3 0 0][@GP_Subp:mystringtokeniser.ads:3] 
   (__void_param : unit)
  requires { [#"mystringtokeniser.ads" 3 0 0] true }
   = [@vc:divergent]
  ( ();
  ([#"mystringtokeniser.ads" 3 0 0] ());
  ([#"mystringtokeniser.ads" 5 0 0] ());
  ([#"mystringtokeniser.ads" 10 0 0] ());
  ([#"mystringtokeniser.ads" 12 0 0] ());
  ([#"mystringtokeniser.ads" 70 0 0] ());
  ([#"mystringtokeniser.ads" 72 0 0] ());
  ([#"mystringtokeniser.ads" 71 0 0] ());
  ([#"mystringtokeniser.ads" 71 0 0] ());
  ([#"mystringtokeniser.ads" 71 0 0] ());
  ([#"mystringtokeniser.ads" 71 0 0] ());
  ([#"mystringtokeniser.ads" 12 0 0] ());
  ();
  ([#"mystringtokeniser.adb" 2 0 0] ());
  ([#"mystringtokeniser.adb" 40 0 0] ());
  ();
  ( [@GP_Sloc:mystringtokeniser.adb:93:1] ([#"mystringtokeniser.adb" 93 0 0] ()) ) )
end

(* Module for checking contracts and absence of run-time errors in subprogram "is_whitespace" defined at mystringtokeniser.ads:12, created in Gnat2Why.Subprograms.Generate_VCs_For_Subprogram *)
module Mystringtokeniser__is_whitespace__subprogram_def
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int
 use        "_gnatprove_standard".Boolean as Boolean
 use        Standard__character___axiom as Standard__character___axiom
 use        Mystringtokeniser__is_whitespace__ch as Mystringtokeniser__is_whitespace__ch
 use        Standard__character___axiom as Standard__character___axiom
 use        Mystringtokeniser__is_whitespace__ch___axiom as Mystringtokeniser__is_whitespace__ch___axiom

 val mystringtokeniser__is_whitespace__result [@model_projected] [@model_trace:2720@result] [@name:Is_Whitespace]  : bool__ref 
 
 let def [#"mystringtokeniser.ads" 12 0 0][@GP_Subp:mystringtokeniser.ads:12] 
   (__void_param : unit)
  requires { [#"mystringtokeniser.ads" 12 0 0] true }
   = [@vc:divergent]
  ( () (* Assume dynamic invariants of inputs of the subprogram mystringtokeniser.ads:12 *)
  ;
  [#"mystringtokeniser.ads" 12 0 0] assume { [#"mystringtokeniser.ads" 12 0 0] (Standard__character___axiom.dynamic_invariant Mystringtokeniser__is_whitespace__ch.ch True False True True) };
  () (* Declarations introduced by the compiler at the beginning of the subprogram mystringtokeniser.ads:12 *)
  ;
  () (* Check for RTE in the Pre of the subprogram mystringtokeniser.ads:12 *)
  ;
  [#"mystringtokeniser.ads" 12 0 0] begin ensures {true} let _ = (let _ = True in (
   ()))
   in () end ;
  () (* Assume Pre of the subprogram mystringtokeniser.ads:12 *)
  ;
  [#"mystringtokeniser.ads" 12 0 0] try
   ( ();
   ( [@GP_Sloc:mystringtokeniser.ads:13:50] ([#"mystringtokeniser.ads" 13 0 0] ( [#"mystringtokeniser.ads" 13 0 0] (mystringtokeniser__is_whitespace__result.bool__content <- ( (Boolean.orb((Boolean.orb((Mystringtokeniser__is_whitespace__ch.ch = (32 : int))) ((Mystringtokeniser__is_whitespace__ch.ch = (10 : int))))) ((Mystringtokeniser__is_whitespace__ch.ch = (9 : int)))) ));
   [#"mystringtokeniser.ads" 13 0 0] raise Return__exc )) );
    raise Return__exc )
  with
   Return__exc -> ()
  end;
   begin ensures {true} let _ = (let _ = True in (
   ()))
   in () end ;
  mystringtokeniser__is_whitespace__result.bool__content )
end

(* Module for checking contracts and absence of run-time errors in subprogram "tokenise" defined at mystringtokeniser.ads:70, created in Gnat2Why.Subprograms.Generate_VCs_For_Subprogram *)
module Mystringtokeniser__tokenise__subprogram_def
 use        "_gnatprove_standard".Main
 use        "int".Int
 use        "int".Int
 use        "_gnatprove_standard".Main as Main
 use        "_gnatprove_standard".Integer as Integer
 use        "_gnatprove_standard".Boolean as Boolean
 use        Standard__integer as Standard__integer
 use        Standard__integer___axiom as Standard__integer___axiom
 use        Standard__natural as Standard__natural
 use        Standard__natural___axiom as Standard__natural___axiom
 use        Standard__natural__rep as Standard__natural__rep
 use        Standard__positive as Standard__positive
 use        Standard__positive___axiom as Standard__positive___axiom
 use        Standard__positive__rep as Standard__positive__rep
 use        Standard__character as Standard__character
 use        Standard__character__rep as Standard__character__rep
 use        Array__Int__Standard__character as Array__Int__Standard__character
 use        Standard__string as Standard__string
 use        Standard__integer__rep as Standard__integer__rep
 use        Standard__string___axiom as Standard__string___axiom
 use        Mystringtokeniser__is_whitespace as Mystringtokeniser__is_whitespace
 use        Mystringtokeniser__is_whitespace___axiom as Mystringtokeniser__is_whitespace___axiom
 use        Mystringtokeniser__tokenise__s as Mystringtokeniser__tokenise__s
 use        Array__Int__Mystringtokeniser__tokenextent as Array__Int__Mystringtokeniser__tokenextent
 use        Mystringtokeniser__tokenise__tokens as Mystringtokeniser__tokenise__tokens
 use        Mystringtokeniser__tokenise__count as Mystringtokeniser__tokenise__count
 use        Mystringtokeniser__tokenise__index as Mystringtokeniser__tokenise__index
 use        Mystringtokeniser__tokenise__extent as Mystringtokeniser__tokenise__extent
 use        Mystringtokeniser__tokenextent as Mystringtokeniser__tokenextent
 use        Mystringtokeniser__tokenextent___axiom as Mystringtokeniser__tokenextent___axiom
 use        Mystringtokeniser__tokenise__outindex as Mystringtokeniser__tokenise__outindex
 use        Array__Int__Mystringtokeniser__tokenextent as Array__Int__Mystringtokeniser__tokenextent
 use        Mystringtokeniser__tokenarray as Mystringtokeniser__tokenarray
 use        Mystringtokeniser__tokenise__L_1 as Mystringtokeniser__tokenise__L_1
 use        Mystringtokeniser__tokenise__L_2 as Mystringtokeniser__tokenise__L_2
 use        Mystringtokeniser__tokenise__L_3 as Mystringtokeniser__tokenise__L_3
 use        Array__Int__Standard__character as Array__Int__Standard__character
 use        Array__Int__Standard__character as Array__Int__Standard__character
 use        Array__Int__Standard__character as Array__Int__Standard__character
 use        Array__Int__Standard__character as Array__Int__Standard__character
 use        Array__Int__Standard__character as Array__Int__Standard__character
 use        Standard__integer___axiom as Standard__integer___axiom
 use        Standard__natural___axiom as Standard__natural___axiom
 use        Standard__positive___axiom as Standard__positive___axiom
 use        Standard__character___axiom as Standard__character___axiom
 use        Standard__string___axiom as Standard__string___axiom
 use        Standard__integer___axiom as Standard__integer___axiom
 use        Mystringtokeniser__tokenise__index___axiom as Mystringtokeniser__tokenise__index___axiom
 use        Mystringtokeniser__tokenise__extent___axiom as Mystringtokeniser__tokenise__extent___axiom
 use        Mystringtokeniser__tokenise__outindex___axiom as Mystringtokeniser__tokenise__outindex___axiom
 use        Mystringtokeniser__tokenise__L_2___axiom as Mystringtokeniser__tokenise__L_2___axiom
 use        Mystringtokeniser__tokenise__L_3___axiom as Mystringtokeniser__tokenise__L_3___axiom
 use        Mystringtokeniser__tokenise__L_1___axiom as Mystringtokeniser__tokenise__L_1___axiom
 use        Mystringtokeniser__tokenextent___axiom as Mystringtokeniser__tokenextent___axiom
 use        Mystringtokeniser__tokenarray___axiom as Mystringtokeniser__tokenarray___axiom
 use        Mystringtokeniser__is_whitespace___axiom as Mystringtokeniser__is_whitespace___axiom
 use        Mystringtokeniser__tokenise__s___axiom as Mystringtokeniser__tokenise__s___axiom
 use        Mystringtokeniser__tokenise__tokens___axiom as Mystringtokeniser__tokenise__tokens___axiom
 use        Mystringtokeniser__tokenise__count___axiom as Mystringtokeniser__tokenise__count___axiom

 let def [#"mystringtokeniser.ads" 70 0 0][@GP_Subp:mystringtokeniser.ads:70] 
   (__void_param : unit)
  requires { [#"mystringtokeniser.ads" 70 0 0] true }
  ensures { [#"mystringtokeniser.ads" 70 0 0] ([#"mystringtokeniser.ads" 72 0 0] ( [@GP_Reason:VC_POSTCONDITION] [@GP_Sloc:mystringtokeniser.ads:72:14] [@GP_Id:32] [@comment:     Post => Count <= Tokens'Length and              ^ mystringtokeniser.ads:72:14:VC_POSTCONDITION] [@model_vc_post] [@GP_Shape:pragargs__and] ( ( [@GP_Sloc:mystringtokeniser.ads:72:14] [@GP_Pretty_Ada:2828] (Mystringtokeniser__tokenise__count.count.int__content <= (Integer.length (Standard__integer__rep.to_rep Mystringtokeniser__tokenise__tokens.tokens__first) (Standard__integer__rep.to_rep Mystringtokeniser__tokenise__tokens.tokens__last))) ) /\ (forall index [#"mystringtokeniser.ads" 72 0 0]  [@model_trace:2833] [@name:Index]  : int.
   ( ( ((Standard__integer__rep.to_rep Mystringtokeniser__tokenise__tokens.tokens__first) <= index) /\ (index <= ((Standard__integer__rep.to_rep Mystringtokeniser__tokenise__tokens.tokens__first) + (Mystringtokeniser__tokenise__count.count.int__content - (1 : int)))) ) -> ( ( ( [@GP_Sloc:mystringtokeniser.ads:74:12] [@GP_Pretty_Ada:2858] ((Standard__positive__rep.to_rep (Array__Int__Mystringtokeniser__tokenextent.get Mystringtokeniser__tokenise__tokens.tokens.Array__Int__Mystringtokeniser__tokenextent.map__content index).Mystringtokeniser__tokenextent.__split_fields.Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__start) >= (Standard__string.first Mystringtokeniser__tokenise__s.s)) ) /\ ( [@GP_Sloc:mystringtokeniser.ads:75:11] [@GP_Pretty_Ada:2868] ((Standard__natural__rep.to_rep (Array__Int__Mystringtokeniser__tokenextent.get Mystringtokeniser__tokenise__tokens.tokens.Array__Int__Mystringtokeniser__tokenextent.map__content index).Mystringtokeniser__tokenextent.__split_fields.Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__length) > (0 : int)) ) ) /\ ( [@GP_Sloc:mystringtokeniser.ads:76:13] [@GP_Pretty_Ada:2878] (((Standard__natural__rep.to_rep (Array__Int__Mystringtokeniser__tokenextent.get Mystringtokeniser__tokenise__tokens.tokens.Array__Int__Mystringtokeniser__tokenextent.map__content index).Mystringtokeniser__tokenextent.__split_fields.Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__length) - (1 : int)) <= ((Standard__string.last Mystringtokeniser__tokenise__s.s) - (Standard__positive__rep.to_rep (Array__Int__Mystringtokeniser__tokenextent.get Mystringtokeniser__tokenise__tokens.tokens.Array__Int__Mystringtokeniser__tokenextent.map__content index).Mystringtokeniser__tokenextent.__split_fields.Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__start))) ) ) )) ) )) }
   = [@vc:divergent]
  ( () (* Assume dynamic invariants of inputs of the subprogram mystringtokeniser.ads:70 *)
  ;
   assume {  (Standard__string___axiom.dynamic_invariant Mystringtokeniser__tokenise__s.s True False True True) };
   assume {  ( (if (False) then (
   true) else (
   (Mystringtokeniser__tokenarray.dynamic_property Standard__positive.first Standard__positive.last (Standard__integer__rep.to_rep Mystringtokeniser__tokenise__tokens.tokens__first) (Standard__integer__rep.to_rep Mystringtokeniser__tokenise__tokens.tokens__last)))) /\ (forall temp___220   : int.
   (if (( ((Standard__integer__rep.to_rep Mystringtokeniser__tokenise__tokens.tokens__first) <= temp___220) /\ (temp___220 <= (Standard__integer__rep.to_rep Mystringtokeniser__tokenise__tokens.tokens__last)) )) then (
    (Mystringtokeniser__tokenextent___axiom.dynamic_invariant (Array__Int__Mystringtokeniser__tokenextent.get Mystringtokeniser__tokenise__tokens.tokens.Array__Int__Mystringtokeniser__tokenextent.map__content temp___220) True False True True)) else true)) ) };
   assume {  (Standard__natural___axiom.dynamic_invariant Mystringtokeniser__tokenise__count.count.int__content False False True True) };
  () (* Declarations introduced by the compiler at the beginning of the subprogram mystringtokeniser.ads:70 *)
  ;
  () (* Check for RTE in the Pre of the subprogram mystringtokeniser.ads:70 *)
  ;
   begin ensures {true} let _ = (let _ = (Boolean.andb((if (((  begin ensures {true} let _ = (let _ = Mystringtokeniser__tokenise__s.s in (
   ()))
   in () end ;
  (Standard__string.length(Mystringtokeniser__tokenise__s.s)) ) > (0 : int))) then (
   ((  begin ensures {true} let _ = (let _ = Mystringtokeniser__tokenise__s.s in (
    ()))
    in () end ;
   (Standard__string.first(Mystringtokeniser__tokenise__s.s)) ) <= (  begin ensures {true} let _ = (let _ = Mystringtokeniser__tokenise__s.s in (
    ()))
    in () end ;
   (Standard__string.last(Mystringtokeniser__tokenise__s.s)) ))) else (
   (Boolean.of_int((1 : int)))))) (((Standard__integer__rep.to_rep((  begin ensures {true} let _ = (let _ = Mystringtokeniser__tokenise__tokens.tokens.Array__Int__Mystringtokeniser__tokenextent.map__content in (
   ()))
   in () end ;
  Mystringtokeniser__tokenise__tokens.tokens__first ))) <= (Standard__integer__rep.to_rep((  begin ensures {true} let _ = (let _ = Mystringtokeniser__tokenise__tokens.tokens.Array__Int__Mystringtokeniser__tokenextent.map__content in (
   ()))
   in () end ;
  Mystringtokeniser__tokenise__tokens.tokens__last )))))) in (
   ()))
   in () end ;
  () (* Assume Pre of the subprogram mystringtokeniser.ads:70 *)
  ;
   assume {  ( (if (((Standard__string.length Mystringtokeniser__tokenise__s.s) > (0 : int))) then (
   ( [@GP_Pretty_Ada:2811] ((Standard__string.first Mystringtokeniser__tokenise__s.s) <= (Standard__string.last Mystringtokeniser__tokenise__s.s)) )) else (
   ( [@GP_Pretty_Ada:6213] true ))) /\ ( [@GP_Pretty_Ada:2820] ((Standard__integer__rep.to_rep Mystringtokeniser__tokenise__tokens.tokens__first) <= (Standard__integer__rep.to_rep Mystringtokeniser__tokenise__tokens.tokens__last)) ) ) };
   try
   ( ();
   ([#"mystringtokeniser.adb" 40 0 0] ());
   ([#"mystringtokeniser.adb" 40 0 0] ());
   ([#"mystringtokeniser.adb" 41 0 0] ( [#"mystringtokeniser.adb" 41 0 0] assume { [#"mystringtokeniser.adb" 41 0 0] (Standard__positive___axiom.default_initial_assumption Mystringtokeniser__tokenise__index.index.int__content False) };
   [#"mystringtokeniser.adb" 41 0 0] assume { [#"mystringtokeniser.adb" 41 0 0] (Standard__positive___axiom.dynamic_invariant Mystringtokeniser__tokenise__index.index.int__content False False True True) } ));
   ([#"mystringtokeniser.adb" 42 0 0] ( [#"mystringtokeniser.adb" 42 0 0] begin ensures {true} let _ = (let _ = (let temp___197 [@mlw:proxy_symbol] [@introduced] = [#"mystringtokeniser.adb" 42 0 0] (val _f : Mystringtokeniser__tokenextent.tokenextent
   ensures {[#"mystringtokeniser.adb" 42 0 0] true} 
   in _f) in (
    ( () )))
    in (
    ()))
    in () end ;
   [#"mystringtokeniser.adb" 42 0 0] assume { [#"mystringtokeniser.adb" 42 0 0] (Mystringtokeniser__tokenextent___axiom.default_initial_assumption { Mystringtokeniser__tokenextent.__split_fields = Mystringtokeniser__tokenise__extent.extent__split_fields.Mystringtokeniser__tokenextent.__split_fields__content } False) };
   [#"mystringtokeniser.adb" 42 0 0] assume { [#"mystringtokeniser.adb" 42 0 0] (Mystringtokeniser__tokenextent___axiom.dynamic_invariant { Mystringtokeniser__tokenextent.__split_fields = Mystringtokeniser__tokenise__extent.extent__split_fields.Mystringtokeniser__tokenextent.__split_fields__content } False False True True) } ));
   ([#"mystringtokeniser.adb" 43 0 0] ( [#"mystringtokeniser.adb" 43 0 0] (Mystringtokeniser__tokenise__outindex.outindex.int__content <- ( (Standard__integer__rep.to_rep(( [#"mystringtokeniser.adb" 43 0 0] begin ensures {true} let _ = (let _ = Mystringtokeniser__tokenise__tokens.tokens.Array__Int__Mystringtokeniser__tokenextent.map__content in (
    ()))
    in () end ;
   Mystringtokeniser__tokenise__tokens.tokens__first ))) ));
   [#"mystringtokeniser.adb" 43 0 0] assume { [#"mystringtokeniser.adb" 43 0 0] (Standard__integer___axiom.dynamic_invariant Mystringtokeniser__tokenise__outindex.outindex.int__content True False True True) } ));
   ([#"mystringtokeniser.adb" 60 0 0] ());
   ([#"mystringtokeniser.adb" 69 0 0] ());
   ([#"mystringtokeniser.adb" 50 0 0] ());
   ();
   ( [@GP_Sloc:mystringtokeniser.adb:45:13] ([#"mystringtokeniser.adb" 45 0 0] [#"mystringtokeniser.adb" 45 0 0] (Mystringtokeniser__tokenise__count.count.int__content <- ( (0 : int) ))) );
   ( [@GP_Sloc:mystringtokeniser.adb:46:7] ([#"mystringtokeniser.adb" 46 0 0] (if (( ([#"mystringtokeniser.adb" 46 0 0] [#"mystringtokeniser.adb" 46 0 0] (([@branch_id=2377] Main.spark__branch).bool__content <- ( (( [#"mystringtokeniser.adb" 46 0 0] begin ensures {true} let _ = (let _ = Mystringtokeniser__tokenise__s.s in (
    ()))
    in () end ;
   (Standard__string.first(Mystringtokeniser__tokenise__s.s)) ) > ( [#"mystringtokeniser.adb" 46 0 0] begin ensures {true} let _ = (let _ = Mystringtokeniser__tokenise__s.s in (
    ()))
    in () end ;
   (Standard__string.last(Mystringtokeniser__tokenise__s.s)) )) )));
   ( [@branch_id=2377] Main.spark__branch ).bool__content )) then (
    ( ();
    ( [@GP_Sloc:mystringtokeniser.adb:47:10] ([#"mystringtokeniser.adb" 47 0 0] [#"mystringtokeniser.adb" 47 0 0] raise Return__exc) ) )) else (
    ()))) );
   ( [@GP_Sloc:mystringtokeniser.adb:49:7] ([#"mystringtokeniser.adb" 49 0 0] ()) );
   ( [@GP_Sloc:mystringtokeniser.adb:49:13] ([#"mystringtokeniser.adb" 49 0 0] [#"mystringtokeniser.adb" 49 0 0] (Mystringtokeniser__tokenise__index.index.int__content <- ( ([#"mystringtokeniser.adb" 49 0 0] ( [@GP_Sloc:mystringtokeniser.adb:49:17] [@vc:annotation] [@GP_Shape:index_assign__first_ref] [@GP_Reason:VC_RANGE_CHECK] [@comment:      Index := S'First;                 ^ mystringtokeniser.adb:49:17:VC_RANGE_CHECK] [@GP_Id:0] (Standard__positive.range_check_(( [#"mystringtokeniser.adb" 49 0 0] begin ensures {true} let _ = (let _ = Mystringtokeniser__tokenise__s.s in (
    ()))
    in () end ;
   (Standard__string.first(Mystringtokeniser__tokenise__s.s)) ))) )) ))) );
   ( [@GP_Sloc:mystringtokeniser.adb:50:83] ([#"mystringtokeniser.adb" 50 0 0] ( () (* Translation of an Ada loop from mystringtokeniser.adb:50 *)
   ;
   (if ((Boolean.andb((Boolean.andb((Mystringtokeniser__tokenise__outindex.outindex.int__content <= (Standard__integer__rep.to_rep(( [#"mystringtokeniser.adb" 50 0 0] begin ensures {true} let _ = (let _ = Mystringtokeniser__tokenise__tokens.tokens.Array__Int__Mystringtokeniser__tokenextent.map__content in (
    ()))
    in () end ;
   Mystringtokeniser__tokenise__tokens.tokens__last ))))) ((Mystringtokeniser__tokenise__index.index.int__content <= ( [#"mystringtokeniser.adb" 50 0 0] begin ensures {true} let _ = (let _ = Mystringtokeniser__tokenise__s.s in (
    ()))
    in () end ;
   (Standard__string.last(Mystringtokeniser__tokenise__s.s)) ))))) ((Mystringtokeniser__tokenise__count.count.int__content < ([#"mystringtokeniser.adb" 50 0 0] ( [@GP_Shape:L_1_while__and__cmp__typeconv__length_ref] [@GP_Sloc:mystringtokeniser.adb:50:75] [@vc:annotation] [@comment:      while OutIndex <= Tokens'Last and Index <= S'Last and Count < Tokens'Length loop                                                                           ^ mystringtokeniser.adb:50:75:VC_RANGE_CHECK] [@GP_Reason:VC_RANGE_CHECK] [@GP_Id:24] (Standard__integer.range_check_(( [#"mystringtokeniser.adb" 50 0 0] begin ensures {true} let _ = (let _ = Mystringtokeniser__tokenise__tokens.tokens.Array__Int__Mystringtokeniser__tokenextent.map__content in (
    ()))
    in () end ;
   (Integer.length((Standard__integer__rep.to_rep(Mystringtokeniser__tokenise__tokens.tokens__first))) ((Standard__integer__rep.to_rep(Mystringtokeniser__tokenise__tokens.tokens__last)))) ))) )))))) then (
     try
     (  begin ensures {true} let _ = (let _ = Mystringtokeniser__tokenise__count.count.int__content in (
      ()))
      in () end ;
     (let temp___216 [@mlw:proxy_symbol] [@introduced] =  (val _f : int
     ensures { (result = Mystringtokeniser__tokenise__count.count.int__content)} 
     in _f) in (
      (  begin ensures {true} let _ = (let _ = Mystringtokeniser__tokenise__outindex.outindex.int__content in (
       ()))
       in () end ;
      (let temp___215 [@mlw:proxy_symbol] [@introduced] =  (val _f : int
      ensures { (result = Mystringtokeniser__tokenise__outindex.outindex.int__content)} 
      in _f) in (
       (  begin ensures {true} let _ = (let _ = Mystringtokeniser__tokenise__tokens.tokens.Array__Int__Mystringtokeniser__tokenextent.map__content in (
        ()))
        in () end ;
       (let temp___213 [@mlw:proxy_symbol] [@introduced] =  (val _f : Array__Int__Mystringtokeniser__tokenextent.map
       ensures { (result = Mystringtokeniser__tokenise__tokens.tokens.Array__Int__Mystringtokeniser__tokenextent.map__content)} 
       in _f) in (
        (  begin ensures {true} let _ = (let _ = { Mystringtokeniser__tokenextent.__split_fields = Mystringtokeniser__tokenise__extent.extent__split_fields.Mystringtokeniser__tokenextent.__split_fields__content } in (
         ()))
         in () end ;
        (let temp___211 [@mlw:proxy_symbol] [@introduced] =  (val _f : Mystringtokeniser__tokenextent.tokenextent
        ensures { (result = { Mystringtokeniser__tokenextent.__split_fields = Mystringtokeniser__tokenise__extent.extent__split_fields.Mystringtokeniser__tokenextent.__split_fields__content })} 
        in _f) in (
         (  begin ensures {true} let _ = (let _ = Mystringtokeniser__tokenise__index.index.int__content in (
          ()))
          in () end ;
         (let temp___210 [@mlw:proxy_symbol] [@introduced] =  (val _f : int
         ensures { (result = Mystringtokeniser__tokenise__index.index.int__content)} 
         in _f) in (
          ( () (* First unroling of the loop statements appearing before the loop invariant of loop mystringtokeniser.adb:50 *)
          ;
          () (* While loop translating the Ada loop from mystringtokeniser.adb:50 *)
          ;
          (let temp___inv_218 [@mlw:proxy_symbol] [@introduced] = ( (let j =  (val _f : int
          in _f) in (
           (if ((Boolean.andb(((Standard__integer__rep.to_rep((  begin ensures {true} let _ = (let _ = Mystringtokeniser__tokenise__tokens.tokens.Array__Int__Mystringtokeniser__tokenextent.map__content in (
            ()))
            in () end ;
           Mystringtokeniser__tokenise__tokens.tokens__first ))) <= j)) ((j <= ([#"mystringtokeniser.adb" 52 0 0] ( [@GP_Id:17] [@vc:annotation] [@GP_Reason:VC_OVERFLOW_CHECK] [@GP_Shape:L_1_while__pragargs__forall__range__sub] [@comment:           (for all J in Tokens'First..OutIndex-1 =>                                                ^ mystringtokeniser.adb:52:48:VC_OVERFLOW_CHECK] [@GP_Sloc:mystringtokeniser.adb:52:48] (Standard__integer.range_check_((Mystringtokeniser__tokenise__outindex.outindex.int__content - (1 : int)))) )))))) then (
             begin ensures {true} let _ = (let _ = ( (Boolean.andb(((Standard__positive__rep.to_rep((Array__Int__Mystringtokeniser__tokenextent.get(Mystringtokeniser__tokenise__tokens.tokens.Array__Int__Mystringtokeniser__tokenextent.map__content) ((  assert {  ([#"mystringtokeniser.adb" 53 0 0] ( [@GP_Id:18] [@vc:annotation] [@GP_Shape:L_1_while__pragargs__forall__andthen__and__cmp__selectcomp__ixdcomp] [@GP_Sloc:mystringtokeniser.adb:53:23] [@GP_Reason:VC_INDEX_CHECK] [@comment:              (Tokens(J).Start >= S'First and                       ^ mystringtokeniser.adb:53:23:VC_INDEX_CHECK] ( ((Standard__integer__rep.to_rep Mystringtokeniser__tokenise__tokens.tokens__first) <= j) /\ (j <= (Standard__integer__rep.to_rep Mystringtokeniser__tokenise__tokens.tokens__last)) ) )) };
            j ))).Mystringtokeniser__tokenextent.__split_fields.Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__start)) >= (  begin ensures {true} let _ = (let _ = Mystringtokeniser__tokenise__s.s in (
             ()))
             in () end ;
            (Standard__string.first(Mystringtokeniser__tokenise__s.s)) ))) (((Standard__natural__rep.to_rep((Array__Int__Mystringtokeniser__tokenextent.get(Mystringtokeniser__tokenise__tokens.tokens.Array__Int__Mystringtokeniser__tokenextent.map__content) ((  assert {  ([#"mystringtokeniser.adb" 54 0 0] ( [@comment:                   Tokens(J).Length > 0) and then                           ^ mystringtokeniser.adb:54:27:VC_INDEX_CHECK] [@GP_Id:19] [@vc:annotation] [@GP_Shape:L_1_while__pragargs__forall__andthen__and__cmp__selectcomp__ixdcomp] [@GP_Reason:VC_INDEX_CHECK] [@GP_Sloc:mystringtokeniser.adb:54:27] ( ((Standard__integer__rep.to_rep Mystringtokeniser__tokenise__tokens.tokens__first) <= j) /\ (j <= (Standard__integer__rep.to_rep Mystringtokeniser__tokenise__tokens.tokens__last)) ) )) };
            j ))).Mystringtokeniser__tokenextent.__split_fields.Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__length)) > (0 : int)))) && (((Standard__natural__rep.to_rep((Array__Int__Mystringtokeniser__tokenextent.get(Mystringtokeniser__tokenise__tokens.tokens.Array__Int__Mystringtokeniser__tokenextent.map__content) ((  assert {  ([#"mystringtokeniser.adb" 55 0 0] ( [@comment:            Tokens(J).Length-1 <= S'Last - Tokens(J).Start);                    ^ mystringtokeniser.adb:55:20:VC_INDEX_CHECK] [@vc:annotation] [@GP_Sloc:mystringtokeniser.adb:55:20] [@GP_Reason:VC_INDEX_CHECK] [@GP_Id:20] [@GP_Shape:L_1_while__pragargs__forall__andthen__cmp__sub__selectcomp__ixdcomp] ( ((Standard__integer__rep.to_rep Mystringtokeniser__tokenise__tokens.tokens__first) <= j) /\ (j <= (Standard__integer__rep.to_rep Mystringtokeniser__tokenise__tokens.tokens__last)) ) )) };
            j ))).Mystringtokeniser__tokenextent.__split_fields.Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__length)) - (1 : int)) <= ([#"mystringtokeniser.adb" 55 0 0] ( [@vc:annotation] [@GP_Reason:VC_OVERFLOW_CHECK] [@GP_Shape:L_1_while__pragargs__forall__andthen__cmp__sub] [@GP_Id:22] [@comment:            Tokens(J).Length-1 <= S'Last - Tokens(J).Start);                                          ^ mystringtokeniser.adb:55:42:VC_OVERFLOW_CHECK] [@GP_Sloc:mystringtokeniser.adb:55:42] (Standard__integer.range_check_((( [#"mystringtokeniser.adb" 55 0 0] begin ensures {true} let _ = (let _ = Mystringtokeniser__tokenise__s.s in (
             ()))
             in () end ;
            (Standard__string.last(Mystringtokeniser__tokenise__s.s)) ) - (Standard__positive__rep.to_rep((Array__Int__Mystringtokeniser__tokenextent.get(Mystringtokeniser__tokenise__tokens.tokens.Array__Int__Mystringtokeniser__tokenextent.map__content) (( [#"mystringtokeniser.adb" 55 0 0] assert { [#"mystringtokeniser.adb" 55 0 0] ([#"mystringtokeniser.adb" 55 0 0] ( [@vc:annotation] [@GP_Sloc:mystringtokeniser.adb:55:51] [@GP_Reason:VC_INDEX_CHECK] [@GP_Id:21] [@comment:            Tokens(J).Length-1 <= S'Last - Tokens(J).Start);                                                   ^ mystringtokeniser.adb:55:51:VC_INDEX_CHECK] [@GP_Shape:L_1_while__pragargs__forall__andthen__cmp__sub__selectcomp__ixdcomp] ( ((Standard__integer__rep.to_rep Mystringtokeniser__tokenise__tokens.tokens__first) <= j) /\ (j <= (Standard__integer__rep.to_rep Mystringtokeniser__tokenise__tokens.tokens__last)) ) )) };
            j ))).Mystringtokeniser__tokenextent.__split_fields.Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__start))))) ))) ) in (
             ()))
             in () end ))))
          ;
           (val _f : bool
          ensures { ( (result = True) <-> (forall j   [@name:J] [@model_trace:2415]  : int.
           ( ( ((Standard__integer__rep.to_rep Mystringtokeniser__tokenise__tokens.tokens__first) <= j) /\ (j <= (Mystringtokeniser__tokenise__outindex.outindex.int__content - (1 : int))) ) -> ( ( ((Standard__positive__rep.to_rep (Array__Int__Mystringtokeniser__tokenextent.get Mystringtokeniser__tokenise__tokens.tokens.Array__Int__Mystringtokeniser__tokenextent.map__content j).Mystringtokeniser__tokenextent.__split_fields.Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__start) >= (Standard__string.first Mystringtokeniser__tokenise__s.s)) /\ ((Standard__natural__rep.to_rep (Array__Int__Mystringtokeniser__tokenextent.get Mystringtokeniser__tokenise__tokens.tokens.Array__Int__Mystringtokeniser__tokenextent.map__content j).Mystringtokeniser__tokenextent.__split_fields.Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__length) > (0 : int)) ) /\ (((Standard__natural__rep.to_rep (Array__Int__Mystringtokeniser__tokenextent.get Mystringtokeniser__tokenise__tokens.tokens.Array__Int__Mystringtokeniser__tokenextent.map__content j).Mystringtokeniser__tokenextent.__split_fields.Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__length) - (1 : int)) <= ((Standard__string.last Mystringtokeniser__tokenise__s.s) - (Standard__positive__rep.to_rep (Array__Int__Mystringtokeniser__tokenextent.get Mystringtokeniser__tokenise__tokens.tokens.Array__Int__Mystringtokeniser__tokenextent.map__content j).Mystringtokeniser__tokenextent.__split_fields.Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__start))) ) )) )} 
          in _f) ) in (
            begin ensures {true} let _ = (let _ = (let temp___inv_217 [@mlw:proxy_symbol] [@introduced] = (Mystringtokeniser__tokenise__outindex.outindex.int__content = ([#"mystringtokeniser.adb" 57 0 0] ( [@GP_Id:15] [@GP_Shape:L_1_while__pragargs__cmp__add] [@vc:annotation] [@GP_Sloc:mystringtokeniser.adb:57:57] [@GP_Reason:VC_OVERFLOW_CHECK] [@comment:         pragma Loop_Invariant (OutIndex = Tokens'First + Count);                                                         ^ mystringtokeniser.adb:57:57:VC_OVERFLOW_CHECK] (Standard__integer.range_check_(((Standard__integer__rep.to_rep(( [#"mystringtokeniser.adb" 57 0 0] begin ensures {true} let _ = (let _ = Mystringtokeniser__tokenise__tokens.tokens.Array__Int__Mystringtokeniser__tokenextent.map__content in (
            ()))
            in () end ;
           Mystringtokeniser__tokenise__tokens.tokens__first ))) + Mystringtokeniser__tokenise__count.count.int__content))) ))) in (
             begin ensures {true} let _ = (let _ = () in (
             ()))
             in () end ))
            in (
            ()))
            in () end ))
          ;
           while True do
           invariant { 
            ([#"mystringtokeniser.adb" 52 0 0] ( [@GP_Reason:VC_LOOP_INVARIANT] [@vc:annotation] [@GP_Shape:L_1_while__pragargs__forall] [@GP_Sloc:mystringtokeniser.adb:52:13] [@comment:           (for all J in Tokens'First..OutIndex-1 =>             ^ mystringtokeniser.adb:52:13:VC_LOOP_INVARIANT] [@GP_Id:23] (forall j [#"mystringtokeniser.adb" 52 0 0]  [@name:J] [@model_trace:2415]  : int.
             ( ( ((Standard__integer__rep.to_rep Mystringtokeniser__tokenise__tokens.tokens__first) <= j) /\ (j <= (Mystringtokeniser__tokenise__outindex.outindex.int__content - (1 : int))) ) -> ( ( ( [@GP_Pretty_Ada:2436] [@GP_Sloc:mystringtokeniser.adb:53:16] ((Standard__positive__rep.to_rep (Array__Int__Mystringtokeniser__tokenextent.get Mystringtokeniser__tokenise__tokens.tokens.Array__Int__Mystringtokeniser__tokenextent.map__content j).Mystringtokeniser__tokenextent.__split_fields.Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__start) >= (Standard__string.first Mystringtokeniser__tokenise__s.s)) ) /\ ( [@GP_Pretty_Ada:2446] [@GP_Sloc:mystringtokeniser.adb:54:20] ((Standard__natural__rep.to_rep (Array__Int__Mystringtokeniser__tokenextent.get Mystringtokeniser__tokenise__tokens.tokens.Array__Int__Mystringtokeniser__tokenextent.map__content j).Mystringtokeniser__tokenextent.__split_fields.Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__length) > (0 : int)) ) ) /\ ( [@GP_Sloc:mystringtokeniser.adb:55:13] [@GP_Pretty_Ada:2456] (((Standard__natural__rep.to_rep (Array__Int__Mystringtokeniser__tokenextent.get Mystringtokeniser__tokenise__tokens.tokens.Array__Int__Mystringtokeniser__tokenextent.map__content j).Mystringtokeniser__tokenextent.__split_fields.Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__length) - (1 : int)) <= ((Standard__string.last Mystringtokeniser__tokenise__s.s) - (Standard__positive__rep.to_rep (Array__Int__Mystringtokeniser__tokenextent.get Mystringtokeniser__tokenise__tokens.tokens.Array__Int__Mystringtokeniser__tokenextent.map__content j).Mystringtokeniser__tokenextent.__split_fields.Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__start))) ) ) )) ))
            }
           invariant { 
            ([#"mystringtokeniser.adb" 57 0 0] ( [@comment:         pragma Loop_Invariant (OutIndex = Tokens'First + Count);                                 ^ mystringtokeniser.adb:57:33:VC_LOOP_INVARIANT] [@GP_Reason:VC_LOOP_INVARIANT] [@GP_Id:16] [@vc:annotation] [@GP_Shape:L_1_while__pragargs__cmp] [@GP_Sloc:mystringtokeniser.adb:57:33] ( [@GP_Pretty_Ada:2470] [@GP_Sloc:mystringtokeniser.adb:57:33] (Mystringtokeniser__tokenise__outindex.outindex.int__content = ((Standard__integer__rep.to_rep Mystringtokeniser__tokenise__tokens.tokens__first) + Mystringtokeniser__tokenise__count.count.int__content)) ) ))
            }
           ( () (* Assume implicit invariants from the loop mystringtokeniser.adb:50 *)
           ;
            assume {  (Boolean.andb(( ( ( ( ( ( ( ( ( ( true /\ (Standard__positive___axiom.dynamic_invariant Mystringtokeniser__tokenise__index.index.int__content False True True True) )/\true ) /\ (Mystringtokeniser__tokenextent___axiom.dynamic_invariant { Mystringtokeniser__tokenextent.__split_fields = Mystringtokeniser__tokenise__extent.extent__split_fields.Mystringtokeniser__tokenextent.__split_fields__content } False True True True) )/\true ) /\ ( (if (True) then (
            true) else (
            (Mystringtokeniser__tokenarray.dynamic_property Standard__positive.first Standard__positive.last (Standard__integer__rep.to_rep Mystringtokeniser__tokenise__tokens.tokens__first) (Standard__integer__rep.to_rep Mystringtokeniser__tokenise__tokens.tokens__last)))) /\ (forall temp___212   : int.
            (if (( ((Standard__integer__rep.to_rep Mystringtokeniser__tokenise__tokens.tokens__first) <= temp___212) /\ (temp___212 <= (Standard__integer__rep.to_rep Mystringtokeniser__tokenise__tokens.tokens__last)) )) then (
             (Mystringtokeniser__tokenextent___axiom.dynamic_invariant (Array__Int__Mystringtokeniser__tokenextent.get Mystringtokeniser__tokenise__tokens.tokens.Array__Int__Mystringtokeniser__tokenextent.map__content temp___212) True False True True)) else true)) ) )/\true ) /\ (Standard__integer___axiom.dynamic_invariant Mystringtokeniser__tokenise__outindex.outindex.int__content True True True True) )/\true ) /\ (Standard__natural___axiom.dynamic_invariant Mystringtokeniser__tokenise__count.count.int__content False True True True) )/\true )) (( ( (Mystringtokeniser__tokenise__outindex.outindex.int__content <= (Standard__integer__rep.to_rep Mystringtokeniser__tokenise__tokens.tokens__last)) /\ (Mystringtokeniser__tokenise__index.index.int__content <= (Standard__string.last Mystringtokeniser__tokenise__s.s)) ) /\ (Mystringtokeniser__tokenise__count.count.int__content < (Integer.length (Standard__integer__rep.to_rep Mystringtokeniser__tokenise__tokens.tokens__first) (Standard__integer__rep.to_rep Mystringtokeniser__tokenise__tokens.tokens__last))) ))) };
           () (* Check for absence of RTE in the invariant of loop mystringtokeniser.adb:50 *)
           ;
           () (* Loop statements appearing after the loop invariant of loop mystringtokeniser.adb:50 *)
           ;
           ();
           ( [@GP_Sloc:mystringtokeniser.adb:60:87] ([#"mystringtokeniser.adb" 60 0 0] ( () (* Translation of an Ada loop from mystringtokeniser.adb:60 *)
           ;
           (if (( (Boolean.andb((Mystringtokeniser__tokenise__index.index.int__content >= ( [#"mystringtokeniser.adb" 60 0 0] begin ensures {true} let _ = (let _ = Mystringtokeniser__tokenise__s.s in (
            ()))
            in () end ;
           (Standard__string.first(Mystringtokeniser__tokenise__s.s)) ))) ((Mystringtokeniser__tokenise__index.index.int__content < ( [#"mystringtokeniser.adb" 60 0 0] begin ensures {true} let _ = (let _ = Mystringtokeniser__tokenise__s.s in (
            ()))
            in () end ;
           (Standard__string.last(Mystringtokeniser__tokenise__s.s)) )))) && (Mystringtokeniser__is_whitespace___axiom.is_whitespace((Standard__character__rep.to_rep((Array__Int__Standard__character.get((Standard__string.to_array(Mystringtokeniser__tokenise__s.s))) (( [#"mystringtokeniser.adb" 60 0 0] assert { [#"mystringtokeniser.adb" 60 0 0] ([#"mystringtokeniser.adb" 60 0 0] ( [@GP_Sloc:mystringtokeniser.adb:60:79] [@vc:annotation] [@GP_Shape:L_1_while__L_2_while__andthen__call_is_whitespace__ixdcomp] [@GP_Reason:VC_INDEX_CHECK] [@GP_Id:2] [@comment:         while (Index >= S'First and Index < S'Last) and then Is_Whitespace(S(Index)) loop                                                                               ^ mystringtokeniser.adb:60:79:VC_INDEX_CHECK] ( ((Standard__string.first Mystringtokeniser__tokenise__s.s) <= Mystringtokeniser__tokenise__index.index.int__content) /\ (Mystringtokeniser__tokenise__index.index.int__content <= (Standard__string.last Mystringtokeniser__tokenise__s.s)) ) )) };
           Mystringtokeniser__tokenise__index.index.int__content ))))))) )) then (
             try
             (  begin ensures {true} let _ = (let _ = Mystringtokeniser__tokenise__index.index.int__content in (
              ()))
              in () end ;
             (let temp___199 [@mlw:proxy_symbol] [@introduced] =  (val _f : int
             ensures { (result = Mystringtokeniser__tokenise__index.index.int__content)} 
             in _f) in (
              ( () (* First unroling of the loop statements appearing before the loop invariant of loop mystringtokeniser.adb:60 *)
              ;
              () (* While loop translating the Ada loop from mystringtokeniser.adb:60 *)
              ;
               while True do
               ( () (* Assume implicit invariants from the loop mystringtokeniser.adb:60 *)
               ;
                assume {  (Boolean.andb(( ( true /\ (Standard__positive___axiom.dynamic_invariant Mystringtokeniser__tokenise__index.index.int__content False True True True) )/\true )) (( ( (Mystringtokeniser__tokenise__index.index.int__content >= (Standard__string.first Mystringtokeniser__tokenise__s.s)) /\ (Mystringtokeniser__tokenise__index.index.int__content < (Standard__string.last Mystringtokeniser__tokenise__s.s)) ) /\ ((epsilon temp___result_200 : bool.
                ( (temp___result_200 = (Mystringtokeniser__is_whitespace.is_whitespace (Standard__character__rep.to_rep (Array__Int__Standard__character.get (Standard__string.to_array Mystringtokeniser__tokenise__s.s) Mystringtokeniser__tokenise__index.index.int__content)))) /\ (Mystringtokeniser__is_whitespace.is_whitespace__function_guard temp___result_200 (Standard__character__rep.to_rep (Array__Int__Standard__character.get (Standard__string.to_array Mystringtokeniser__tokenise__s.s) Mystringtokeniser__tokenise__index.index.int__content))) )) = True) ))) };
               () (* Check for absence of RTE in the invariant of loop mystringtokeniser.adb:60 *)
               ;
               () (* Loop statements appearing after the loop invariant of loop mystringtokeniser.adb:60 *)
               ;
               ();
               ( [@GP_Sloc:mystringtokeniser.adb:61:13] ([#"mystringtokeniser.adb" 61 0 0] ()) );
               ( [@GP_Sloc:mystringtokeniser.adb:61:22] ([#"mystringtokeniser.adb" 61 0 0] ()) );
               ( [@GP_Sloc:mystringtokeniser.adb:61:19] ([#"mystringtokeniser.adb" 61 0 0] [#"mystringtokeniser.adb" 61 0 0] (Mystringtokeniser__tokenise__index.index.int__content <- ( ([#"mystringtokeniser.adb" 61 0 0] ( [@comment:            Index := Index + 1;                            ^ mystringtokeniser.adb:61:28:VC_OVERFLOW_CHECK] [@GP_Shape:L_1_while__L_2_while__index_assign__add] [@vc:annotation] [@GP_Reason:VC_OVERFLOW_CHECK] [@GP_Id:1] [@GP_Sloc:mystringtokeniser.adb:61:28] (Standard__integer.range_check_((Mystringtokeniser__tokenise__index.index.int__content + (1 : int)))) )) ))) );
               () (* Check for the exit condition and loop statements appearing before the loop invariant of loop mystringtokeniser.adb:60 *)
               ;
               (if (not ( ( (Boolean.andb((Mystringtokeniser__tokenise__index.index.int__content >= (  begin ensures {true} let _ = (let _ = Mystringtokeniser__tokenise__s.s in (
                ()))
                in () end ;
               (Standard__string.first(Mystringtokeniser__tokenise__s.s)) ))) ((Mystringtokeniser__tokenise__index.index.int__content < (  begin ensures {true} let _ = (let _ = Mystringtokeniser__tokenise__s.s in (
                ()))
                in () end ;
               (Standard__string.last(Mystringtokeniser__tokenise__s.s)) )))) && (Mystringtokeniser__is_whitespace___axiom.is_whitespace((Standard__character__rep.to_rep((Array__Int__Standard__character.get((Standard__string.to_array(Mystringtokeniser__tokenise__s.s))) ((  assert {  ([#"mystringtokeniser.adb" 60 0 0] ( [@GP_Sloc:mystringtokeniser.adb:60:79] [@vc:annotation] [@GP_Shape:L_1_while__L_2_while__andthen__call_is_whitespace__ixdcomp] [@GP_Reason:VC_INDEX_CHECK] [@GP_Id:2] [@comment:         while (Index >= S'First and Index < S'Last) and then Is_Whitespace(S(Index)) loop                                                                               ^ mystringtokeniser.adb:60:79:VC_INDEX_CHECK] ( ((Standard__string.first Mystringtokeniser__tokenise__s.s) <= Mystringtokeniser__tokenise__index.index.int__content) /\ (Mystringtokeniser__tokenise__index.index.int__content <= (Standard__string.last Mystringtokeniser__tokenise__s.s)) ) )) };
               Mystringtokeniser__tokenise__index.index.int__content ))))))) ) )) then (
                 raise Mystringtokeniser__tokenise__L_2.L_2)) )
              done )))
              )
            with
             Mystringtokeniser__tokenise__L_2.L_2 -> ()
            end)) )) );
           ( [@GP_Sloc:mystringtokeniser.adb:63:14] ([#"mystringtokeniser.adb" 63 0 0] ()) );
           ( [@GP_Sloc:mystringtokeniser.adb:63:35] ([#"mystringtokeniser.adb" 63 0 0] ()) );
           ( [@GP_Sloc:mystringtokeniser.adb:63:10] ([#"mystringtokeniser.adb" 63 0 0] (if (( ([#"mystringtokeniser.adb" 63 0 0] [#"mystringtokeniser.adb" 63 0 0] (([@branch_id=2501] Main.spark__branch).bool__content <- ( ( (Boolean.andb((Mystringtokeniser__tokenise__index.index.int__content >= ( [#"mystringtokeniser.adb" 63 0 0] begin ensures {true} let _ = (let _ = Mystringtokeniser__tokenise__s.s in (
            ()))
            in () end ;
           (Standard__string.first(Mystringtokeniser__tokenise__s.s)) ))) ((Mystringtokeniser__tokenise__index.index.int__content <= ( [#"mystringtokeniser.adb" 63 0 0] begin ensures {true} let _ = (let _ = Mystringtokeniser__tokenise__s.s in (
            ()))
            in () end ;
           (Standard__string.last(Mystringtokeniser__tokenise__s.s)) )))) && not ( (Mystringtokeniser__is_whitespace___axiom.is_whitespace((Standard__character__rep.to_rep((Array__Int__Standard__character.get((Standard__string.to_array(Mystringtokeniser__tokenise__s.s))) (( [#"mystringtokeniser.adb" 63 0 0] assert { [#"mystringtokeniser.adb" 63 0 0] ([#"mystringtokeniser.adb" 63 0 0] ( [@GP_Id:14] [@vc:annotation] [@GP_Shape:L_1_while__if__andthen__not__call_is_whitespace__ixdcomp] [@GP_Reason:VC_INDEX_CHECK] [@comment:         if (Index >= S'First and Index <= S'Last) and then not Is_Whitespace(S(Index)) then                                                                                 ^ mystringtokeniser.adb:63:81:VC_INDEX_CHECK] [@GP_Sloc:mystringtokeniser.adb:63:81] ( ((Standard__string.first Mystringtokeniser__tokenise__s.s) <= Mystringtokeniser__tokenise__index.index.int__content) /\ (Mystringtokeniser__tokenise__index.index.int__content <= (Standard__string.last Mystringtokeniser__tokenise__s.s)) ) )) };
           Mystringtokeniser__tokenise__index.index.int__content ))))))) ) ) )));
           ( [@branch_id=2501] Main.spark__branch ).bool__content )) then (
            ( ();
            ( [@GP_Sloc:mystringtokeniser.adb:65:13] ([#"mystringtokeniser.adb" 65 0 0] ()) );
            ( [@GP_Sloc:mystringtokeniser.adb:65:29] ([#"mystringtokeniser.adb" 65 0 0] ()) );
            ( [@GP_Sloc:mystringtokeniser.adb:65:26] ([#"mystringtokeniser.adb" 65 0 0] (let temp___202 [@mlw:proxy_symbol] [@introduced] = (let temp___201 [@mlw:proxy_symbol] [@introduced] = { Mystringtokeniser__tokenextent.__split_fields = Mystringtokeniser__tokenise__extent.extent__split_fields.Mystringtokeniser__tokenextent.__split_fields__content } in (
             ( [#"mystringtokeniser.adb" 65 0 0] begin ensures {true} let _ = (let _ = temp___201.Mystringtokeniser__tokenextent.__split_fields.Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__start in (
              ()))
              in () end ;
             { ( temp___201 ) with Mystringtokeniser__tokenextent.__split_fields = { ( temp___201.Mystringtokeniser__tokenextent.__split_fields ) with Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__start = (Standard__positive__rep.of_rep(Mystringtokeniser__tokenise__index.index.int__content)) } } )))
             in (
             [#"mystringtokeniser.adb" 65 0 0] (Mystringtokeniser__tokenise__extent.extent__split_fields.Mystringtokeniser__tokenextent.__split_fields__content <- ( temp___202.Mystringtokeniser__tokenextent.__split_fields ))))
            ) );
            ( [@GP_Sloc:mystringtokeniser.adb:66:13] ([#"mystringtokeniser.adb" 66 0 0] ()) );
            ( [@GP_Sloc:mystringtokeniser.adb:66:27] ([#"mystringtokeniser.adb" 66 0 0] (let temp___204 [@mlw:proxy_symbol] [@introduced] = (let temp___203 [@mlw:proxy_symbol] [@introduced] = { Mystringtokeniser__tokenextent.__split_fields = Mystringtokeniser__tokenise__extent.extent__split_fields.Mystringtokeniser__tokenextent.__split_fields__content } in (
             ( [#"mystringtokeniser.adb" 66 0 0] begin ensures {true} let _ = (let _ = temp___203.Mystringtokeniser__tokenextent.__split_fields.Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__length in (
              ()))
              in () end ;
             { ( temp___203 ) with Mystringtokeniser__tokenextent.__split_fields = { ( temp___203.Mystringtokeniser__tokenextent.__split_fields ) with Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__length = ( 0 : Standard__natural.natural ) } } )))
             in (
             [#"mystringtokeniser.adb" 66 0 0] (Mystringtokeniser__tokenise__extent.extent__split_fields.Mystringtokeniser__tokenextent.__split_fields__content <- ( temp___204.Mystringtokeniser__tokenextent.__split_fields ))))
            ) );
            ( [@GP_Sloc:mystringtokeniser.adb:69:185] ([#"mystringtokeniser.adb" 69 0 0] ( () (* Translation of an Ada loop from mystringtokeniser.adb:69 *)
            ;
            (if (( ( (([#"mystringtokeniser.adb" 69 0 0] ( [@GP_Shape:L_1_while__if__L_3_while__andthen__andthen__cmp__sub] [@vc:annotation] [@GP_Reason:VC_OVERFLOW_CHECK] [@GP_Sloc:mystringtokeniser.adb:69:33] [@GP_Id:4] [@comment:            while Positive'Last - Extent.Length >= Index and then (Index+Extent.Length >= S'First and Index+Extent.Length <= S'Last) and then not Is_Whitespace(S(Index+Extent.Length)) loop                                 ^ mystringtokeniser.adb:69:33:VC_OVERFLOW_CHECK] (Standard__integer.range_check_(((2147483647 : int) - (Standard__natural__rep.to_rep({ Mystringtokeniser__tokenextent.__split_fields = Mystringtokeniser__tokenise__extent.extent__split_fields.Mystringtokeniser__tokenextent.__split_fields__content }.Mystringtokeniser__tokenextent.__split_fields.Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__length))))) )) >= Mystringtokeniser__tokenise__index.index.int__content) && (Boolean.andb((([#"mystringtokeniser.adb" 69 0 0] ( [@comment:            while Positive'Last - Extent.Length >= Index and then (Index+Extent.Length >= S'First and Index+Extent.Length <= S'Last) and then not Is_Whitespace(S(Index+Extent.Length)) loop                                                                         ^ mystringtokeniser.adb:69:73:VC_OVERFLOW_CHECK] [@vc:annotation] [@GP_Reason:VC_OVERFLOW_CHECK] [@GP_Sloc:mystringtokeniser.adb:69:73] [@GP_Id:5] [@GP_Shape:L_1_while__if__L_3_while__andthen__andthen__and__cmp__add] (Standard__integer.range_check_((Mystringtokeniser__tokenise__index.index.int__content + (Standard__natural__rep.to_rep({ Mystringtokeniser__tokenextent.__split_fields = Mystringtokeniser__tokenise__extent.extent__split_fields.Mystringtokeniser__tokenextent.__split_fields__content }.Mystringtokeniser__tokenextent.__split_fields.Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__length))))) )) >= (  begin ensures {true} let _ = (let _ = Mystringtokeniser__tokenise__s.s in (
             ()))
             in () end ;
            (Standard__string.first(Mystringtokeniser__tokenise__s.s)) ))) ((([#"mystringtokeniser.adb" 69 0 0] ( [@GP_Sloc:mystringtokeniser.adb:69:108] [@vc:annotation] [@GP_Reason:VC_OVERFLOW_CHECK] [@comment:            while Positive'Last - Extent.Length >= Index and then (Index+Extent.Length >= S'First and Index+Extent.Length <= S'Last) and then not Is_Whitespace(S(Index+Extent.Length)) loop                                                                                                            ^ mystringtokeniser.adb:69:108:VC_OVERFLOW_CHECK] [@GP_Shape:L_1_while__if__L_3_while__andthen__andthen__and__cmp__add] [@GP_Id:6] (Standard__integer.range_check_((Mystringtokeniser__tokenise__index.index.int__content + (Standard__natural__rep.to_rep({ Mystringtokeniser__tokenextent.__split_fields = Mystringtokeniser__tokenise__extent.extent__split_fields.Mystringtokeniser__tokenextent.__split_fields__content }.Mystringtokeniser__tokenextent.__split_fields.Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__length))))) )) <= (  begin ensures {true} let _ = (let _ = Mystringtokeniser__tokenise__s.s in (
             ()))
             in () end ;
            (Standard__string.last(Mystringtokeniser__tokenise__s.s)) )))) ) && not ( (Mystringtokeniser__is_whitespace___axiom.is_whitespace((Standard__character__rep.to_rep((Array__Int__Standard__character.get((Standard__string.to_array(Mystringtokeniser__tokenise__s.s))) ((let temp___208 [@mlw:proxy_symbol] [@introduced] = ([#"mystringtokeniser.adb" 69 0 0] ( [@comment:            while Positive'Last - Extent.Length >= Index and then (Index+Extent.Length >= S'First and Index+Extent.Length <= S'Last) and then not Is_Whitespace(S(Index+Extent.Length)) loop                                                                                                                                                                        ^ mystringtokeniser.adb:69:168:VC_OVERFLOW_CHECK] [@GP_Sloc:mystringtokeniser.adb:69:168] [@vc:annotation] [@GP_Reason:VC_OVERFLOW_CHECK] [@GP_Id:7] [@GP_Shape:L_1_while__if__L_3_while__andthen__not__call_is_whitespace__ixdcomp__add] (Standard__integer.range_check_((Mystringtokeniser__tokenise__index.index.int__content + (Standard__natural__rep.to_rep({ Mystringtokeniser__tokenextent.__split_fields = Mystringtokeniser__tokenise__extent.extent__split_fields.Mystringtokeniser__tokenextent.__split_fields__content }.Mystringtokeniser__tokenextent.__split_fields.Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__length))))) )) in (
             (  assert {  ([#"mystringtokeniser.adb" 69 0 0] ( [@comment:            while Positive'Last - Extent.Length >= Index and then (Index+Extent.Length >= S'First and Index+Extent.Length <= S'Last) and then not Is_Whitespace(S(Index+Extent.Length)) loop                                                                                                                                                                        ^ mystringtokeniser.adb:69:168:VC_INDEX_CHECK] [@GP_Sloc:mystringtokeniser.adb:69:168] [@vc:annotation] [@GP_Reason:VC_INDEX_CHECK] [@GP_Id:8] [@GP_Shape:L_1_while__if__L_3_while__andthen__not__call_is_whitespace__ixdcomp__add] ( ((Standard__string.first Mystringtokeniser__tokenise__s.s) <= temp___208) /\ (temp___208 <= (Standard__string.last Mystringtokeniser__tokenise__s.s)) ) )) };
             temp___208 )))
            )))))) ) )) then (
              try
              (  begin ensures {true} let _ = (let _ = { Mystringtokeniser__tokenextent.__split_fields = Mystringtokeniser__tokenise__extent.extent__split_fields.Mystringtokeniser__tokenextent.__split_fields__content } in (
               ()))
               in () end ;
              (let temp___207 [@mlw:proxy_symbol] [@introduced] =  (val _f : Mystringtokeniser__tokenextent.tokenextent
              ensures { (result = { Mystringtokeniser__tokenextent.__split_fields = Mystringtokeniser__tokenise__extent.extent__split_fields.Mystringtokeniser__tokenextent.__split_fields__content })} 
              in _f) in (
               ( () (* First unroling of the loop statements appearing before the loop invariant of loop mystringtokeniser.adb:69 *)
               ;
               () (* While loop translating the Ada loop from mystringtokeniser.adb:69 *)
               ;
                while True do
                ( () (* Assume implicit invariants from the loop mystringtokeniser.adb:69 *)
                ;
                 assume {  (Boolean.andb(( ( true /\ (Mystringtokeniser__tokenextent___axiom.dynamic_invariant { Mystringtokeniser__tokenextent.__split_fields = Mystringtokeniser__tokenise__extent.extent__split_fields.Mystringtokeniser__tokenextent.__split_fields__content } False True True True) )/\({ Mystringtokeniser__tokenextent.__split_fields = Mystringtokeniser__tokenise__extent.extent__split_fields.Mystringtokeniser__tokenextent.__split_fields__content }.Mystringtokeniser__tokenextent.__split_fields.Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__start = temp___207.Mystringtokeniser__tokenextent.__split_fields.Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__start) )) (( ( (((2147483647 : int) - (Standard__natural__rep.to_rep { Mystringtokeniser__tokenextent.__split_fields = Mystringtokeniser__tokenise__extent.extent__split_fields.Mystringtokeniser__tokenextent.__split_fields__content }.Mystringtokeniser__tokenextent.__split_fields.Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__length)) >= Mystringtokeniser__tokenise__index.index.int__content) /\ ( ((Mystringtokeniser__tokenise__index.index.int__content + (Standard__natural__rep.to_rep { Mystringtokeniser__tokenextent.__split_fields = Mystringtokeniser__tokenise__extent.extent__split_fields.Mystringtokeniser__tokenextent.__split_fields__content }.Mystringtokeniser__tokenextent.__split_fields.Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__length)) >= (Standard__string.first Mystringtokeniser__tokenise__s.s)) /\ ((Mystringtokeniser__tokenise__index.index.int__content + (Standard__natural__rep.to_rep { Mystringtokeniser__tokenextent.__split_fields = Mystringtokeniser__tokenise__extent.extent__split_fields.Mystringtokeniser__tokenextent.__split_fields__content }.Mystringtokeniser__tokenextent.__split_fields.Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__length)) <= (Standard__string.last Mystringtokeniser__tokenise__s.s)) ) ) /\ not ( ((epsilon temp___result_209 : bool.
                 ( (temp___result_209 = (Mystringtokeniser__is_whitespace.is_whitespace (Standard__character__rep.to_rep (Array__Int__Standard__character.get (Standard__string.to_array Mystringtokeniser__tokenise__s.s) (Mystringtokeniser__tokenise__index.index.int__content + (Standard__natural__rep.to_rep { Mystringtokeniser__tokenextent.__split_fields = Mystringtokeniser__tokenise__extent.extent__split_fields.Mystringtokeniser__tokenextent.__split_fields__content }.Mystringtokeniser__tokenextent.__split_fields.Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__length)))))) /\ (Mystringtokeniser__is_whitespace.is_whitespace__function_guard temp___result_209 (Standard__character__rep.to_rep (Array__Int__Standard__character.get (Standard__string.to_array Mystringtokeniser__tokenise__s.s) (Mystringtokeniser__tokenise__index.index.int__content + (Standard__natural__rep.to_rep { Mystringtokeniser__tokenextent.__split_fields = Mystringtokeniser__tokenise__extent.extent__split_fields.Mystringtokeniser__tokenextent.__split_fields__content }.Mystringtokeniser__tokenextent.__split_fields.Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__length))))) )) = True) ) ))) };
                () (* Check for absence of RTE in the invariant of loop mystringtokeniser.adb:69 *)
                ;
                () (* Loop statements appearing after the loop invariant of loop mystringtokeniser.adb:69 *)
                ;
                ();
                ( [@GP_Sloc:mystringtokeniser.adb:70:16] ([#"mystringtokeniser.adb" 70 0 0] ()) );
                ( [@GP_Sloc:mystringtokeniser.adb:70:33] ([#"mystringtokeniser.adb" 70 0 0] ()) );
                ( [@GP_Sloc:mystringtokeniser.adb:70:30] ([#"mystringtokeniser.adb" 70 0 0] (let temp___206 [@mlw:proxy_symbol] [@introduced] = (let temp___205 [@mlw:proxy_symbol] [@introduced] = { Mystringtokeniser__tokenextent.__split_fields = Mystringtokeniser__tokenise__extent.extent__split_fields.Mystringtokeniser__tokenextent.__split_fields__content } in (
                 ( [#"mystringtokeniser.adb" 70 0 0] begin ensures {true} let _ = (let _ = temp___205.Mystringtokeniser__tokenextent.__split_fields.Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__length in (
                  ()))
                  in () end ;
                 { ( temp___205 ) with Mystringtokeniser__tokenextent.__split_fields = { ( temp___205.Mystringtokeniser__tokenextent.__split_fields ) with Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__length = (Standard__natural__rep.of_rep(([#"mystringtokeniser.adb" 70 0 0] ( [@GP_Shape:L_1_while__if__L_3_while__extent_assign__add] [@vc:annotation] [@GP_Reason:VC_OVERFLOW_CHECK] [@comment:               Extent.Length := Extent.Length + 1;                                               ^ mystringtokeniser.adb:70:47:VC_OVERFLOW_CHECK] [@GP_Id:3] [@GP_Sloc:mystringtokeniser.adb:70:47] (Standard__integer.range_check_(((Standard__natural__rep.to_rep({ Mystringtokeniser__tokenextent.__split_fields = Mystringtokeniser__tokenise__extent.extent__split_fields.Mystringtokeniser__tokenextent.__split_fields__content }.Mystringtokeniser__tokenextent.__split_fields.Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__length)) + (1 : int)))) )))) } } )))
                 in (
                  (Mystringtokeniser__tokenise__extent.extent__split_fields.Mystringtokeniser__tokenextent.__split_fields__content <- ( temp___206.Mystringtokeniser__tokenextent.__split_fields ))))
                ) );
                () (* Check for the exit condition and loop statements appearing before the loop invariant of loop mystringtokeniser.adb:69 *)
                ;
                (if (not ( ( ( (([#"mystringtokeniser.adb" 69 0 0] ( [@GP_Shape:L_1_while__if__L_3_while__andthen__andthen__cmp__sub] [@vc:annotation] [@GP_Reason:VC_OVERFLOW_CHECK] [@GP_Sloc:mystringtokeniser.adb:69:33] [@GP_Id:4] [@comment:            while Positive'Last - Extent.Length >= Index and then (Index+Extent.Length >= S'First and Index+Extent.Length <= S'Last) and then not Is_Whitespace(S(Index+Extent.Length)) loop                                 ^ mystringtokeniser.adb:69:33:VC_OVERFLOW_CHECK] (Standard__integer.range_check_(((2147483647 : int) - (Standard__natural__rep.to_rep({ Mystringtokeniser__tokenextent.__split_fields = Mystringtokeniser__tokenise__extent.extent__split_fields.Mystringtokeniser__tokenextent.__split_fields__content }.Mystringtokeniser__tokenextent.__split_fields.Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__length))))) )) >= Mystringtokeniser__tokenise__index.index.int__content) && (Boolean.andb((([#"mystringtokeniser.adb" 69 0 0] ( [@comment:            while Positive'Last - Extent.Length >= Index and then (Index+Extent.Length >= S'First and Index+Extent.Length <= S'Last) and then not Is_Whitespace(S(Index+Extent.Length)) loop                                                                         ^ mystringtokeniser.adb:69:73:VC_OVERFLOW_CHECK] [@vc:annotation] [@GP_Reason:VC_OVERFLOW_CHECK] [@GP_Sloc:mystringtokeniser.adb:69:73] [@GP_Id:5] [@GP_Shape:L_1_while__if__L_3_while__andthen__andthen__and__cmp__add] (Standard__integer.range_check_((Mystringtokeniser__tokenise__index.index.int__content + (Standard__natural__rep.to_rep({ Mystringtokeniser__tokenextent.__split_fields = Mystringtokeniser__tokenise__extent.extent__split_fields.Mystringtokeniser__tokenextent.__split_fields__content }.Mystringtokeniser__tokenextent.__split_fields.Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__length))))) )) >= (  begin ensures {true} let _ = (let _ = Mystringtokeniser__tokenise__s.s in (
                 ()))
                 in () end ;
                (Standard__string.first(Mystringtokeniser__tokenise__s.s)) ))) ((([#"mystringtokeniser.adb" 69 0 0] ( [@GP_Sloc:mystringtokeniser.adb:69:108] [@vc:annotation] [@GP_Reason:VC_OVERFLOW_CHECK] [@comment:            while Positive'Last - Extent.Length >= Index and then (Index+Extent.Length >= S'First and Index+Extent.Length <= S'Last) and then not Is_Whitespace(S(Index+Extent.Length)) loop                                                                                                            ^ mystringtokeniser.adb:69:108:VC_OVERFLOW_CHECK] [@GP_Shape:L_1_while__if__L_3_while__andthen__andthen__and__cmp__add] [@GP_Id:6] (Standard__integer.range_check_((Mystringtokeniser__tokenise__index.index.int__content + (Standard__natural__rep.to_rep({ Mystringtokeniser__tokenextent.__split_fields = Mystringtokeniser__tokenise__extent.extent__split_fields.Mystringtokeniser__tokenextent.__split_fields__content }.Mystringtokeniser__tokenextent.__split_fields.Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__length))))) )) <= (  begin ensures {true} let _ = (let _ = Mystringtokeniser__tokenise__s.s in (
                 ()))
                 in () end ;
                (Standard__string.last(Mystringtokeniser__tokenise__s.s)) )))) ) && not ( (Mystringtokeniser__is_whitespace___axiom.is_whitespace((Standard__character__rep.to_rep((Array__Int__Standard__character.get((Standard__string.to_array(Mystringtokeniser__tokenise__s.s))) ((let temp___208 [@mlw:proxy_symbol] [@introduced] = ([#"mystringtokeniser.adb" 69 0 0] ( [@comment:            while Positive'Last - Extent.Length >= Index and then (Index+Extent.Length >= S'First and Index+Extent.Length <= S'Last) and then not Is_Whitespace(S(Index+Extent.Length)) loop                                                                                                                                                                        ^ mystringtokeniser.adb:69:168:VC_OVERFLOW_CHECK] [@GP_Sloc:mystringtokeniser.adb:69:168] [@vc:annotation] [@GP_Reason:VC_OVERFLOW_CHECK] [@GP_Id:7] [@GP_Shape:L_1_while__if__L_3_while__andthen__not__call_is_whitespace__ixdcomp__add] (Standard__integer.range_check_((Mystringtokeniser__tokenise__index.index.int__content + (Standard__natural__rep.to_rep({ Mystringtokeniser__tokenextent.__split_fields = Mystringtokeniser__tokenise__extent.extent__split_fields.Mystringtokeniser__tokenextent.__split_fields__content }.Mystringtokeniser__tokenextent.__split_fields.Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__length))))) )) in (
                 (  assert {  ([#"mystringtokeniser.adb" 69 0 0] ( [@comment:            while Positive'Last - Extent.Length >= Index and then (Index+Extent.Length >= S'First and Index+Extent.Length <= S'Last) and then not Is_Whitespace(S(Index+Extent.Length)) loop                                                                                                                                                                        ^ mystringtokeniser.adb:69:168:VC_INDEX_CHECK] [@GP_Sloc:mystringtokeniser.adb:69:168] [@vc:annotation] [@GP_Reason:VC_INDEX_CHECK] [@GP_Id:8] [@GP_Shape:L_1_while__if__L_3_while__andthen__not__call_is_whitespace__ixdcomp__add] ( ((Standard__string.first Mystringtokeniser__tokenise__s.s) <= temp___208) /\ (temp___208 <= (Standard__string.last Mystringtokeniser__tokenise__s.s)) ) )) };
                 temp___208 )))
                )))))) ) ) )) then (
                  raise Mystringtokeniser__tokenise__L_3.L_3)) )
               done )))
               )
             with
              Mystringtokeniser__tokenise__L_3.L_3 -> ()
             end)) )) );
            ( [@GP_Sloc:mystringtokeniser.adb:73:20] ([#"mystringtokeniser.adb" 73 0 0] ()) );
            ( [@GP_Sloc:mystringtokeniser.adb:73:33] ([#"mystringtokeniser.adb" 73 0 0] ()) );
            ( [@GP_Sloc:mystringtokeniser.adb:73:30] ([#"mystringtokeniser.adb" 73 0 0] [#"mystringtokeniser.adb" 73 0 0] (Mystringtokeniser__tokenise__tokens.tokens.Array__Int__Mystringtokeniser__tokenextent.map__content <- ( (Array__Int__Mystringtokeniser__tokenextent.set(Mystringtokeniser__tokenise__tokens.tokens.Array__Int__Mystringtokeniser__tokenextent.map__content) (( [#"mystringtokeniser.adb" 73 0 0] assert { [#"mystringtokeniser.adb" 73 0 0] ([#"mystringtokeniser.adb" 73 0 0] ( [@GP_Shape:L_1_while__if__tokens_assign__ixdcomp] [@comment:            Tokens(OutIndex) := Extent;                    ^ mystringtokeniser.adb:73:20:VC_INDEX_CHECK] [@vc:annotation] [@GP_Sloc:mystringtokeniser.adb:73:20] [@GP_Reason:VC_INDEX_CHECK] [@GP_Id:9] ( ((Standard__integer__rep.to_rep Mystringtokeniser__tokenise__tokens.tokens__first) <= Mystringtokeniser__tokenise__outindex.outindex.int__content) /\ (Mystringtokeniser__tokenise__outindex.outindex.int__content <= (Standard__integer__rep.to_rep Mystringtokeniser__tokenise__tokens.tokens__last)) ) )) };
            Mystringtokeniser__tokenise__outindex.outindex.int__content )) ({ Mystringtokeniser__tokenextent.__split_fields = Mystringtokeniser__tokenise__extent.extent__split_fields.Mystringtokeniser__tokenextent.__split_fields__content })) ))) );
            ( [@GP_Sloc:mystringtokeniser.adb:74:19] ([#"mystringtokeniser.adb" 74 0 0] [#"mystringtokeniser.adb" 74 0 0] (Mystringtokeniser__tokenise__count.count.int__content <- ( ([#"mystringtokeniser.adb" 74 0 0] ( [@GP_Id:10] [@GP_Shape:L_1_while__if__count_assign__add] [@GP_Sloc:mystringtokeniser.adb:74:28] [@comment:            Count := Count + 1;                            ^ mystringtokeniser.adb:74:28:VC_OVERFLOW_CHECK] [@vc:annotation] [@GP_Reason:VC_OVERFLOW_CHECK] (Standard__integer.range_check_((Mystringtokeniser__tokenise__count.count.int__content + (1 : int)))) )) ))) );
            ( [@GP_Sloc:mystringtokeniser.adb:77:17] ([#"mystringtokeniser.adb" 77 0 0] ()) );
            ( [@GP_Sloc:mystringtokeniser.adb:77:13] ([#"mystringtokeniser.adb" 77 0 0] (if (( ([#"mystringtokeniser.adb" 77 0 0] [#"mystringtokeniser.adb" 77 0 0] (([@branch_id=2593] Main.spark__branch).bool__content <- ( (Mystringtokeniser__tokenise__outindex.outindex.int__content = (Standard__integer__rep.to_rep(( [#"mystringtokeniser.adb" 77 0 0] begin ensures {true} let _ = (let _ = Mystringtokeniser__tokenise__tokens.tokens.Array__Int__Mystringtokeniser__tokenextent.map__content in (
             ()))
             in () end ;
            Mystringtokeniser__tokenise__tokens.tokens__last )))) )));
            ( [@branch_id=2593] Main.spark__branch ).bool__content )) then (
             ( ();
             ( [@GP_Sloc:mystringtokeniser.adb:78:16] ([#"mystringtokeniser.adb" 78 0 0] [#"mystringtokeniser.adb" 78 0 0] raise Return__exc) ) )) else (
             ( ();
             ( [@GP_Sloc:mystringtokeniser.adb:80:16] ([#"mystringtokeniser.adb" 80 0 0] ()) );
             ( [@GP_Sloc:mystringtokeniser.adb:80:28] ([#"mystringtokeniser.adb" 80 0 0] ()) );
             ( [@GP_Sloc:mystringtokeniser.adb:80:25] ([#"mystringtokeniser.adb" 80 0 0] [#"mystringtokeniser.adb" 80 0 0] (Mystringtokeniser__tokenise__outindex.outindex.int__content <- ( ([#"mystringtokeniser.adb" 80 0 0] ( [@GP_Id:11] [@GP_Sloc:mystringtokeniser.adb:80:37] [@comment:               OutIndex := OutIndex + 1;                                     ^ mystringtokeniser.adb:80:37:VC_OVERFLOW_CHECK] [@vc:annotation] [@GP_Reason:VC_OVERFLOW_CHECK] [@GP_Shape:L_1_while__if__if__outindex_assign__add] (Standard__integer.range_check_((Mystringtokeniser__tokenise__outindex.outindex.int__content + (1 : int)))) )) ))) ) )))) );
            ( [@GP_Sloc:mystringtokeniser.adb:84:25] ([#"mystringtokeniser.adb" 84 0 0] ()) );
            ( [@GP_Sloc:mystringtokeniser.adb:84:41] ([#"mystringtokeniser.adb" 84 0 0] ()) );
            ( [@GP_Sloc:mystringtokeniser.adb:84:13] ([#"mystringtokeniser.adb" 84 0 0] (if (( ([#"mystringtokeniser.adb" 84 0 0] [#"mystringtokeniser.adb" 84 0 0] (([@branch_id=2605] Main.spark__branch).bool__content <- ( (([#"mystringtokeniser.adb" 84 0 0] ( [@GP_Id:13] [@GP_Sloc:mystringtokeniser.adb:84:23] [@vc:annotation] [@GP_Reason:VC_OVERFLOW_CHECK] [@comment:            if S'Last - Extent.Length < Index then                       ^ mystringtokeniser.adb:84:23:VC_OVERFLOW_CHECK] [@GP_Shape:L_1_while__if__if__cmp__sub] (Standard__integer.range_check_((( [#"mystringtokeniser.adb" 84 0 0] begin ensures {true} let _ = (let _ = Mystringtokeniser__tokenise__s.s in (
             ()))
             in () end ;
            (Standard__string.last(Mystringtokeniser__tokenise__s.s)) ) - (Standard__natural__rep.to_rep({ Mystringtokeniser__tokenextent.__split_fields = Mystringtokeniser__tokenise__extent.extent__split_fields.Mystringtokeniser__tokenextent.__split_fields__content }.Mystringtokeniser__tokenextent.__split_fields.Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__length))))) )) < Mystringtokeniser__tokenise__index.index.int__content) )));
            ( [@branch_id=2605] Main.spark__branch ).bool__content )) then (
             ( ();
             ( [@GP_Sloc:mystringtokeniser.adb:85:16] ([#"mystringtokeniser.adb" 85 0 0] [#"mystringtokeniser.adb" 85 0 0] raise Return__exc) ) )) else (
             ( ();
             ( [@GP_Sloc:mystringtokeniser.adb:87:16] ([#"mystringtokeniser.adb" 87 0 0] ()) );
             ( [@GP_Sloc:mystringtokeniser.adb:87:25] ([#"mystringtokeniser.adb" 87 0 0] ()) );
             ( [@GP_Sloc:mystringtokeniser.adb:87:33] ([#"mystringtokeniser.adb" 87 0 0] ()) );
             ( [@GP_Sloc:mystringtokeniser.adb:87:22] ([#"mystringtokeniser.adb" 87 0 0] [#"mystringtokeniser.adb" 87 0 0] (Mystringtokeniser__tokenise__index.index.int__content <- ( ([#"mystringtokeniser.adb" 87 0 0] ( [@GP_Shape:L_1_while__if__if__index_assign__add] [@GP_Sloc:mystringtokeniser.adb:87:31] [@GP_Id:12] [@vc:annotation] [@GP_Reason:VC_OVERFLOW_CHECK] [@comment:               Index := Index + Extent.Length;                               ^ mystringtokeniser.adb:87:31:VC_OVERFLOW_CHECK] (Standard__integer.range_check_((Mystringtokeniser__tokenise__index.index.int__content + (Standard__natural__rep.to_rep({ Mystringtokeniser__tokenextent.__split_fields = Mystringtokeniser__tokenise__extent.extent__split_fields.Mystringtokeniser__tokenextent.__split_fields__content }.Mystringtokeniser__tokenextent.__split_fields.Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__length))))) )) ))) ) )))) ) )) else (
            ()))) );
           () (* Check for the exit condition and loop statements appearing before the loop invariant of loop mystringtokeniser.adb:50 *)
           ;
           (if (not ( (Boolean.andb((Boolean.andb((Mystringtokeniser__tokenise__outindex.outindex.int__content <= (Standard__integer__rep.to_rep((  begin ensures {true} let _ = (let _ = Mystringtokeniser__tokenise__tokens.tokens.Array__Int__Mystringtokeniser__tokenextent.map__content in (
            ()))
            in () end ;
           Mystringtokeniser__tokenise__tokens.tokens__last ))))) ((Mystringtokeniser__tokenise__index.index.int__content <= (  begin ensures {true} let _ = (let _ = Mystringtokeniser__tokenise__s.s in (
            ()))
            in () end ;
           (Standard__string.last(Mystringtokeniser__tokenise__s.s)) ))))) ((Mystringtokeniser__tokenise__count.count.int__content < ([#"mystringtokeniser.adb" 50 0 0] ( [@GP_Shape:L_1_while__and__cmp__typeconv__length_ref] [@GP_Sloc:mystringtokeniser.adb:50:75] [@vc:annotation] [@comment:      while OutIndex <= Tokens'Last and Index <= S'Last and Count < Tokens'Length loop                                                                           ^ mystringtokeniser.adb:50:75:VC_RANGE_CHECK] [@GP_Reason:VC_RANGE_CHECK] [@GP_Id:24] (Standard__integer.range_check_(( [#"mystringtokeniser.adb" 50 0 0] begin ensures {true} let _ = (let _ = Mystringtokeniser__tokenise__tokens.tokens.Array__Int__Mystringtokeniser__tokenextent.map__content in (
            ()))
            in () end ;
           (Integer.length((Standard__integer__rep.to_rep(Mystringtokeniser__tokenise__tokens.tokens__first))) ((Standard__integer__rep.to_rep(Mystringtokeniser__tokenise__tokens.tokens__last)))) ))) ))))) )) then (
             raise Mystringtokeniser__tokenise__L_1.L_1));
           (let temp___inv_218 [@mlw:proxy_symbol] [@introduced] = ( (let j =  (val _f : int
           in _f) in (
            (if ((Boolean.andb(((Standard__integer__rep.to_rep((  begin ensures {true} let _ = (let _ = Mystringtokeniser__tokenise__tokens.tokens.Array__Int__Mystringtokeniser__tokenextent.map__content in (
             ()))
             in () end ;
            Mystringtokeniser__tokenise__tokens.tokens__first ))) <= j)) ((j <= ([#"mystringtokeniser.adb" 52 0 0] ( [@GP_Id:17] [@vc:annotation] [@GP_Reason:VC_OVERFLOW_CHECK] [@GP_Shape:L_1_while__pragargs__forall__range__sub] [@comment:           (for all J in Tokens'First..OutIndex-1 =>                                                ^ mystringtokeniser.adb:52:48:VC_OVERFLOW_CHECK] [@GP_Sloc:mystringtokeniser.adb:52:48] (Standard__integer.range_check_((Mystringtokeniser__tokenise__outindex.outindex.int__content - (1 : int)))) )))))) then (
              begin ensures {true} let _ = (let _ = ( (Boolean.andb(((Standard__positive__rep.to_rep((Array__Int__Mystringtokeniser__tokenextent.get(Mystringtokeniser__tokenise__tokens.tokens.Array__Int__Mystringtokeniser__tokenextent.map__content) ((  assert {  ([#"mystringtokeniser.adb" 53 0 0] ( [@GP_Id:18] [@vc:annotation] [@GP_Shape:L_1_while__pragargs__forall__andthen__and__cmp__selectcomp__ixdcomp] [@GP_Sloc:mystringtokeniser.adb:53:23] [@GP_Reason:VC_INDEX_CHECK] [@comment:              (Tokens(J).Start >= S'First and                       ^ mystringtokeniser.adb:53:23:VC_INDEX_CHECK] ( ((Standard__integer__rep.to_rep Mystringtokeniser__tokenise__tokens.tokens__first) <= j) /\ (j <= (Standard__integer__rep.to_rep Mystringtokeniser__tokenise__tokens.tokens__last)) ) )) };
             j ))).Mystringtokeniser__tokenextent.__split_fields.Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__start)) >= (  begin ensures {true} let _ = (let _ = Mystringtokeniser__tokenise__s.s in (
              ()))
              in () end ;
             (Standard__string.first(Mystringtokeniser__tokenise__s.s)) ))) (((Standard__natural__rep.to_rep((Array__Int__Mystringtokeniser__tokenextent.get(Mystringtokeniser__tokenise__tokens.tokens.Array__Int__Mystringtokeniser__tokenextent.map__content) ((  assert {  ([#"mystringtokeniser.adb" 54 0 0] ( [@comment:                   Tokens(J).Length > 0) and then                           ^ mystringtokeniser.adb:54:27:VC_INDEX_CHECK] [@GP_Id:19] [@vc:annotation] [@GP_Shape:L_1_while__pragargs__forall__andthen__and__cmp__selectcomp__ixdcomp] [@GP_Reason:VC_INDEX_CHECK] [@GP_Sloc:mystringtokeniser.adb:54:27] ( ((Standard__integer__rep.to_rep Mystringtokeniser__tokenise__tokens.tokens__first) <= j) /\ (j <= (Standard__integer__rep.to_rep Mystringtokeniser__tokenise__tokens.tokens__last)) ) )) };
             j ))).Mystringtokeniser__tokenextent.__split_fields.Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__length)) > (0 : int)))) && (((Standard__natural__rep.to_rep((Array__Int__Mystringtokeniser__tokenextent.get(Mystringtokeniser__tokenise__tokens.tokens.Array__Int__Mystringtokeniser__tokenextent.map__content) ((  assert {  ([#"mystringtokeniser.adb" 55 0 0] ( [@comment:            Tokens(J).Length-1 <= S'Last - Tokens(J).Start);                    ^ mystringtokeniser.adb:55:20:VC_INDEX_CHECK] [@vc:annotation] [@GP_Sloc:mystringtokeniser.adb:55:20] [@GP_Reason:VC_INDEX_CHECK] [@GP_Id:20] [@GP_Shape:L_1_while__pragargs__forall__andthen__cmp__sub__selectcomp__ixdcomp] ( ((Standard__integer__rep.to_rep Mystringtokeniser__tokenise__tokens.tokens__first) <= j) /\ (j <= (Standard__integer__rep.to_rep Mystringtokeniser__tokenise__tokens.tokens__last)) ) )) };
             j ))).Mystringtokeniser__tokenextent.__split_fields.Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__length)) - (1 : int)) <= ([#"mystringtokeniser.adb" 55 0 0] ( [@vc:annotation] [@GP_Reason:VC_OVERFLOW_CHECK] [@GP_Shape:L_1_while__pragargs__forall__andthen__cmp__sub] [@GP_Id:22] [@comment:            Tokens(J).Length-1 <= S'Last - Tokens(J).Start);                                          ^ mystringtokeniser.adb:55:42:VC_OVERFLOW_CHECK] [@GP_Sloc:mystringtokeniser.adb:55:42] (Standard__integer.range_check_((( [#"mystringtokeniser.adb" 55 0 0] begin ensures {true} let _ = (let _ = Mystringtokeniser__tokenise__s.s in (
              ()))
              in () end ;
             (Standard__string.last(Mystringtokeniser__tokenise__s.s)) ) - (Standard__positive__rep.to_rep((Array__Int__Mystringtokeniser__tokenextent.get(Mystringtokeniser__tokenise__tokens.tokens.Array__Int__Mystringtokeniser__tokenextent.map__content) (( [#"mystringtokeniser.adb" 55 0 0] assert { [#"mystringtokeniser.adb" 55 0 0] ([#"mystringtokeniser.adb" 55 0 0] ( [@vc:annotation] [@GP_Sloc:mystringtokeniser.adb:55:51] [@GP_Reason:VC_INDEX_CHECK] [@GP_Id:21] [@comment:            Tokens(J).Length-1 <= S'Last - Tokens(J).Start);                                                   ^ mystringtokeniser.adb:55:51:VC_INDEX_CHECK] [@GP_Shape:L_1_while__pragargs__forall__andthen__cmp__sub__selectcomp__ixdcomp] ( ((Standard__integer__rep.to_rep Mystringtokeniser__tokenise__tokens.tokens__first) <= j) /\ (j <= (Standard__integer__rep.to_rep Mystringtokeniser__tokenise__tokens.tokens__last)) ) )) };
             j ))).Mystringtokeniser__tokenextent.__split_fields.Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__start))))) ))) ) in (
              ()))
              in () end ))))
           ;
            (val _f : bool
           ensures { ( (result = True) <-> (forall j   [@name:J] [@model_trace:2415]  : int.
            ( ( ((Standard__integer__rep.to_rep Mystringtokeniser__tokenise__tokens.tokens__first) <= j) /\ (j <= (Mystringtokeniser__tokenise__outindex.outindex.int__content - (1 : int))) ) -> ( ( ((Standard__positive__rep.to_rep (Array__Int__Mystringtokeniser__tokenextent.get Mystringtokeniser__tokenise__tokens.tokens.Array__Int__Mystringtokeniser__tokenextent.map__content j).Mystringtokeniser__tokenextent.__split_fields.Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__start) >= (Standard__string.first Mystringtokeniser__tokenise__s.s)) /\ ((Standard__natural__rep.to_rep (Array__Int__Mystringtokeniser__tokenextent.get Mystringtokeniser__tokenise__tokens.tokens.Array__Int__Mystringtokeniser__tokenextent.map__content j).Mystringtokeniser__tokenextent.__split_fields.Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__length) > (0 : int)) ) /\ (((Standard__natural__rep.to_rep (Array__Int__Mystringtokeniser__tokenextent.get Mystringtokeniser__tokenise__tokens.tokens.Array__Int__Mystringtokeniser__tokenextent.map__content j).Mystringtokeniser__tokenextent.__split_fields.Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__length) - (1 : int)) <= ((Standard__string.last Mystringtokeniser__tokenise__s.s) - (Standard__positive__rep.to_rep (Array__Int__Mystringtokeniser__tokenextent.get Mystringtokeniser__tokenise__tokens.tokens.Array__Int__Mystringtokeniser__tokenextent.map__content j).Mystringtokeniser__tokenextent.__split_fields.Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__start))) ) )) )} 
           in _f) ) in (
             begin ensures {true} let _ = (let _ = (let temp___inv_217 [@mlw:proxy_symbol] [@introduced] = (Mystringtokeniser__tokenise__outindex.outindex.int__content = ([#"mystringtokeniser.adb" 57 0 0] ( [@GP_Id:15] [@GP_Shape:L_1_while__pragargs__cmp__add] [@vc:annotation] [@GP_Sloc:mystringtokeniser.adb:57:57] [@GP_Reason:VC_OVERFLOW_CHECK] [@comment:         pragma Loop_Invariant (OutIndex = Tokens'First + Count);                                                         ^ mystringtokeniser.adb:57:57:VC_OVERFLOW_CHECK] (Standard__integer.range_check_(((Standard__integer__rep.to_rep(( [#"mystringtokeniser.adb" 57 0 0] begin ensures {true} let _ = (let _ = Mystringtokeniser__tokenise__tokens.tokens.Array__Int__Mystringtokeniser__tokenextent.map__content in (
             ()))
             in () end ;
            Mystringtokeniser__tokenise__tokens.tokens__first ))) + Mystringtokeniser__tokenise__count.count.int__content))) ))) in (
              begin ensures {true} let _ = (let _ = () in (
              ()))
              in () end ))
             in (
             ()))
             in () end ))
            )
          done )))
          )))
         )))
        )))
       )))
      )
    with
     Mystringtokeniser__tokenise__L_1.L_1 -> ()
    end)) )) );
    raise Return__exc )
  with
   Return__exc -> ()
  end;
   begin ensures {true} let _ = (let _ = (Boolean.andb((Mystringtokeniser__tokenise__count.count.int__content <= ([#"mystringtokeniser.ads" 72 0 0] ( [@comment:     Post => Count <= Tokens'Length and                             ^ mystringtokeniser.ads:72:29:VC_RANGE_CHECK] [@GP_Shape:pragargs__and__cmp__typeconv__length_ref] [@GP_Sloc:mystringtokeniser.ads:72:29] [@vc:annotation] [@GP_Reason:VC_RANGE_CHECK] [@GP_Id:25] (Standard__integer.range_check_(( [#"mystringtokeniser.ads" 72 0 0] begin ensures {true} let _ = (let _ = Mystringtokeniser__tokenise__tokens.tokens.Array__Int__Mystringtokeniser__tokenextent.map__content in (
   ()))
   in () end ;
  (Integer.length((Standard__integer__rep.to_rep(Mystringtokeniser__tokenise__tokens.tokens__first))) ((Standard__integer__rep.to_rep(Mystringtokeniser__tokenise__tokens.tokens__last)))) ))) )))) (( (let index =  (val _f : int
  in _f) in (
   (if ((Boolean.andb(((Standard__integer__rep.to_rep((  begin ensures {true} let _ = (let _ = Mystringtokeniser__tokenise__tokens.tokens.Array__Int__Mystringtokeniser__tokenextent.map__content in (
    ()))
    in () end ;
   Mystringtokeniser__tokenise__tokens.tokens__first ))) <= index)) ((index <= ([#"mystringtokeniser.ads" 73 0 0] ( [@GP_Shape:pragargs__and__forall__range__add] [@comment:     (for all Index in Tokens'First..Tokens'First+(Count-1) =>                                                  ^ mystringtokeniser.ads:73:50:VC_OVERFLOW_CHECK] [@vc:annotation] [@GP_Reason:VC_OVERFLOW_CHECK] [@GP_Sloc:mystringtokeniser.ads:73:50] [@GP_Id:26] (Standard__integer.range_check_(((Standard__integer__rep.to_rep(( [#"mystringtokeniser.ads" 73 0 0] begin ensures {true} let _ = (let _ = Mystringtokeniser__tokenise__tokens.tokens.Array__Int__Mystringtokeniser__tokenextent.map__content in (
    ()))
    in () end ;
   Mystringtokeniser__tokenise__tokens.tokens__first ))) + (Mystringtokeniser__tokenise__count.count.int__content - (1 : int))))) )))))) then (
     begin ensures {true} let _ = (let _ = ( (Boolean.andb(((Standard__positive__rep.to_rep((Array__Int__Mystringtokeniser__tokenextent.get(Mystringtokeniser__tokenise__tokens.tokens.Array__Int__Mystringtokeniser__tokenextent.map__content) ((  assert {  ([#"mystringtokeniser.ads" 74 0 0] ( [@GP_Sloc:mystringtokeniser.ads:74:19] [@vc:annotation] [@comment:          (Tokens(Index).Start >= S'First and                   ^ mystringtokeniser.ads:74:19:VC_INDEX_CHECK] [@GP_Shape:pragargs__and__forall__andthen__and__cmp__selectcomp__ixdcomp] [@GP_Reason:VC_INDEX_CHECK] [@GP_Id:27] ( ((Standard__integer__rep.to_rep Mystringtokeniser__tokenise__tokens.tokens__first) <= index) /\ (index <= (Standard__integer__rep.to_rep Mystringtokeniser__tokenise__tokens.tokens__last)) ) )) };
    index ))).Mystringtokeniser__tokenextent.__split_fields.Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__start)) >= (  begin ensures {true} let _ = (let _ = Mystringtokeniser__tokenise__s.s in (
     ()))
     in () end ;
    (Standard__string.first(Mystringtokeniser__tokenise__s.s)) ))) (((Standard__natural__rep.to_rep((Array__Int__Mystringtokeniser__tokenextent.get(Mystringtokeniser__tokenise__tokens.tokens.Array__Int__Mystringtokeniser__tokenextent.map__content) ((  assert {  ([#"mystringtokeniser.ads" 75 0 0] ( [@vc:annotation] [@GP_Shape:pragargs__and__forall__andthen__and__cmp__selectcomp__ixdcomp] [@GP_Sloc:mystringtokeniser.ads:75:18] [@GP_Reason:VC_INDEX_CHECK] [@comment:          Tokens(Index).Length > 0) and then                  ^ mystringtokeniser.ads:75:18:VC_INDEX_CHECK] [@GP_Id:28] ( ((Standard__integer__rep.to_rep Mystringtokeniser__tokenise__tokens.tokens__first) <= index) /\ (index <= (Standard__integer__rep.to_rep Mystringtokeniser__tokenise__tokens.tokens__last)) ) )) };
    index ))).Mystringtokeniser__tokenextent.__split_fields.Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__length)) > (0 : int)))) && (((Standard__natural__rep.to_rep((Array__Int__Mystringtokeniser__tokenextent.get(Mystringtokeniser__tokenise__tokens.tokens.Array__Int__Mystringtokeniser__tokenextent.map__content) ((  assert {  ([#"mystringtokeniser.ads" 76 0 0] ( [@vc:annotation] [@GP_Reason:VC_INDEX_CHECK] [@GP_Sloc:mystringtokeniser.ads:76:20] [@GP_Id:29] [@comment:            Tokens(Index).Length-1 <= S'Last - Tokens(Index).Start);                    ^ mystringtokeniser.ads:76:20:VC_INDEX_CHECK] [@GP_Shape:pragargs__and__forall__andthen__cmp__sub__selectcomp__ixdcomp] ( ((Standard__integer__rep.to_rep Mystringtokeniser__tokenise__tokens.tokens__first) <= index) /\ (index <= (Standard__integer__rep.to_rep Mystringtokeniser__tokenise__tokens.tokens__last)) ) )) };
    index ))).Mystringtokeniser__tokenextent.__split_fields.Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__length)) - (1 : int)) <= ([#"mystringtokeniser.ads" 76 0 0] ( [@GP_Sloc:mystringtokeniser.ads:76:46] [@vc:annotation] [@GP_Reason:VC_OVERFLOW_CHECK] [@GP_Id:31] [@GP_Shape:pragargs__and__forall__andthen__cmp__sub] [@comment:            Tokens(Index).Length-1 <= S'Last - Tokens(Index).Start);                                              ^ mystringtokeniser.ads:76:46:VC_OVERFLOW_CHECK] (Standard__integer.range_check_((( [#"mystringtokeniser.ads" 76 0 0] begin ensures {true} let _ = (let _ = Mystringtokeniser__tokenise__s.s in (
     ()))
     in () end ;
    (Standard__string.last(Mystringtokeniser__tokenise__s.s)) ) - (Standard__positive__rep.to_rep((Array__Int__Mystringtokeniser__tokenextent.get(Mystringtokeniser__tokenise__tokens.tokens.Array__Int__Mystringtokeniser__tokenextent.map__content) (( [#"mystringtokeniser.ads" 76 0 0] assert { [#"mystringtokeniser.ads" 76 0 0] ([#"mystringtokeniser.ads" 76 0 0] ( [@vc:annotation] [@GP_Id:30] [@GP_Reason:VC_INDEX_CHECK] [@comment:            Tokens(Index).Length-1 <= S'Last - Tokens(Index).Start);                                                       ^ mystringtokeniser.ads:76:55:VC_INDEX_CHECK] [@GP_Shape:pragargs__and__forall__andthen__cmp__sub__selectcomp__ixdcomp] [@GP_Sloc:mystringtokeniser.ads:76:55] ( ((Standard__integer__rep.to_rep Mystringtokeniser__tokenise__tokens.tokens__first) <= index) /\ (index <= (Standard__integer__rep.to_rep Mystringtokeniser__tokenise__tokens.tokens__last)) ) )) };
    index ))).Mystringtokeniser__tokenextent.__split_fields.Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__start))))) ))) ) in (
     ()))
     in () end ))))
  ;
   (val _f : bool
  ensures { ( (result = True) <-> (forall index   [@model_trace:2833] [@name:Index]  : int.
   ( ( ((Standard__integer__rep.to_rep Mystringtokeniser__tokenise__tokens.tokens__first) <= index) /\ (index <= ((Standard__integer__rep.to_rep Mystringtokeniser__tokenise__tokens.tokens__first) + (Mystringtokeniser__tokenise__count.count.int__content - (1 : int)))) ) -> ( ( ( [@GP_Pretty_Ada:2858] ((Standard__positive__rep.to_rep (Array__Int__Mystringtokeniser__tokenextent.get Mystringtokeniser__tokenise__tokens.tokens.Array__Int__Mystringtokeniser__tokenextent.map__content index).Mystringtokeniser__tokenextent.__split_fields.Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__start) >= (Standard__string.first Mystringtokeniser__tokenise__s.s)) ) /\ ( [@GP_Pretty_Ada:2868] ((Standard__natural__rep.to_rep (Array__Int__Mystringtokeniser__tokenextent.get Mystringtokeniser__tokenise__tokens.tokens.Array__Int__Mystringtokeniser__tokenextent.map__content index).Mystringtokeniser__tokenextent.__split_fields.Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__length) > (0 : int)) ) ) /\ ( [@GP_Pretty_Ada:2878] (((Standard__natural__rep.to_rep (Array__Int__Mystringtokeniser__tokenextent.get Mystringtokeniser__tokenise__tokens.tokens.Array__Int__Mystringtokeniser__tokenextent.map__content index).Mystringtokeniser__tokenextent.__split_fields.Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__length) - (1 : int)) <= ((Standard__string.last Mystringtokeniser__tokenise__s.s) - (Standard__positive__rep.to_rep (Array__Int__Mystringtokeniser__tokenextent.get Mystringtokeniser__tokenise__tokens.tokens.Array__Int__Mystringtokeniser__tokenextent.map__content index).Mystringtokeniser__tokenextent.__split_fields.Mystringtokeniser__tokenextent.rec__mystringtokeniser__tokenextent__start))) ) ) )) )} 
  in _f) ))) in (
   ()))
   in () end  )
end
